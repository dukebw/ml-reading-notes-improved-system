# Deep NLP course

- Google books n-gram viewer

## NLP _without_ deep learning

- "Syntax" Andrew Carnie: sentences are generated by a subconcious set of
  procedures that are part of our minds.
        
        - Generative view (pre-programmed rules). Plug context into a set of
          hard-coded grammar rules.

## NLP with deep learning

- Language as a function approximation, taking inputs (sentence, world)

        - Rely on corpus (data) to learn function

## Natural language understanding

- What does it mean that a machine understands natural languages?

        - It is about understanding the likelihood of a sentence (in a given
          context).

        - E.g., caption machine needs to score a caption, given an image.

        - Scoring equivalent to generation, but intractable.

- Should we start reading linguistics?

        - U.F. Goldberg / Yann LeCun debate about whether it is necessary.

        - Fred Jelinek, 1988 "Every time I fire a linguist, the performance of
          the [speech] recognizer goes up."

## Language Modelling

- A sentence (x1, x2, ..., xt), {xi} words.

- p(x1, x2, ..., xt) = ?

- Rewrite p(x1, x2, ..., xt) as \product p(x{i+1}|xi, ..., x1)

## Statistical language modelling

- Maximize the log likelihood of sentences in the corpora.

        - max E_D [log p(x1, x2, ..., xt)]

## n-gram language modelling

- non-parametric approach

- n-th order Markov assumption:

        - p(x1, x2, ..., xt) \approx \product p(x{i+1}|xi, ..., x{i-n})

        - Issue: dependency beyond the context window is _ignored_.

- Collect n-gram statistics from a large corpus:

        - p(xt|x{t-n}, ..., x{t-1}) \approx count(x1, ..., xt)/count(x1, ..., x{t-1})

- Issues: data sparsity. # of all possible n-grams: |V|^n, where V is the
  vocabulary.

        - E.g. if any given n-gram sequence doesn't exist in the corpus, then
          the probability of the entire sentence is zero.

        - Laplace smoothing: add some small constant value.

                - alpha smoothing. count() + alpha in numerator and
                  denominator.

        - Backoff: fall back on a smaller n'.

- Second issue: lack of generalization.

        - E.g. (chases, a, dog), (chases, a, cat) -> (chases, a, llama)

        - Class-based language model (chases, a, <animal>)

                - Ontologies

                - Issue: exceptions, e.g. <birds> fly, but penguins don't fly.

## Neural language modelling

- 1-of-K or one-hot encoding of each word xt'.

        - Removes any prior knowledge about relationships between any pair of
          words (since their distance is always 1?).

        - What is the actual encoding of languages?

                - Doesn't matter for neural networks, as this is a learned
                  representation.

- Continuous space word representation: st' = W'xt', where W \in R^{|V| x d}

- Nonlinearity h = tanh(U'[s{t-1}; ...; s{t-n}] + b)

- Unnormalized probabilities: y = Vh + c, where V \in R^{|V| x d} and
  c \in R^|V|

- Softmax normalization

- Why does neural language model (one-hot -> cell state st -> nonlinear ->
  softmax) group similar terms together in the cell state?

        - Neural net has a fixed capacity, so it tends to group similar terms
          together?

        - Nearest neighbours -> radial basis function (take subset of total
          dataset).

        - If network capacity grows, approaches nearest neighbour (the set of
          one-hot vectors).

- Therefore neural language model generalizes, since similar terms are placed
  together in the hidden state.

- Neural language models (or recurrent neural language model) can be used for
  any known languages.

- Transfer learning: at character level, can work with multiple languages.

## Continuous-space representation: embeddings

- Grouping related words together is exactly what the neural network has been
  trained to do.

- t-SNE

- What is the metric in an embedding space? Cosine distance between points.
  Magnitude?

## Non-Markovian language modelling

- Directly model the original conditional probabilities
  p(x1, ..., xT) = \product p(xt|x1, ..., x{t-1})

- Feature extraction ht = f(x1, ..., x{t-1})

- Readout: p(xt|x1, ..., x{t-1}) = g(ht)

## Language modelling via recursion / RNN language modelling

- Recursion: ht = f(x{t-1}, h{t-1}), ht called hidden state or memory.

- Combination: p(the, cat, is, eating) = g(h0)g(h1)g(h2)g(h3)

- Read, update and predict.

- Need: transition and readout functions.

- Could have decomposed into product of conditionals: left-to-right,
  right-to-left, more exotic orderings.

- Inputs: input one-hot vector, hidden state h{t-1}

- Parameters: W

- ht = nonlinearity(Wx{t-1} + Uh{t-1} + b)

- Softmax readout function

## Cost function

- Maximize log likelihood probability of a sentence <=>
  minimize -1/N \sum{n=1}{N} \sum{t=1}{Tn} log p(...)

## Minibatch SGD

## Backprop

- Per-sample cost function decomposed into per-timestep cost function
  J(\theta, x) = -\sum{t=1}{T} Jt(\theta, xt)

## Gated recurrent units

- Temporal dependency and vanishing gradient

- Multiply transition matrix N times in backprop gradient computation.

        - If largest eigenvalue is < 1 -> vanishing gradient.

        - If largest eigenvalue is > 1 -> potential for exploding gradient.

- Issue: no learning signal.

## Exploding gradient

- Gradient clipping. (Pascanu et al., 2013).

        - (1) Gradient norm clipping: Gradients divided by their own norm.

        - (2) Element-wise gradient clipping (faster).

## Vanishing gradient

- Two potential reasons: either because data has no temporal dependency, or
  because the largest eigenvalue of the transition matrix is smaller than 1.

- (Pascanu et al., 2013) regularize omega (assume only second case, which is
  the problematic case).

- Avoid vanishing gradients via temporal shortcut connections.

        - 90s (higher-order RNN).

## Gated Recurrent Units

- Adaptive leaky integration: ht = (1 - ut) \hadamard h{t-1} + ut \hadamard h't

- Update gate

- Candidate state

- Pruning connections.

        - Adaptive reset

        - Reset gate

- (Hochreiter and Schmidhuber, 1999), (Gers et al., 2001)

### tanh-RNN vs. CPU

- Registers h

- Execution:

        1. Read whole register h

        2. Update whole register h \gets tanh(W[x] + Uh + b)

                - Should not update whole set of registers.

### GRU vs. CPU

- Execution:

        1. Select a readable subset r

        2. Read the subset r \hadamard h

        3. Select a writable subset u

        4. Update the subset h \gets u \hadamard h' + (1 - ut) \hadamard h

## GRUs vs. LSTM

- Nearly equivalent

- Number of parameters and number of gates differ. LSTM ~two lines more
  complicated.

- Google Deepmind David Silver extensive comparison between GRUs and LSTM:

        - New problem: use GRU with single hidden layer.

        - Problem requiring a more complex solution: use LSTMs.

- ht summary of full sentence x1, ..., x{t-1}

- g(ht): distribution over all the words.

## Statistical machine translation

- 1949: Warren Weaver's memorandum <translation>

- 1991-1993 statistical machine translation from IBM

- Important to understand history of statistical machine translation, and other
  paradigms.

- log p(f|e) = log p(e|f) + log p(f)

        - Translation model: log p(e|f)

                - Fit with parallel corpora

        - Language model: log p(f)

                - Fit with monolingual corpora

        - log p(f|e) is _conditional_ language modelling.

### Statistical machine translation - reality

- log p(f|e) \approx \sum{n=1}{N} fn(e, f) + C

        - log-linear model

        - feature function fn(e, f)

        - Steps:

                1. Experts engineer _useful_ features.

                2. Use simple log-linear model.

                3. Use a strong, external language model on the target side to
                   sieve out nonsense translations.

## Toward neural machine translation

- Source sentence -> SMT -> Neural net -> Target sentence

- Downside: no integration between neural net and statistical machine
  translation system.

- Neural network as yet another feature for log-linear model:
  source sentence -> SMT + neural net -> target sentence

- Downside: ?

- Replace entire MT system with a neural network.

        - Pros: no feature engineering.

        - End-to-end network tuning to maximize translation quality.

        - Cons: increased complexity during training.

## Neural MT in history

- (Allen, 1987) neural net model for translation. 3310 sentence pairs.

- (Chrimas, 1992) dual-ported RAAM architecture. 216 sentence pairs, 75%
  accuracy on test set.

- (Forcada and Neco, 1997), (Castano and ?, ?)

- Issue: vanishing gradient, therefore could not train a model.

- (Kalchbrener and Blunsom, 2013), (Sutskever et al., 2014),
  (Cho et al., 2014). Encoder-decoder networks.

## Sequence-to-sequence learning - encoder

- Encoder:

        1. 1-of-K coding of _source_ words.

        2. Continuous space representation.
                - st' = W'xt', where W \in R^{|V| x d}

        3. Recursively read words.

- Decoder:

        1. Recursively update memory: zt' = f(z{t'-1}, u{t'-1}, hT)

        2. Compute the next word prob.

        3. Sample a next word.
                - Beam search.

- At every step in decoding, use the compressed, summarized source input.

- A sufficiently deep encoder-decoder model can match the state of the art
  statistical machine translation models (Sutskever et al., 2014).

- Overfitting: what is the model capacity of a neural net? Affected by:

        - # of parameters

        - Depth

- Deterministic or stochastic? Stochastic (beam search etc.?)

- Segmenting sequences of characters into sequences of words, largely the same
  problem as segmenting sequences of words into sentences.

- RNNs can handle sentences hundreds of words long.

- Issue with encoder-decoder: assumes that you can fit the entire meaning of
  the sentence into one vector. - Ray Mooney

## Attention/alignment-based neural MT systems

- Encoder: bidirectional RNN

        - A set of annotation vectors {h1, ..., hT}

- Attention-based decoder:

        1. Compute attention weights.

        2. Weighted-sum of the annotation vectors: at',t \propto exp(e(z{t'-1}, u{t'-1}, ht))

        3. Use ct' instead of hT.

- Weights of annotation vectors re-computed at each step of the output
  sequence.

- Therefore alignments are learned.

- Arabic: interesting morphological structure that makes machine translation
  difficult. -> character-level translation. Unicode as character in CJK?

## Decoding sampling strategies

- Ancestral sampling (inefficient).

        - Generate N samples from the distribution, and choose argmax.

- Greedy search: pick the most likely symbol at each timestep.

        - Sub-optimal, because each selection doesn't consider future symbols.

        - x't = argmax{x} log p(x|x<t, Y)

- Beam search:

        - Maintain K hypotheses at a time.

        - Expand each hypothesis.

        - Pick top-K hypotheses from the union Ht = \union{k=1}{K} Bk, where
          Bk = argmax{X' \in Ak} log p(X'|Y)

                - Practice is to use 5-20 chains.

        - BLEU score has a peak, i.e. optimal value for K number of chains.

- Monte-Carlo tree search

- Intermediate between greedy search and beam search?

- Pruning out beam search?

- Keeping beams that are within a certain range of the most likely hypothesis
  at each step.

## Very large target vocabulary (Jean et al., 2015)

- Most time spent and most memory spent in operations proportional to
  vocabulary size times dimensionality of latent vector.

- Importance sampling _without sampling_ (becomes biased)

- Use all the words from the minibatch, plus some set of very frequent words.

- I.e. use V' such that |V'| << |V|

        - Training time:

                - Divide training corpus into D subsets.

                - Separately build vocabulary V' for each subset.

        - Test time:

                - K-most frequent words.

                - K' words aligned to each source word.

## InterLingua 2.0

- Multi-way, multilingual machine translation.

- Encoder: project a source sentence into a set of continuous vectors.

        - Language agnostic

- Want near-bijective mapping between continuous vector space and sentence
  space.

- (Firat et al., 2016; Luong et al., 2015; Dong et al., 2015)

- One encoder for each source language, one decoder for each target language,
  and _shared_ attention mechanism.

- Must be able to be trained with bilingual corpa only.

- Hope that transfer learning occurs from high-resource language pairs to
  language pairs for which we have low resources.

- 6 languages in -> 6 languages out (Firat et al., 2016a)

- Remaining questions:

        1. What is it good for, other than parameter saving?

        2. What if a source sentence is given in multiple input languages?

        3. Transfer to low resource language pair settings?
           (Firat et al., 2016b) 17.28 -> 22.56 BLEU score by adding high
           resource languages.

           - Improvements may come from the better decoder coming from more
             data, therefore this needs to be controlled for.

- If data has noisy source sentence, training is robust. But not robust to
  noisy target sentence.

- FAIR Paris training encoder-decoder model for sentence retrieval.

- Can we explicitly split the hidden vector space into a shared space and
  subspace for given language pairs?

- Para-phrasing: datasets built with neural MT model. Unclear how to train a
  para-phrase model or how to evaluate.

        - Para-phrase detection datasets exist.

- BLEU score evaluation:

        - Take source: 一只猫坐在毯子上
        - Target: the cat is sitting on the mat.
        - Translation: A cat is sitting on the couch.
        - Look at unigram and bigram matches.
        - Multiple references.
        - Correlates to human evaluation.

## Multi-source translation with multi-way, multilingual neural MT

- ?

## Multimodal translation

- (Xu et al, 2015) image captioning.

- Feature maps produced by convolution analogous to bi-directional annotation
  vectors computed by attention-based decoder.

- Multilingual translation system where one of the inputs is an image
  (multi-modal translation system). -> no improvements so far.

- (Caglayan et al., 2016; Elliott and Kadar, 2017)

## Character level neural MT

- What is the minimal unit of meaning, in language?

- Choice of words as minimal unit somewhat ad hoc: belief of lexeme (word) as
  basic unit of meaning, "fear" of data sparsity, concern about training RNNs.

        - State space grows exponentially with length of input
          (1M**20 vs. 50**100).

        - Neural language model projects discrete input space into continuous
          latent space, avoiding data sparsity issue by placing similar input
          vectors close together.

- Issues with treating each token separately: sub-optimal
  segmentation/tokenization (run, runs, ran, running).
  (Chung et al., 2016; Almahairi et al., 2016; Both and Blunsom, 2014).

- Issues with tokenizing words if the language allows compounding of words.

- Consider word as sequence of characters:
  (Luong and Manning, 2016; Ling et al., 2015ab; Kim et al., 2015;
   Bellesteros et al., 2015; dos Santos and Zadrozny 2014).

        - Hybrid approach of using character level translation when the neural
          MT word-level model says that the word is rare.

        - "Decoder within decoder"

        - Still does not solve the compound-word problem, since the full source
          sequence must still be represented by a single vector.

- Source: a sequence of byte pair encoding based character n-grams.

        - Byte pair encoding: (Sennrich et al., 2015) gives statistically
          coherent encoding.

- Target: an unbroken sequence of characters.

- Outperforms word-level model

- Attention intermittently target end-of-sentence: potentially the model is
  learning to constantly compare the length of the generated target sentence
  with the length of the source sentence.

- Issue: Training slow due to serial nature of RNNs.

- Fully character-level translation: CNN + RNN (Lee et al., 2017)

        - Convolutional-recurrent encoder, where kernels have different sizes.

        - Kernel size two detects bigrams, size three detects trigrams, etc. up
          to kernel size of eight.

                - Eight-gram in order to, on average, cover 1-2 words.

        - Then apply pooling to reduce the sequence size by a factor of 5-10
          (max pooling with stride 5).

                - Max size of conv kernel must be larger than the pooling size.

        - Four layer highway network.

        - Single-layer bidirectional GRU.

        - char -> char outperforms bpe -> char.

        - Multi-lingual models perform well (except for German).

        - Takes 2-3 weeks to train. Due to bi-directional RNN, and the fact
          that the attention is quadratic. Also converges faster with
          convolution -- conjectured to be due to convnets extracting abstracts
          on which the RNNs operate.

- Intra-sentence code switching to test multi-lingual language model
  (Lee et al., 2017; Johnson et al., 2016; Ha et al., 2016).

        - Able to manage intra-sentence code switching without explicit
          training.

- Character level more robust to spelling mistakes, rare and nonce words.

- Character-level for question-answer, dialogue models, social media?

## A general question-answering machine

- Question -> Q-module (processes query) -> S-module (selects answers) -> A-module -> Answer
                |                               ^
                |                               |
                --> R-module (retrieves relevant documents) <-> Knowledge source

- Problem: models trained on massive training set of all possible
  question-answer pairs will have to be re-trained to answer questions about
  newly-generated knowledge.

- Train a neural net to access a knowledge source.

        - Use off-the-shelf search engine.

        - Retrieved information assumed noisy.

        - Learn to use the search engine under these conditions.

- Can neural machine translation also use a search engine? (Gu et al., 2017)

        - Can add example source/target pairs to the search engine, and the
          neural MT can use those pairs to translate similar examples, without
          re-training.

        - I.e. train a model to use an external knowledge source, and then
          augment that knowledge source.

        - Similar idea: skip-thought vectors + instance-based learning

                - Doesn't help with adding new knowledge of new
                  source -> target mappings.

- Retrieval stage:

        - Query indexed database of millions of (source, target) sentence
          pairs.

        - Reduce the results from hundreds down to one using fuzzy scores from
          neural network.

- Translation stage (1):

        - Store the retrieved pair in key-value memory
          (Gulcehre et al., 2016; Henaff et al., 2017).

        - Use attention-based neural MT, where key is context vector and value
          is target symbol.

- Translation stage (2):

        - Retrieves relevant target symbol from memory using input context
          vector.

        - Consider \beta values from memory as actual probability distribution
          over all the target symbols.

                - Have softmax probability distribution of words and \beta
                  distribution over memory. Take convex sum over these words.
                  Decides whether the usual translation distribution is used or
                  the the non-parametric distribution is used.

- Translation with more consistent style and word choice / personalized
  translation (e.g. legal document translation).

- Translation often one-to-one in order to exactly capture nuance.

- Retrieval important because of inability to compress entire word knowledge
  into a single sentence.

## Reinforcement learning

- Issue: how to deal with weak learning signal when the state space is large,
  as in machine translation?

- Supervised + reinforcement learning (feature extraction by unsupervised
  learning)

        - By looking at unlabelled data, reduce size of state space.

- Imitation learning (Ross et al., 2011; Daume et al., 2007)

        - Supervisor adjusts the reward signal from the world.

        - Issue: where does the supervisor come from?

        - (SafeDAgger) SafetyNet skips easy examples past the supervisor, so
          that the learner learns from difficult examples.

                - SafetyNet looks at internal structure of learning, and ground
                  truth label, and decides whether to gate or allow supervision
                  for the given example.

                - E.g. driver takes over from model when SafetyNet predicts
                  that the decision is dangerous, thereby generating an extra
                  ground truth for the difficult example.

                - SafetyNet is trained by a held-out set.

- Combine supervised/unsupervised/reinforcement learning in creative ways.

## Simultaneous Translation (Gu, Cho and Li, 2016)

- Decoding:

        1. Pre-train neural MT model (as classifier for each example?)

        2. Build simultaneous decoder.

        3. Decoder either forces pretrained model to output a target symbol, or
           wait for the next source symbol.

- Learning:

        1. Delay vs. quality tradeoff.

                - Reward is interpolation between delay and translation
                  quality.

        2. Stochastic policy gradient (REINFORCE).

- Objective functions beyond log p(Y|X) maximum log likelihood function.

        - E.g. use only 4000 most common phrases.

- Trained Encoder-Decoder-Attention model with supervised learning

- Pre-trained neural MT model summarizes source context vector,
  already-translated sentence and next action into a single vector.
  (ct, z*, y*)

- Variable length attention in simultaneous translation: invariant to length of
  input.

- Take advantage of existing trainable systems to "meta-learn" your problem.

- Next-word prediction (E.g. president of the united _).

- Erase potentially not yet addressed (i.e. backtracking in the output buffer,
  given more source input).

        - Potentially problematic due to evaluating the "delay" value in the
          objective function.

        - Also computational difficulty of implementing backtracking on GPU.

- Neural net that can both view and manipulate internal state of another neural
  net.

## Trainable decoding algorithm (2) (Gu, Cho and Li, 2017)

- Decoding:

        1. Start with pre-trained neural MT model.

        2. Trainable decoder intercepts and interprets incoming signal.

        3. Trainable decoder sends out altering signal back to the pre-trained
           model.

        - Updates according to zt = \phi(z{t-1} + b, ct, y'{t-1})

        - "+ b" term has been added by trainable decoder.

        - Trainable decoder has 32 hidden units only.

        - Greedy decoding: y'1 = argmax{y1} log p(y1|x), y'2 = argmax{y2} log p(y2|y'1 x)

        - Learned virus: manipulates existing pre-trained neural MT model, in order to
          achieve the virus's (agent's) objective.

- Learning:

        1. Deterministic policy gradient.

        - Actor and critic

        - Actor a = \pi(o)

        - Critic Rt = C(o, a)

        - Use gradient of C approximation to arbitrary objective function to
          update model parameters.

        2. Maximize any arbitrary objective.

## (1) Trainable decoding algorithm

1. Actor \pi : R{3d} -> R{d}

## Future ideas

- Single visual cortex used for Q&A, object detection, driving etc.

- Single language model used for talking to parents, understanding news etc.

- Single motor control system...

- Does end-to-end learning make sense with this goal in mind?

- Idea: neural networks are modules (software engineering)

        - Each module used for multiple tasks.

        - Communication of information between modules using shared
          representations.

        - E.g. manipulate motor control NN module to drive a car.

        - Issue: target tasks are not known at training time.

        - Need access to internals of NN modules when re-using for new tasks.

        - This is the motivation for trainable gradient decoding and
          simultaneous translation.

- E.g. image captioning: should have trained a language model with a large
  corpus, and then use a higher level module to look at the features produced
  by the convnet, and use the language model to produce a description
  corresponding to the image.

- Curriculum learning: difficult to decide on the curriculum.

        - Automatic curriculum learning: Alex Graves Google Deepmind.

- Limitation of existing translation systems: input is segmented into
  sentences, which are translated independently.

        - How to incorporate the textual context of the entire paragraph in the
          translation?

        - Issue: improvement from the context is difficult to evaluate.

        - E.g. score difference from translating "it" to correctly-gendered
          pro-noun in French based on surrounding context will hardly register
          in overall BLEU score.

        - Evaluation metrics (such as gender one) can be exploited (e.g. by
          surrounding word differences).

## Learning to Google

- Motivation: compressing universal word knowledge into a fixed set of
  parameters is unrealistic.

- In general, two different short search inputs will cause search engines to
  retrieve different results, one set of which will be better than the other.

        - Want to reformulate "google artificial intelligence paper asian board
          game" into "deepmind go paper".

- Multiple reformulations of an original search query, which fetch results,
  which are scored (reward).

- Future work: automated researcher - hypothesis -> result -> reformulation.

- At every step can we run the RNN a number of times to solve problems that
  require computational complexity greater than O(n).

- Emergent language learning (two NNs communicating).
