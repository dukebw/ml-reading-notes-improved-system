# Deep NLP course

- Google books n-gram viewer

## NLP _without_ deep learning

- "Syntax" Andrew Carnie: sentences are generated by a subconcious set of
  procedures that are part of our minds.
        
        - Generative view (pre-programmed rules). Plug context into a set of
          hard-coded grammar rules.

## NLP with deep learning

- Language as a function approximation, taking inputs (sentence, world)

        - Rely on corpus (data) to learn function

## Natural language understanding

- What does it mean that a machine understands natural languages?

        - It is about understanding the likelihood of a sentence (in a given
          context).

        - E.g., caption machine needs to score a caption, given an image.

        - Scoring equivalent to generation, but intractable.

- Should we start reading linguistics?

        - U.F. Goldberg / Yann LeCun debate about whether it is necessary.

        - Fred Jelinek, 1988 "Every time I fire a linguist, the performance of
          the [speech] recognizer goes up."

## Language Modelling

- A sentence (x1, x2, ..., xt), {xi} words.

- p(x1, x2, ..., xt) = ?

- Rewrite p(x1, x2, ..., xt) as \product p(x{i+1}|xi, ..., x1)

## Statistical language modelling

- Maximize the log likelihood of sentences in the corpora.

        - max E_D [log p(x1, x2, ..., xt)]

## n-gram language modelling

- non-parametric approach

- n-th order Markov assumption:

        - p(x1, x2, ..., xt) \approx \product p(x{i+1}|xi, ..., x{i-n})

        - Issue: dependency beyond the context window is _ignored_.

- Collect n-gram statistics from a large corpus:

        - p(xt|x{t-n}, ..., x{t-1}) \approx count(x1, ..., xt)/count(x1, ..., x{t-1})

- Issues: data sparsity. # of all possible n-grams: |V|^n, where V is the
  vocabulary.

        - E.g. if any given n-gram sequence doesn't exist in the corpus, then
          the probability of the entire sentence is zero.

        - Laplace smoothing: add some small constant value.

                - alpha smoothing. count() + alpha in numerator and
                  denominator.

        - Backoff: fall back on a smaller n'.

- Second issue: lack of generalization.

        - E.g. (chases, a, dog), (chases, a, cat) -> (chases, a, llama)

        - Class-based language model (chases, a, <animal>)

                - Ontologies

                - Issue: exceptions, e.g. <birds> fly, but penguins don't fly.

## Neural language modelling

- 1-of-K or one-hot encoding of each word xt'.

        - Removes any prior knowledge about relationships between any pair of
          words (since their distance is always 1?).

        - What is the actual encoding of languages?

                - Doesn't matter for neural networks, as this is a learned
                  representation.

- Continuous space word representation: st' = W'xt', where W \in R^{|V| x d}

- Nonlinearity h = tanh(U'[s{t-1}; ...; s{t-n}] + b)

- Unnormalized probabilities: y = Vh + c, where V \in R^{|V| x d} and
  c \in R^|V|

- Softmax normalization

- Why does neural language model (one-hot -> cell state st -> nonlinear ->
  softmax) group similar terms together in the cell state?

        - Neural net has a fixed capacity, so it tends to group similar terms
          together?

        - Nearest neighbours -> radial basis function (take subset of total
          dataset).

        - If network capacity grows, approaches nearest neighbour (the set of
          one-hot vectors).

- Therefore neural language model generalizes, since similar terms are placed
  together in the hidden state.

- Neural language models (or recurrent neural language model) can be used for
  any known languages.

- Transfer learning: at character level, can work with multiple languages.

## Continuous-space representation: embeddings

- Grouping related words together is exactly what the neural network has been
  trained to do.

- t-SNE

- What is the metric in an embedding space? Cosine distance between points.
  Magnitude?

## Non-Markovian language modelling

- Directly model the original conditional probabilities
  p(x1, ..., xT) = \product p(xt|x1, ..., x{t-1})

- Feature extraction ht = f(x1, ..., x{t-1})

- Readout: p(xt|x1, ..., x{t-1}) = g(ht)

## Language modelling via recursion / RNN language modelling

- Recursion: ht = f(x{t-1}, h{t-1}), ht called hidden state or memory.

- Combination: p(the, cat, is, eating) = g(h0)g(h1)g(h2)g(h3)

- Read, update and predict.

- Need: transition and readout functions.

- Could have decomposed into product of conditionals: left-to-right,
  right-to-left, more exotic orderings.

- Inputs: input one-hot vector, hidden state h{t-1}

- Parameters: W

- ht = nonlinearity(Wx{t-1} + Uh{t-1} + b)

- Softmax readout function

## Cost function

- Maximize log likelihood probability of a sentence <=>
  minimize -1/N \sum{n=1}{N} \sum{t=1}{Tn} log p(...)

## Minibatch SGD

## Backprop

- Per-sample cost function decomposed into per-timestep cost function
  J(\theta, x) = -\sum{t=1}{T} Jt(\theta, xt)

## Gated recurrent units

- Temporal dependency and vanishing gradient

- Multiply transition matrix N times in backprop gradient computation.

        - If largest eigenvalue is < 1 -> vanishing gradient.

        - If largest eigenvalue is > 1 -> potential for exploding gradient.

- Issue: no learning signal.

## Exploding gradient

- Gradient clipping. (Pascanu et al., 2013).

        - (1) Gradient norm clipping: Gradients divided by their own norm.

        - (2) Element-wise gradient clipping (faster).

## Vanishing gradient

- Two potential reasons: either because data has no temporal dependency, or
  because the largest eigenvalue of the transition matrix is smaller than 1.

- (Pascanu et al., 2013) regularize omega (assume only second case, which is
  the problematic case).

- Avoid vanishing gradients via temporal shortcut connections.

        - 90s (higher-order RNN).

## Gated Recurrent Units

- Adaptive leaky integration: ht = (1 - ut) \hadamard h{t-1} + ut \hadamard h't

- Update gate

- Candidate state

- Pruning connections.

        - Adaptive reset

        - Reset gate

- (Hochreiter and Schmidhuber, 1999), (Gers et al., 2001)

### tanh-RNN vs. CPU

- Registers h

- Execution:

        1. Read whole register h

        2. Update whole register h \gets tanh(W[x] + Uh + b)

                - Should not update whole set of registers.

### GRU vs. CPU

- Execution:

        1. Select a readable subset r

        2. Read the subset r \hadamard h

        3. Select a writable subset u

        4. Update the subset h \gets u \hadamard h' + (1 - ut) \hadamard h

## GRUs vs. LSTM

- Nearly equivalent

- Number of parameters and number of gates differ. LSTM ~two lines more
  complicated.

- Google Deepmind David Silver extensive comparison between GRUs and LSTM:

        - New problem: use GRU with single hidden layer.

        - Problem requiring a more complex solution: use LSTMs.

- ht summary of full sentence x1, ..., x{t-1}

- g(ht): distribution over all the words.

## Statistical machine translation

- 1949: Warren Weaver's memorandum <translation>

- 1991-1993 statistical machine translation from IBM

- Important to understand history of statistical machine translation, and other
  paradigms.

- log p(f|e) = log p(e|f) + log p(f)

        - Translation model: log p(e|f)

                - Fit with parallel corpora

        - Language model: log p(f)

                - Fit with monolingual corpora

        - log p(f|e) is _conditional_ language modelling.

### Statistical machine translation - reality

- log p(f|e) \approx \sum{n=1}{N} fn(e, f) + C

        - log-linear model

        - feature function fn(e, f)

        - Steps:

                1. Experts engineer _useful_ features.

                2. Use simple log-linear model.

                3. Use a strong, external language model on the target side to
                   sieve out nonsense translations.

## Toward neural machine translation

- Source sentence -> SMT -> Neural net -> Target sentence

- Downside: no integration between neural net and statistical machine
  translation system.

- Neural network as yet another feature for log-linear model:
  source sentence -> SMT + neural net -> target sentence

- Downside: ?

- Replace entire MT system with a neural network.

        - Pros: no feature engineering.

        - End-to-end network tuning to maximize translation quality.

        - Cons: increased complexity during training.

## Neural MT in history

- (Allen, 1987) neural net model for translation. 3310 sentence pairs.

- (Chrimas, 1992) dual-ported RAAM architecture. 216 sentence pairs, 75%
  accuracy on test set.

- (Forcada and Neco, 1997), (Castano and ?, ?)

- Issue: vanishing gradient, therefore could not train a model.

- (Kalchbrener and Blunsom, 2013), (Sutskever et al., 2014),
  (Cho et al., 2014). Encoder-decoder networks.

## Sequence-to-sequence learning - encoder

- Encoder:

        1. 1-of-K coding of _source_ words.

        2. Continuous space representation.
                - st' = W'xt', where W \in R^{|V| x d}

        3. Recursively read words.

- Decoder:

        1. Recursively update memory: zt' = f(z{t'-1}, u{t'-1}, hT)

        2. Compute the next word prob.

        3. Sample a next word.
                - Beam search.

- At every step in decoding, use the compressed, summarized source input.

- A sufficiently deep encoder-decoder model can match the state of the art
  statistical machine translation models (Sutskever et al., 2014).

- Overfitting: what is the model capacity of a neural net? Affected by:

        - # of parameters

        - Depth

- Deterministic or stochastic? Stochastic (beam search etc.?)

- Segmenting sequences of characters into sequences of words, largely the same
  problem as segmenting sequences of words into sentences.

- RNNs can handle sentences hundreds of words long.

- Issue with encoder-decoder: assumes that you can fit the entire meaning of
  the sentence into one vector. - Ray Mooney

## Attention/alignment-based neural MT systems

- Encoder: bidirectional RNN

        - A set of annotation vectors {h1, ..., hT}

- Attention-based decoder:

        1. Compute attention weights.

        2. Weighted-sum of the annotation vectors: at',t \propto exp(e(z{t'-1}, u{t'-1}, ht))

        3. Use ct' instead of hT.

- Weights of annotation vectors re-computed at each step of the output
  sequence.

- Therefore alignments are learned.

- Arabic: interesting morphological structure that makes machine translation
  difficult. -> character-level translation. Unicode as character in CJK?

## Decoding sampling strategies

- Ancestral sampling (inefficient).

        - Generate N samples from the distribution, and choose argmax.

- Greedy search: pick the most likely symbol at each timestep.

        - Sub-optimal, because each selection doesn't consider future symbols.

        - x't = argmax{x} log p(x|x<t, Y)

- Beam search:

        - Maintain K hypotheses at a time.

        - Expand each hypothesis.

        - Pick top-K hypotheses from the union Ht = \union{k=1}{K} Bk, where
          Bk = argmax{X' \in Ak} log p(X'|Y)

                - Practice is to use 5-20 chains.

        - BLEU score has a peak, i.e. optimal value for K number of chains.

- Monte-Carlo tree search

- Intermediate between greedy search and beam search?

- Pruning out beam search?

- Keeping beams that are within a certain range of the most likely hypothesis
  at each step.

## Very large target vocabulary (Jean et al., 2015)

- Most time spent and most memory spent in operations proportional to
  vocabulary size times dimensionality of latent vector.

- Importance sampling _without sampling_ (becomes biased)

- Use all the words from the minibatch, plus some set of very frequent words.

- I.e. use V' such that |V'| << |V|

        - Training time:

                - Divide training corpus into D subsets.

                - Separately build vocabulary V' for each subset.

        - Test time:

                - K-most frequent words.

                - K' words aligned to each source word.

## InterLingua 2.0

- Multi-way, multilingual machine translation.

- Encoder: project a source sentence into a set of continuous vectors.

        - Language agnostic

- Want near-bijective mapping between continuous vector space and sentence
  space.

- (Firat et al., 2016; Luong et al., 2015; Dong et al., 2015)

- One encoder for each source language, one decoder for each target language,
  and _shared_ attention mechanism.

- Must be able to be trained with bilingual corpa only.

- Hope that transfer learning occurs from high-resource language pairs to
  language pairs for which we have low resources.

- 6 languages in -> 6 languages out (Firat et al., 2016a)

- Remaining questions:

        1. What is it good for, other than parameter saving?

        2. What if a source sentence is given in multiple input languages?

        3. Transfer to low resource language pair settings?
           (Firat et al., 2016b) 17.28 -> 22.56 BLEU score by adding high
           resource languages.

           - Improvements may come from the better decoder coming from more
             data, therefore this needs to be controlled for.

- If data has noisy source sentence, training is robust. But not robust to
  noisy target sentence.

- FAIR Paris training encoder-decoder model for sentence retrieval.

- Can we explicitly split the hidden vector space into a shared space and
  subspace for given language pairs?

- Para-phrasing: datasets built with neural MT model. Unclear how to train a
  para-phrase model or how to evaluate.

        - Para-phrase detection datasets exist.

- BLEU score evaluation:

        - Take source: 一只猫坐在毯子上
        - Target: the cat is sitting on the mat.
        - Translation: A cat is sitting on the couch.
        - Look at unigram and bigram matches.
        - Multiple references.
        - Correlates to human evaluation.

## Multi-source translation with multi-way, multilingual neural MT

- ?

## Multimodal translation

- (Xu et al, 2015) image captioning.

- Feature maps produced by convolution analogous to bi-directional annotation
  vectors computed by attention-based decoder.

- Multilingual translation system where one of the inputs is an image
  (multi-modal translation system). -> no improvements so far.

- (Caglayan et al., 2016; Elliott and Kadar, 2017)
