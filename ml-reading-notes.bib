@article{Srivastava:2014:DSW:2627435.2670313,
        author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
        title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
        journal = {J. Mach. Learn. Res.},
        issue_date = {January 2014},
        volume = {15},
        number = {1},
        month = jan,
        year = {2014},
        issn = {1532-4435},
        pages = {1929--1958},
        numpages = {30},
        url = {http://dl.acm.org/citation.cfm?id=2627435.2670313},
        acmid = {2670313},
        publisher = {JMLR.org},
        keywords = {deep learning, model combination, neural networks, regularization},
}

@article{Nowlan:1992:SNN:148167.148169,
        author = {Nowlan, Steven J. and Hinton, Geoffrey E.},
        title = {Simplifying Neural Networks by Soft Weight-sharing},
        journal = {Neural Comput.},
        issue_date = {July 1992},
        volume = {4},
        number = {4},
        month = jul,
        year = {1992},
        issn = {0899-7667},
        pages = {473--493},
        numpages = {21},
        url = {http://dx.doi.org/10.1162/neco.1992.4.4.473},
        doi = {10.1162/neco.1992.4.4.473},
        acmid = {148169},
        publisher = {MIT Press},
        address = {Cambridge, MA, USA},
}

@article{DBLP:journals/corr/ToshevS13,
  author    = {Alexander Toshev and Christian Szegedy},
  title     = {DeepPose: Human Pose Estimation via Deep Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1312.4659},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.4659},
  timestamp = {Mon, 06 Jan 2014 15:10:41 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ToshevS13},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@unpublished{Goodfellow-et-al-2016-Book,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    note={Book in preparation for MIT Press},
    url={http://www.deeplearningbook.org},
    year={2016}
}

@incollection{NIPS2013_5207,
        title = {Deep Neural Networks for Object Detection},
        author = {Szegedy, Christian and Toshev, Alexander and Erhan, Dumitru},
        booktitle = {Advances in Neural Information Processing Systems 26},
        editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
        pages = {2553--2561},
        year = {2013},
        publisher = {Curran Associates, Inc.},
        url = {http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf}
}

@article{DBLP:journals/corr/GirshickDDM13,
  author    = {Ross B. Girshick and
               Jeff Donahue and
               Trevor Darrell and
               Jitendra Malik},
  title     = {Rich feature hierarchies for accurate object detection and semantic
               segmentation},
  journal   = {CoRR},
  volume    = {abs/1311.2524},
  year      = {2013},
  url       = {http://arxiv.org/abs/1311.2524},
  timestamp = {Tue, 03 Dec 2013 15:04:21 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/GirshickDDM13},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@incollection{NIPS2012_4824,
        title = {ImageNet Classification with Deep Convolutional Neural Networks},
        author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
        booktitle = {Advances in Neural Information Processing Systems 25},
        editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
        pages = {1097--1105},
        year = {2012},
        publisher = {Curran Associates, Inc.},
        url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@incollection{NIPS2010_4143,
        title = {Pose-Sensitive Embedding by Nonlinear NCA Regression},
        author = {Graham W. Taylor and Fergus, Rob and George Williams and Ian Spiro and Bregler, Christoph},
        booktitle = {Advances in Neural Information Processing Systems 23},
        editor = {J. D. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R. S. Zemel and A. Culotta},
        pages = {2280--2288},
        year = {2010},
        publisher = {Curran Associates, Inc.},
        url = {http://papers.nips.cc/paper/4143-pose-sensitive-embedding-by-nonlinear-nca-regression.pdf}
}

@incollection{NIPS2004_2566,
        title = {Neighbourhood Components Analysis},
        author = {Jacob Goldberger and Hinton, Geoffrey E and Sam T. Roweis and Salakhutdinov, Ruslan R},
        booktitle = {Advances in Neural Information Processing Systems 17},
        editor = {L. K. Saul and Y. Weiss and L. Bottou},
        pages = {513--520},
        year = {2005},
        publisher = {MIT Press},
        url = {http://papers.nips.cc/paper/2566-neighbourhood-components-analysis.pdf}
}

@inproceedings {andriluka-2d-2014-853,
        title = {2D Human Pose Estimation: New Benchmark and State of the Art Analysis},
        booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
        year = {2014},
        month = {June},
        attachments = {https://www.d2.mpi-inf.mpg.de/sites/default/files/andriluka14cvpr.pdf , https://www.d2.mpi-inf.mpg.de/sites/default/files/supplementary_material.pdf},
        author = {Mykhaylo Andriluka and Leonid Pishchulin and Peter Gehler and Schiele, Bernt}
}

@article{DBLP:journals/corr/StewartA15,
        author    = {Russell Stewart and Mykhaylo Andriluka},
        title     = {End-to-end people detection in crowded scenes},
        journal   = {CoRR},
        volume    = {abs/1506.04878},
        year      = {2015},
        url       = {http://arxiv.org/abs/1506.04878},
        timestamp = {Wed, 01 Jul 2015 15:10:24 +0200},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/StewartA15},
        bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{DBLP:conf/accv/RotheGG14,
        author    = {Rasmus Rothe and Matthieu Guillaumin and Luc J. Van Gool},
        title     = {Non-maximum Suppression for Object Detection by Passing Messages Between Windows},
        booktitle = {Computer Vision - {ACCV} 2014 - 12th Asian Conference on Computer Vision, Singapore, Singapore, November 1-5, 2014, Revised Selected Papers, Part {I}},
        pages     = {290--306},
        year      = {2014},
        url       = {http://dx.doi.org/10.1007/978-3-319-16865-4_19},
        doi       = {10.1007/978-3-319-16865-4_19},
        timestamp = {Mon, 05 Sep 2016 10:00:01 +0200},
        biburl    = {http://dblp.uni-trier.de/rec/bib/conf/accv/RotheGG14},
        bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{going-deeper-szegedy43022,
        title = {Going Deeper with Convolutions},
        author  = {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre
                   Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan
                   and Vincent Vanhoucke and Andrew Rabinovich},
        abstract = {We propose a deep convolutional neural network architecture
                    codenamed "Inception", which was responsible for setting the
                    new state of the art for classification and detection
                    in the ImageNet Large-Scale Visual Recognition
                    Challenge 2014 (ILSVRC 2014). The main hallmark of this
                    architecture is the improved utilization of the
                    computing resources inside the network. This was
                    achieved by a carefully crafted design that allows for
                    increasing the depth and width of the network while
                    keeping the computational budget constant. To optimize
                    quality, the architectural decisions were based on the
                    Hebbian principle and the intuition of multi-scale
                    processing. One particular incarnation used in our
                    submission for ILSVRC 2014 is called GoogLeNet, a 22
                    layers deep network, the quality of which is assessed
                    in the context of classification and detection.},
        comments = {Suggest as future work to automate use of the principles
                    used to design GoogLeNet to find architectures for other
                    applications.},
        year  = 2015,
        URL = {http://arxiv.org/abs/1409.4842},
        booktitle = {Computer Vision and Pattern Recognition (CVPR)}
}

@article{DBLP:journals/corr/SermanetEZMFL13,
        author    = {Pierre Sermanet and
                David Eigen and
                        Xiang Zhang and
                        Micha{\"{e}}l Mathieu and
                        Rob Fergus and
                        Yann LeCun},
        title     = {OverFeat: Integrated Recognition, Localization and Detection using
                Convolutional Networks},
        journal   = {CoRR},
        volume    = {abs/1312.6229},
        year      = {2013},
        url       = {http://arxiv.org/abs/1312.6229},
        timestamp = {Mon, 06 Jan 2014 15:10:41 +0100},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/SermanetEZMFL13},
        bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{VisualPhrases,
        author = {Sadeghi, Mohammad Amin and Farhadi, Ali},
        title = {Recognition using Visual Phrases},
        conferense = {Computer Vision and Pattern Recognition (CVPR)},
        year = {2011},
}

@article{TaAnSc_14:occluded,
        title = {Detection and Tracking of Occluded People},
        author = {Siyu Tang nd Mykhaylo Andriluka nd Bernt Schiele},
        year = {2014},
        date = {2014-01-01},
        journal = {International Journal of Computer Vision (IJCV)},
        volume = {110},
        number = {1},
        pages = {58--69},
        publisher = {Springer}
}

@article{Felzenszwalb:2010:ODD:1850486.1850574,
        author = {Felzenszwalb, Pedro F. and Girshick, Ross B. and McAllester, David and Ramanan, Deva},
        title = {Object Detection with Discriminatively Trained Part-Based Models},
        journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
        issue_date = {September 2010},
        volume = {32},
        number = {9},
        month = sep,
        year = {2010},
        issn = {0162-8828},
        pages = {1627--1645},
        numpages = {19},
        url = {http://dx.doi.org/10.1109/TPAMI.2009.167},
        doi = {10.1109/TPAMI.2009.167},
        acmid = {1850574},
        publisher = {IEEE Computer Society},
        address = {Washington, DC, USA},
        keywords = {Object recognition, Object recognition, deformable models, pictorial structures, discriminative training, latent SVM., deformable models, discriminative training, latent SVM., pictorial structures},
}

@inproceedings{Leibe:2005:PDC:1068507.1069006,
        author = {Leibe, Bastian and Seemann, Edgar and Schiele, Bernt},
        title = {Pedestrian Detection in Crowded Scenes},
        booktitle = {Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) - Volume 1 - Volume 01},
        series = {CVPR '05},
        year = {2005},
        isbn = {0-7695-2372-2},
        pages = {878--885},
        numpages = {8},
        url = {http://dx.doi.org/10.1109/CVPR.2005.272},
        doi = {10.1109/CVPR.2005.272},
        acmid = {1069006},
        publisher = {IEEE Computer Society},
        address = {Washington, DC, USA},
}

@ARTICLE{Uijlings13,
        author = {J.R.R. Uijlings and K.E.A. van de Sande and T. Gevers and A.W.M.
                Smeulders},
        title = {Selective Search for Object Recognition},
        journal = {International Journal of Computer Vision},
        year = {2013},
        doi = {10.1007/s11263-013-0620-5},
        owner = {jrruijli},
        timestamp = {2013.02.06},
        url = {http://www.huppelen.nl/publications/selectiveSearchDraft.pdf}
}

@article{DBLP:journals/corr/ZhangBS15,
        author    = {Shanshan Zhang and Rodrigo Benenson and Bernt Schiele},
        title     = {Filtered Channel Features for Pedestrian Detection},
        journal   = {CoRR},
        volume    = {abs/1501.05759},
        year      = {2015},
        url       = {http://arxiv.org/abs/1501.05759},
        timestamp = {Mon, 02 Feb 2015 14:12:25 +0100},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ZhangBS15},
        bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{DBLP:journals/corr/SzegedyREA14,
        author    = {Christian Szegedy and Scott E. Reed and Dumitru Erhan and Dragomir Anguelov},
        title     = {Scalable, High-Quality Object Detection},
        journal   = {CoRR},
        volume    = {abs/1412.1441},
        year      = {2014},
        url       = {http://arxiv.org/abs/1412.1441},
        timestamp = {Fri, 08 Apr 2016 07:34:30 +0200},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/SzegedyREA14},
        bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{DBLP:journals/corr/KarpathyF14,
        author    = {Andrej Karpathy and Fei{-}Fei Li},
        title     = {Deep Visual-Semantic Alignments for Generating Image Descriptions},
        journal   = {CoRR},
        volume    = {abs/1412.2306},
        year      = {2014},
        url       = {http://arxiv.org/abs/1412.2306},
        timestamp = {Mon, 01 Jun 2015 08:26:33 +0200},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/KarpathyF14},
        bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{DBLP:journals/corr/SutskeverVL14,
        author    = {Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
        title     = {Sequence to Sequence Learning with Neural Networks},
        journal   = {CoRR},
        volume    = {abs/1409.3215},
        year      = {2014},
        url       = {http://arxiv.org/abs/1409.3215},
        timestamp = {Wed, 01 Oct 2014 15:00:04 +0200},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/SutskeverVL14},
        bibsource = {dblp computer science bibliography, http://dblp.org}
}

@INPROCEEDINGS{Graves06connectionisttemporal,
        author = {Alex Graves and Santiago Fernández and Faustino Gomez},
        title = {Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks},
        abstract = {Many real-world sequence learning tasks require the
                    prediction of sequences of labels from noisy, unsegmented input
                    data. In speech recognition, for example, an acoustic
                    signal is transcribed into words or sub-word units.
                    Recurrent neural networks (RNNs) are powerful sequence
                    learners that would seem well suited to such tasks.
                    However, because they require pre-segmented training
                    data, and post-processing to transform their outputs
                    into label sequences, their applicability has so far
                    been limited. This paper presents a novel method for
                    training RNNs to label unsegmented sequences directly,
                    thereby solving both problems. An experiment on the TIMIT
                    speech corpus demonstrates its advantages over both a
                    baseline HMM and a hybrid HMM-RNN. 1.},
        booktitle = {In Proceedings of the International Conference on Machine Learning, ICML 2006},
        year = {2006},
        pages = {369--376}
}

@article{Hochreiter:1997:LSM:1246443.1246450,
        author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
        title = {Long Short-Term Memory},
        journal = {Neural Comput.},
        issue_date = {November 15, 1997},
        volume = {9},
        number = {8},
        month = nov,
        year = {1997},
        issn = {0899-7667},
        pages = {1735--1780},
        numpages = {46},
        url = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
        doi = {10.1162/neco.1997.9.8.1735},
        acmid = {1246450},
        publisher = {MIT Press},
        address = {Cambridge, MA, USA},
        abstract = {
                Learning to store information over extended time intervals by
                recurrent backpropagation takes a very long time,
                mostly because of insufficient, decaying error backflow. We
                briefly review Hochreiter's (1991) analysis of this
                problem, then address it by introducing a novel,
                efficient, gradient based method called long short-term memory
                (LSTM).  Truncating the gradient where this does not do
                harm, LSTM can learn to bridge minimal time lags in
                excess of 1000 discrete-time steps by enforcing
                constant error flow through constant error carousels
                within special units.  Multiplicative gate units learn
                to open and close access to the constant error flow.
                LSTM is local in space and time; its computational
                complexity per time step and weight is O. 1. Our
                experiments with artificial data involve local,
                distributed, real-valued, and noisy pattern representations. In
                comparisons with real-time recurrent learning, back
                propagation through time, recurrent cascade
                correlation, Elman nets, and neural sequence chunking,
                LSTM leads to many more successful runs, and learns much
                faster. LSTM also solves complex, artificial
                long-time-lag tasks that have never been solved by
                previous recurrent network algorithms.
        }
}

@article{DBLP:journals/corr/HeZRS15,
        author    = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian
                     Sun},
        title     = {Deep Residual Learning for Image Recognition},
        journal   = {CoRR},
        volume    = {abs/1512.03385},
        year      = {2015},
        url       = {http://arxiv.org/abs/1512.03385},
        timestamp = {Wed, 30 Mar 2016 23:40:00 +0200},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/HeZRS15},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                Deeper neural networks are more difficult to train. We present a residual
                learning framework to ease the training of networks that are
                substantially deeper than those used previously. We explicitly
                reformulate the layers as learning residual functions with
                reference to the layer inputs, instead of learning unreferenced
                functions. We provide comprehensive empirical evidence showing
                that these residual networks are easier to optimize, and can
                gain accuracy from considerably increased depth. On the
                ImageNet dataset we evaluate residual nets with a depth of up
                to 152 layers---8x deeper than VGG nets but still having lower
                complexity. An ensemble of these residual nets achieves 3.57%
                error on the ImageNet test set. This result won the 1st place
                on the ILSVRC 2015 classification task. We also present
                analysis on CIFAR-10 with 100 and 1000 layers.

                The depth of representations is of central importance for many
                visual recognition tasks. Solely due to our extremely deep
                representations, we obtain a 28% relative improvement on the
                COCO object detection dataset. Deep residual nets are
                foundations of our submissions to ILSVRC & COCO 2015
                competitions, where we also won the 1st places on the tasks of
                ImageNet detection, ImageNet localization, COCO detection, and
                COCO segmentation.
        },
        comments = {
                Presents a way of training deeper networks via residual
                functions. Won ILSVRC 2015 CLS. Key insight is that deeper
                networks can be trained by trying to represent residual
                function H(x) - x rather than representing H(x) itself.
        }
}

@article{DBLP:journals/corr/SimonyanZ14a,
        author    = {Karen Simonyan and Andrew Zisserman},
        title     = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
        journal   = {CoRR},
        volume    = {abs/1409.1556},
        year      = {2014},
        url       = {http://arxiv.org/abs/1409.1556},
        timestamp = {Wed, 01 Oct 2014 15:00:05 +0200},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/SimonyanZ14a},
        bibsource = {dblp computer science bibliography, http://dblp.org}
}

@incollection{NIPS2014_5423,
        title = {Generative Adversarial Nets},
        author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and
                  Xu, Bing and Warde-Farley, David and Ozair, Sherjil and
                  Courville, Aaron and Bengio, Yoshua},
        booktitle = {Advances in Neural Information Processing Systems 27},
        editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence
                and K. Q. Weinberger},
        pages = {2672--2680},
        year = {2014},
        publisher = {Curran Associates, Inc.},
        url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
        abstract = {
                We propose a new framework for estimating generative models via
                an adversarial process, in which we simultaneously train two
                models: a generative model G that captures the data
                distribution, and a discriminative model D that estimates the
                probability that a sample came from the training data rather
                than G. The training procedure for G is to maximize the
                probability of D making a mistake. This framework corresponds
                to a minimax two-player game. In the space of arbitrary
                functions G and D, a unique solution exists, with G recovering
                the training data distribution and D equal to 1/2 everywhere.
                In the case where G and D are defined by multilayer
                perceptrons, the entire system can be trained with
                backpropagation. There is no need for any Markov chains or
                unrolled approximate inference networks during either training
                or generation of samples. Experiments demonstrate the potential
                of the framework through qualitative and quantitative
                evaluation of the generated samples.
        },
        comments = {
                Original paper presenting GANs. Presents GAN algorithm, and
                formally proves that the optimal discriminator guesses randomly
                between the data distribution and the generated distribution.
                Shows qualitative generative results on CIFAR-10 using MLPs and
                ConvNets. Suggests issues with GANs such as no exact model of
                the probability distribution p_g produced by the generator.
        }
}

@article{deepSpeechReviewSPM2012,
        author={Geoffrey E. Hinton and Li Deng and Dong Yu and George E. Dahl
                and Abdel{-}rahman Mohamed and Navdeep Jaitly and Andrew Senior
                and Vincent Vanhoucke and Patrick Nguyen and Tara N. Sainath
                and Brian Kingsbury},
        title     = {Deep Neural Networks for Acoustic Modeling in Speech Recognition:
                     The Shared Views of Four Research Groups},
        journal   = {IEEE Signal Process. Mag.},
        volume    = {29},
        number    = {6},
        year      = {2012},
        pages     = {82-97},
        abstract = {
        }
}

@inproceedings{icml2010_NairH10,
        Publisher = {Omnipress},
        Title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
        Url = {http://www.icml2010.org/papers/432.pdf},
        Booktitle = {Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
        Author = {Vinod Nair and Geoffrey E. Hinton},
        Editor = {Johannes Fürnkranz and Thorsten Joachims},
        Year = {2010},
        Pages = {807-814},
        abstract = {
              Restricted Boltzmann machines were devel- oped using binary
              stochastic hidden units.  These can be generalized by
              replacing each binary unit by an infinite number of
              copies that all have the same weights but have pro-
              gressively more negative biases. The learning and
              inference rules for these “Stepped Sig- moid Units” are
              unchanged. They can be ap- proximated efficiently by
              noisy, rectified lin- ear units. Compared with binary
              units, these units learn features that are better for
              object recognition on the NORB dataset and face
              verification on the Labeled Faces in the Wild dataset.
              Unlike binary units, rectified linear units preserve
              information about relative in- tensities as information
              travels through mul- tiple layers of feature detectors.
        }
   }

@article{DBLP:journals/corr/IsolaZZE16,
        author    = {Phillip Isola and
                Jun{-}Yan Zhu and
                        Tinghui Zhou and
                        Alexei A. Efros},
        title     = {Image-to-Image Translation with Conditional Adversarial Networks},
        journal   = {CoRR},
        volume    = {abs/1611.07004},
        year      = {2016},
        url       = {http://arxiv.org/abs/1611.07004},
        timestamp = {Thu, 01 Dec 2016 19:32:08 +0100},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/IsolaZZE16},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                We investigate conditional adversarial networks as a
                general-purpose solution to image-to-image translation
                problems. These networks not only learn the mapping
                from input image to output image, but also learn a loss
                function to train this mapping. This makes it possible
                to apply the same generic approach to problems that
                traditionally would require very different loss
                formulations. We demonstrate that this approach is
                effective at synthesizing photos from label maps,
                reconstructing objects from edge maps, and colorizing images,
                among other tasks. As a community, we no longer hand-engineer
                our mapping functions, and this work suggests we can
                achieve reasonable results without hand-engineering our
                loss functions either.
        },
        comments = {
                Extends the idea of GANs from (Goodfellow et al., 2014) to
                cGANs (conditional GANs) and applies cGANs to various
                image-to-image translation tasks, such as generating images
                from segmentation masks or edges.

                Shows that for generative structured graphical tasks, it is
                possible to learn a loss for the generative network in the form
                of the discriminator loss. The discriminator convolves over and
                tries to classify whether NxN regions of the output are samples
                from the ground truth distribution or from the generated
                distribution. Optimal N is found to be 70px.

                It is left as an open question to figure out how to generate
                stochastic outputs so as to mimic the true distribution of
                outputs.
        }
}

@article{maas_rectified_nonlinearities,
        author = {
                Andrew L. Maas and Awni Y. Hannun and Andrew Y. Ng
        },
        title = {Rectifier Nonlinearities Improve Neural Network Acoustic
                 Models},
        journal = {ICML},
        year = {2013},
        url = {http://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf},
        abstract = {
                Deep neural network acoustic models produce
                substantial gains in large vocabulary
                continuous speech recognition systems.
                Emerging work with rectified linear (ReL)
                hidden units demonstrates additional gains
                in final system performance relative to more
                commonly used sigmoidal nonlinearities. In
                this work, we explore the use of deep rectifier
                networks as acoustic models for the 300 hour
                Switchboard conversational speech recognition
                task. Using simple training procedures
                without pretraining, networks with rectifier
                nonlinearities produce 2% absolute reductions
                in word error rates over their sigmoidal
                counterparts. We analyze hidden layer representations
                to quantify differences in how ReL
                units encode inputs as compared to sigmoidal
                units. Finally, we evaluate a variant of the
                ReL unit with a gradient more amenable to
                optimization in an attempt to further improve
                deep rectifier networks.
        },
        comments = {
                Introduces Leaky ReLUs to deal with vanishing gradient problem
                (leaky ReLUs have non-zero gradient over their domain). Leaky
                ReLUs are found to perform similarly to ReLUs, and to converge
                slightly faster.
        }
}

@article{DBLP:journals/corr/PathakKDDE16,
        author    = {Deepak Pathak and
                     Philipp Kr{\"{a}}henb{\"{u}}hl and
                     Jeff Donahue and
                     Trevor Darrell and
                     Alexei A. Efros},
        title     = {Context Encoders: Feature Learning by Inpainting},
        journal   = {CoRR},
        volume    = {abs/1604.07379},
        year      = {2016},
        url       = {http://arxiv.org/abs/1604.07379},
        timestamp = {Mon, 02 May 2016 18:22:52 +0200},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/PathakKDDE16},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                We present an unsupervised visual feature learning algorithm driven by
                context-based pixel prediction. By analogy with auto-encoders,
                we propose Context Encoders -- a convolutional neural network
                trained to generate the contents of an arbitrary image region
                conditioned on its surroundings. In order to succeed at this
                task, context encoders need to both understand the content of
                the entire image, as well as produce a plausible hypothesis for
                the missing part(s).  When training context encoders, we have
                experimented with both a standard pixel-wise reconstruction
                loss, as well as a reconstruction plus an adversarial loss. The
                latter produces much sharper results because it can better
                handle multiple modes in the output. We found that a context
                encoder learns a representation that captures not just
                appearance but also the semantics of visual structures. We
                quantitatively demonstrate the effectiveness of our learned
                features for CNN pre-training on classification, detection, and
                segmentation tasks. Furthermore, context encoders can be used
                for semantic inpainting tasks, either stand-alone or as
                initialization for non-parametric methods.
        }
}

@article{DBLP:journals/corr/ZhangIE16,
        author    = {Richard Zhang and
                Phillip Isola and
                        Alexei A. Efros},
        title     = {Colorful Image Colorization},
        journal   = {CoRR},
        volume    = {abs/1603.08511},
        year      = {2016},
        url       = {http://arxiv.org/abs/1603.08511},
        timestamp = {Sat, 02 Apr 2016 11:49:48 +0200},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ZhangIE16},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                Given a grayscale photograph as input, this paper attacks the problem of
                hallucinating a plausible color version of the photograph. This problem
                is clearly underconstrained, so previous approaches have either relied
                on significant user interaction or resulted in desaturated
                colorizations. We propose a fully automatic approach that produces
                vibrant and realistic colorizations. We embrace the underlying
                uncertainty of the problem by posing it as a classification task and
                use class-rebalancing at training time to increase the diversity of
                colors in the result. The system is implemented as a feed-forward pass
                in a CNN at test time and is trained on over a million color images. We
                evaluate our algorithm using a "colorization Turing test," asking human
                participants to choose between a generated and ground truth color
                image. Our method successfully fools humans on 32% of the trials,
                significantly higher than previous methods. Moreover, we show that colorization
                can be a powerful pretext task for self-supervised feature learning,
                acting as a cross-channel encoder. This approach results in state-of-the-art
                performance on several feature learning benchmarks.
        }
}

@article{DBLP:journals/corr/DentonCSF15,
        author    = {Emily L. Denton and
                Soumith Chintala and
                        Arthur Szlam and
                        Robert Fergus},
        title     = {Deep Generative Image Models using a Laplacian Pyramid of Adversarial
                Networks},
        journal   = {CoRR},
        volume    = {abs/1506.05751},
        year      = {2015},
        url       = {http://arxiv.org/abs/1506.05751},
        timestamp = {Wed, 01 Jul 2015 15:10:24 +0200},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/DentonCSF15},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                In this paper we introduce a generative parametric model capable of producing
                high quality samples of natural images. Our approach uses a cascade of
                convolutional networks within a Laplacian pyramid framework to generate
                images in a coarse-to-fine fashion. At each level of the pyramid, a
                separate generative convnet model is trained using the Generative
                Adversarial Nets (GAN) approach (Goodfellow et al.). Samples drawn from
                our model are of significantly higher quality than alternate
                approaches. In a quantitative assessment by human evaluators, our
                CIFAR10 samples were mistaken for real images around 40% of the time,
                compared to 10% for samples drawn from a GAN baseline model. We also show
                samples from models trained on the higher resolution images of the LSUN
                scene dataset.
        }
}

@article{DBLP:journals/corr/RadfordMC15,
        author    = {Alec Radford and
                Luke Metz and
                        Soumith Chintala},
        title     = {Unsupervised Representation Learning with Deep Convolutional Generative
                Adversarial Networks},
        journal   = {CoRR},
        volume    = {abs/1511.06434},
        year      = {2015},
        url       = {http://arxiv.org/abs/1511.06434},
        timestamp = {Tue, 01 Dec 2015 19:22:34 +0100},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/RadfordMC15},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                In recent years, supervised learning with convolutional networks (CNNs) has
                seen huge adoption in computer vision applications. Comparatively,
                unsupervised learning with CNNs has received less attention. In this work we
                hope to help bridge the gap between the success of CNNs for supervised
                learning and unsupervised learning. We introduce a class of CNNs called
                deep convolutional generative adversarial networks (DCGANs), that have
                certain architectural constraints, and demonstrate that they are a
                strong candidate for unsupervised learning. Training on various image
                datasets, we show convincing evidence that our deep convolutional
                adversarial pair learns a hierarchy of representations from object
                parts to scenes in both the generator and discriminator. Additionally,
                we use the learned features for novel tasks - demonstrating their applicability
                as general image representations.
        }
}

@article{DBLP:journals/corr/SalimansGZCRC16,
        author    = {Tim Salimans and
                Ian J. Goodfellow and
                        Wojciech Zaremba and
                        Vicki Cheung and
                        Alec Radford and
                        Xi Chen},
        title     = {Improved Techniques for Training GANs},
        journal   = {CoRR},
        volume    = {abs/1606.03498},
        year      = {2016},
        url       = {http://arxiv.org/abs/1606.03498},
        timestamp = {Fri, 01 Jul 2016 17:39:49 +0200},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/SalimansGZCRC16},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                We present a variety of new architectural features and training procedures that
                we apply to the generative adversarial networks (GANs) framework. We
                focus on two applications of GANs: semi-supervised learning, and the
                generation of images that humans find visually realistic. Unlike most
                work on generative models, our primary goal is not to train a model
                that assigns high likelihood to test data, nor do we require the model
                to be able to learn well without using any labels. Using our new
                techniques, we achieve state-of-the-art results in semi-supervised
                classification on MNIST, CIFAR-10 and SVHN. The generated images are of
                high quality as confirmed by a visual Turing test: our model generates
                MNIST samples that humans cannot distinguish from real data, and
                CIFAR-10 samples that yield a human error rate of 21.3%. We also
                present ImageNet samples with unprecedented resolution and show that
                our methods enable the model to learn recognizable features of ImageNet
                classes.
        }
}

@article{DBLP:journals/corr/ZhaoML16,
        author    = {Junbo Jake Zhao and
                Micha{\"{e}}l Mathieu and
                        Yann LeCun},
        title     = {Energy-based Generative Adversarial Network},
        journal   = {CoRR},
        volume    = {abs/1609.03126},
        year      = {2016},
        url       = {http://arxiv.org/abs/1609.03126},
        timestamp = {Fri, 16 Dec 2016 19:42:02 +0100},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ZhaoML16},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                We introduce the "Energy-based Generative Adversarial Network" model (EBGAN)
                which views the discriminator as an energy function that attributes low
                energies to the regions near the data manifold and higher energies to
                other regions. Similar to the probabilistic GANs, a generator is seen
                as being trained to produce contrastive samples with minimal energies,
                while the discriminator is trained to assign high energies to these generated
                samples. Viewing the discriminator as an energy function allows to use
                a wide variety of architectures and loss functionals in
                addition to the usual binary classifier with logistic output.
                Among them, we show one instantiation of EBGAN framework as
                using an auto-encoder architecture, with the energy being the
                reconstruction error, in place of the discriminator. We show
                that this form of EBGAN exhibits more stable behavior than
                regular GANs during training. We also show that a single-scale
                architecture can be trained to generate high-resolution images.
        }
}

@article{DBLP:journals/corr/ShelhamerLD16,
        author    = {Evan Shelhamer and
                Jonathan Long and
                        Trevor Darrell},
        title     = {Fully Convolutional Networks for Semantic Segmentation},
        journal   = {CoRR},
        volume    = {abs/1605.06211},
        year      = {2016},
        url       = {http://arxiv.org/abs/1605.06211},
        timestamp = {Wed, 01 Jun 2016 15:51:08 +0200},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ShelhamerLD16},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                Convolutional networks are powerful visual models that yield hierarchies of
                features. We show that convolutional networks by themselves, trained
                end-to-end, pixels-to-pixels, improve on the previous best result in
                semantic segmentation. Our key insight is to build "fully
                convolutional" networks that take input of arbitrary size and produce
                correspondingly-sized output with efficient inference and learning. We
                define and detail the space of fully convolutional networks, explain
                their application to spatially dense prediction tasks, and draw
                connections to prior models. We adapt contemporary classification
                networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional
                networks and transfer their learned representations by fine-tuning to
                the segmentation task. We then define a skip architecture that combines
                semantic information from a deep, coarse layer with appearance
                information from a shallow, fine layer to produce accurate and detailed
                segmentations. Our fully convolutional network achieves improved
                segmentation of PASCAL VOC (30% relative improvement to 67.2% mean IU
                on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes
                one tenth of a second for a typical image.
        }
}

@article{DBLP:journals/corr/XieT15,
        author    = {Saining Xie and
                Zhuowen Tu},
        title     = {Holistically-Nested Edge Detection},
        journal   = {CoRR},
        volume    = {abs/1504.06375},
        year      = {2015},
        url       = {http://arxiv.org/abs/1504.06375},
        timestamp = {Sat, 02 May 2015 17:50:32 +0200},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/XieT15},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                We develop a new edge detection algorithm that tackles two important issues in
                this long-standing vision problem: (1) holistic image training and
                prediction; and (2) multi-scale and multi-level feature learning. Our
                proposed method, holistically-nested edge detection (HED), performs
                image-to-image prediction by means of a deep learning model that
                leverages fully convolutional neural networks and deeply-supervised
                nets. HED automatically learns rich hierarchical representations
                (guided by deep supervision on side responses) that are important in
                order to approach the human ability resolve the challenging ambiguity
                in edge and object boundary detection. We significantly advance the
                state-of-the-art on the BSD500 dataset (ODS F-score of .782) and the
                NYU Depth dataset (ODS F-score of .746), and do so with an improved
                speed (0.4 second per image) that is orders of magnitude faster than
                some recent CNN-based edge detection algorithms.
        }
}

@Article{IizukaSIGGRAPH2016,
        author  = {Satoshi Iizuka and Edgar Simo-Serra and Hiroshi Ishikawa},
        title   = {{Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification}},
        journal = {ACM Transactions on Graphics (Proc. of SIGGRAPH 2016)},
        year    = {2016},
        volume  = {35},
        number  = {4},
        pages   = {110:1--110:11},
        abstract = {
                We present a novel technique to automatically colorize grayscale images that
                combines both global priors and local image features. Based on
                Convolutional Neural Networks, our deep network features a fusion layer
                that allows us to elegantly merge local information dependent on small
                image patches with global priors computed using the entire image. The
                entire framework, including the global and local priors as well as the
                colorization model, is trained in an end-to-end fashion. Furthermore,
                our architecture can process images of any resolution, unlike most existing
                approaches based on CNN. We leverage an existing large-scale scene
                classification database to train our model, exploiting the class labels
                of the dataset to more efficiently and discriminatively learn the
                global priors. We validate our approach with a user study and compare
                against the state of the art, where we show significant improvements.
                Furthermore, we demonstrate our method extensively on many different
                types of images, including black-and-white photography from over a
                hundred years ago, and show realistic colorizations.
        }
}

@article{DBLP:journals/corr/LarssonMS16,
        author    = {Gustav Larsson and
                Michael Maire and
                        Gregory Shakhnarovich},
        title     = {Learning Representations for Automatic Colorization},
        journal   = {CoRR},
        volume    = {abs/1603.06668},
        year      = {2016},
        url       = {http://arxiv.org/abs/1603.06668},
        timestamp = {Sat, 02 Apr 2016 11:49:48 +0200},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/LarssonMS16},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                We develop a fully automatic image colorization system. Our approach leverages
                recent advances in deep networks, exploiting both low-level and
                semantic representations. As many scene elements naturally appear
                according to multimodal color distributions, we train our model to
                predict per-pixel color histograms. This intermediate output can be
                used to automatically generate a color image, or further manipulated
                prior to image formation. On both fully and partially automatic
                colorization tasks, we outperform existing methods. We also explore
                colorization as a vehicle for self-supervised visual representation
                learning.
        }
}

@article{DBLP:journals/corr/ChenPKMY14,
        author    = {Liang{-}Chieh Chen and
                George Papandreou and
                        Iasonas Kokkinos and
                        Kevin Murphy and
                        Alan L. Yuille},
        title     = {Semantic Image Segmentation with Deep Convolutional Nets and Fully
                Connected CRFs},
        journal   = {CoRR},
        volume    = {abs/1412.7062},
        year      = {2014},
        url       = {http://arxiv.org/abs/1412.7062},
        timestamp = {Fri, 08 Jan 2016 17:51:34 +0100},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ChenPKMY14},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art
                performance in high level vision tasks, such as image classification
                and object detection. This work brings together methods from DCNNs and
                probabilistic graphical models for addressing the task of pixel-level
                classification (also called "semantic image segmentation"). We show
                that responses at the final layer of DCNNs are not sufficiently
                localized for accurate object segmentation. This is due to the very
                invariance properties that make DCNNs good for high level tasks. We
                overcome this poor localization property of deep networks by combining
                the responses at the final DCNN layer with a fully connected
                Conditional Random Field (CRF). Qualitatively, our "DeepLab" system is
                able to localize segment boundaries at a level of accuracy which is
                beyond previous methods. Quantitatively, our method sets the new
                state-of-art at the PASCAL VOC-2012 semantic image segmentation task,
                reaching 71.6% IOU accuracy in the test set. We show how these results can be
                obtained efficiently: Careful network re-purposing and a novel
                application of the 'hole' algorithm from the wavelet community allow
                dense computation of neural net responses at 8 frames per second on a
                modern GPU.
        }
}

@article{DBLP:journals/corr/DosovitskiyB16,
        author    = {Alexey Dosovitskiy and
                Thomas Brox},
        title     = {Generating Images with Perceptual Similarity Metrics based on Deep
                Networks},
        journal   = {CoRR},
        volume    = {abs/1602.02644},
        year      = {2016},
        url       = {http://arxiv.org/abs/1602.02644},
        timestamp = {Tue, 01 Mar 2016 17:47:25 +0100},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/DosovitskiyB16},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                Image-generating machine learning models are typically trained with loss
                functions based on distance in the image space. This often leads to
                over-smoothed results. We propose a class of loss functions, which we
                call deep perceptual similarity metrics (DeePSiM), that mitigate this
                problem. Instead of computing distances in the image space, we compute
                distances between image features extracted by deep neural networks.
                This metric better reflects perceptually similarity of images and thus
                leads to better results. We show three applications: autoencoder
                training, a modification of a variational autoencoder, and inversion of
                deep convolutional networks. In all cases, the generated images look
                sharp and resemble natural images.
        }
}

@article{DBLP:journals/corr/LiW16,
        author    = {Chuan Li and
                Michael Wand},
        title     = {Combining Markov Random Fields and Convolutional Neural Networks for
                Image Synthesis},
        journal   = {CoRR},
        volume    = {abs/1601.04589},
        year      = {2016},
        url       = {http://arxiv.org/abs/1601.04589},
        timestamp = {Mon, 01 Feb 2016 15:36:05 +0100},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/LiW16},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                This paper studies a combination of generative Markov random
                field (MRF) models and discriminatively trained deep
                convolutional neural networks (dCNNs) for synthesizing 2D
                images. The generative MRF acts on higher-levels of a dCNN
                feature pyramid, controling the image layout at an abstract
                level. We apply the method to both photographic and
                non-photo-realistic (artwork) synthesis tasks. The MRF
                regularizer prevents over-excitation artifacts and reduces
                implausible feature mixtures common to previous dCNN inversion
                approaches, permitting synthezing photographic content with
                increased visual plausibility. Unlike standard MRF-based
                texture synthesis, the combined system can both match and adapt
                local features with considerable variability, yielding results
                far out of reach of classic generative MRF methods.
        }
}

@article{DBLP:journals/corr/XieHT15,
        author    = {Saining Xie and Xun Huang and Zhuowen Tu},
        title     = {Convolutional Pseudo-Prior for Structured Labeling},
        journal   = {CoRR},
        volume    = {abs/1511.07409},
        year      = {2015},
        url       = {http://arxiv.org/abs/1511.07409},
        timestamp = {Tue, 01 Dec 2015 19:22:34 +0100},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/XieHT15},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                Current practice in convolutional neural networks (CNN) remains
                largely bottom-up and the role of top-down process in CNN for
                pattern analysis and visual inference is not very clear. In
                this paper, we propose a new method for structured labeling by
                developing convolutional pseudo-prior (ConvPP) on the
                ground-truth labels. Our method has several interesting
                properties: (1) compared with classical machine learning
                algorithms like CRFs and Structural SVM, ConvPP automatically
                learns rich convolutional kernels to capture both short- and
                long- range contexts; (2) compared with cascade classifiers
                like Auto-Context, ConvPP avoids the iterative steps of
                learning a series of discriminative classifiers and
                automatically learns contextual configurations; (3) compared
                with recent efforts combing CNN models with CRFs and RNNs,
                ConvPP learns convolution in the labeling space with much
                improved modeling capability and less manual specification; (4)
                compared with Bayesian models like MRFs, ConvPP capitalizes on
                the rich representation power of convolution by automatically
                learning priors built on convolutional filters. We accomplish
                our task using pseudo-likelihood approximation to the prior
                under a novel fixed-point network structure that facilitates an
                end-to-end learning process. We show state-of-the-art results
                on sequential labeling and image labeling benchmarks.
        }
}

@article{DBLP:journals/corr/JohnsonAL16,
        author    = {Justin Johnson and
                     Alexandre Alahi and
                     Fei{-}Fei Li},
        title     = {Perceptual Losses for Real-Time Style Transfer and Super-Resolution},
        journal   = {CoRR},
        volume    = {abs/1603.08155},
        year      = {2016},
        url       = {http://arxiv.org/abs/1603.08155},
        timestamp = {Sat, 02 Apr 2016 11:49:48 +0200},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/JohnsonAL16},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                We consider image transformation problems, where an input image
                is transformed into an output image. Recent methods for such
                problems typically train feed-forward convolutional neural
                networks using a \emph{per-pixel} loss between the output and
                ground-truth images. Parallel work has shown that high-quality
                images can be generated by defining and optimizing
                \emph{perceptual} loss functions based on high-level features
                extracted from pretrained networks. We combine the benefits of
                both approaches, and propose the use of perceptual loss
                functions for training feed-forward networks for image
                transformation tasks. We show results on image style transfer,
                where a feed-forward network is trained to solve the
                optimization problem proposed by Gatys et al in real-time.
                Compared to the optimization-based method, our network gives
                similar qualitative results but is three orders of magnitude
                faster. We also experiment with single-image super-resolution,
                where replacing a per-pixel loss with a perceptual loss gives
                visually pleasing results.
        }
}

@article{DBLP:journals/corr/MirzaO14,
        author    = {Mehdi Mirza and
                     Simon Osindero},
        title     = {Conditional Generative Adversarial Nets},
        journal   = {CoRR},
        volume    = {abs/1411.1784},
        year      = {2014},
        url       = {http://arxiv.org/abs/1411.1784},
        timestamp = {Mon, 01 Dec 2014 14:32:13 +0100},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/MirzaO14},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                Generative Adversarial Nets [8] were recently introduced as a
                novel way to train generative models. In this work we introduce
                the conditional version of generative adversarial nets, which
                can be constructed by simply feeding the data, y, we wish to
                condition on to both the generator and discriminator. We show
                that this model can generate MNIST digits conditioned on class
                labels. We also illustrate how this model could be used to
                learn a multi-modal model, and provide preliminary examples of
                an application to image tagging in which we demonstrate how
                this approach can generate descriptive tags which are not part
                of training labels.
        }
}

@article{DBLP:journals/corr/ReedAYLSL16,
        author    = {Scott E. Reed and
                     Zeynep Akata and
                     Xinchen Yan and
                     Lajanugen Logeswaran and
                     Bernt Schiele and
                     Honglak Lee},
        title     = {Generative Adversarial Text to Image Synthesis},
        journal   = {CoRR},
        volume    = {abs/1605.05396},
        year      = {2016},
        url       = {http://arxiv.org/abs/1605.05396},
        timestamp = {Wed, 01 Jun 2016 15:51:08 +0200},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ReedAYLSL16},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                Automatic synthesis of realistic images from text would be
                interesting and useful, but current AI systems are still far
                from this goal. However, in recent years generic and powerful
                recurrent neural network architectures have been developed to
                learn discriminative text feature representations. Meanwhile,
                deep convolutional generative adversarial networks (GANs) have
                begun to generate highly compelling images of specific
                categories, such as faces, album covers, and room interiors. In
                this work, we develop a novel deep architecture and GAN
                formulation to effectively bridge these advances in text and
                image model- ing, translating visual concepts from characters
                to pixels. We demonstrate the capability of our model to
                generate plausible images of birds and flowers from detailed
                text descriptions.
        }
}

@article{DBLP:journals/corr/WangG16,
        author    = {Xiaolong Wang and
                     Abhinav Gupta},
        title     = {Generative Image Modeling using Style and Structure Adversarial Networks},
        journal   = {CoRR},
        volume    = {abs/1603.05631},
        year      = {2016},
        url       = {http://arxiv.org/abs/1603.05631},
        timestamp = {Sat, 02 Apr 2016 11:49:48 +0200},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/WangG16},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                Current generative frameworks use end-to-end learning and
                generate images by sampling from uniform noise distribution.
                However, these approaches ignore the most basic principle of
                image formation: images are product of: (a) Structure: the
                underlying 3D model; (b) Style: the texture mapped onto
                structure. In this paper, we factorize the image generation
                process and propose Style and Structure Generative Adversarial
                Network (S^2-GAN). Our S^2-GAN has two components: the
                Structure-GAN generates a surface normal map; the Style-GAN
                takes the surface normal map as input and generates the 2D
                image. Apart from a real vs. generated loss function, we use an
                additional loss with computed surface normals from generated
                images. The two GANs are first trained independently, and then
                merged together via joint learning. We show our S^2-GAN model
                is interpretable, generates more realistic images and can be
                used to learn unsupervised RGBD representations.
        }
}

@inproceedings{zhu2016generative,
        title={Generative Visual Manipulation on the Natural Image Manifold},
        author={Zhu, Jun-Yan and Kr{\"a}henb{\"u}hl, Philipp and Shechtman, Eli and Efros, Alexei A.},
        booktitle={Proceedings of European Conference on Computer Vision (ECCV)},
        year={2016},
        abstract = {
                Realistic image manipulation is challenging because it requires
                modifying the image appearance in a user-controlled way, while
                preserving the realism of the result. Unless the user has
                considerable artistic skill, it is easy to "fall off" the
                manifold of natural images while editing. In this paper, we
                propose to learn the natural image manifold directly from data
                using a generative adversarial neural network. We then define a
                class of image editing operations, and constrain their output
                to lie on that learned manifold at all times. The model
                automatically adjusts the output keeping all edits as realistic
                as possible. All our manipulations are expressed in terms of
                constrained optimization and are applied in near-real time. We
                evaluate our algorithm on the task of realistic photo
                manipulation of shape and color. The presented method can
                further be used for changing one image to look like the other,
                as well as generating novel imagery from scratch based on
                user's scribbles.
        }
}

@article{DBLP:journals/corr/MathieuCL15,
        author    = {Micha{\"{e}}l Mathieu and
                     Camille Couprie and
                     Yann LeCun},
        title     = {Deep multi-scale video prediction beyond mean square error},
        journal   = {CoRR},
        volume    = {abs/1511.05440},
        year      = {2015},
        url       = {http://arxiv.org/abs/1511.05440},
        timestamp = {Tue, 01 Dec 2015 19:22:34 +0100},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/MathieuCL15},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                Learning to predict future images from a video sequence
                involves the construction of an internal representation that
                models the image evolution accurately, and therefore, to some
                degree, its content and dynamics. This is why pixel-space video
                prediction may be viewed as a promising avenue for unsupervised
                feature learning. In addition, while optical flow has been a
                very studied problem in computer vision for a long time, future
                frame prediction is rarely approached. Still, many vision
                applications could benefit from the knowledge of the next
                frames of videos, that does not require the complexity of
                tracking every pixel trajectories. In this work, we train a
                convolutional network to generate future frames given an input
                sequence. To deal with the inherently blurry predictions
                obtained from the standard Mean Squared Error (MSE) loss
                function, we propose three different and complementary feature
                learning strategies: a multi-scale architecture, an adversarial
                training method, and an image gradient difference loss
                function. We compare our predictions to different published
                results based on recurrent neural networks on the UCF101
                dataset
        }
}

@article{DBLP:journals/corr/ZhouB16b,
        author    = {Yipin Zhou and
                     Tamara L. Berg},
        title     = {Learning Temporal Transformations From Time-Lapse Videos},
        journal   = {CoRR},
        volume    = {abs/1608.07724},
        year      = {2016},
        url       = {http://arxiv.org/abs/1608.07724},
        timestamp = {Fri, 02 Sep 2016 17:46:24 +0200},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ZhouB16b},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                Based on life-long observations of physical, chemical, and
                biologic phenomena in the natural world, humans can often
                easily picture in their minds what an object will look like in
                the future. But, what about computers? In this paper, we learn
                computational models of object transformations from time-lapse
                videos. In particular, we explore the use of generative models
                to create depictions of objects at future times. These models
                explore several different prediction tasks: generating a future
                state given a single depiction of an object, generating a
                future state given two depictions of an object at different
                times, and generating future states recursively in a recurrent
                framework. We provide both qualitative and quantitative
                evaluations of the generated results, and also conduct a human
                evaluation to compare variations of our models.
        }
}

@article{DBLP:journals/corr/YooKPPK16,
        author    = {Donggeun Yoo and
                     Namil Kim and
                     Sunggyun Park and
                     Anthony S. Paek and
                     In{-}So Kweon},
        title     = {Pixel-Level Domain Transfer},
        journal   = {CoRR},
        volume    = {abs/1603.07442},
        year      = {2016},
        url       = {http://arxiv.org/abs/1603.07442},
        timestamp = {Sat, 02 Apr 2016 11:49:48 +0200},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/YooKPPK16},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                We present an image-conditional image generation model. The
                model transfers an input domain to a target domain in semantic
                level, and generates the target image in pixel level. To
                generate realistic target images, we employ the
                real/fake-discriminator as in Generative Adversarial Nets, but
                also introduce a novel domain-discriminator to make the
                generated image relevant to the input image. We verify our
                model through a challenging task of generating a piece of
                clothing from an input image of a dressed person. We present a
                high quality clothing dataset containing the two domains, and
                succeed in demonstrating decent results.
        }
}

@article{DBLP:journals/corr/LiW16b,
        author    = {Chuan Li and
                     Michael Wand},
        title     = {Precomputed Real-Time Texture Synthesis with Markovian Generative
                     Adversarial Networks},
        journal   = {CoRR},
        volume    = {abs/1604.04382},
        year      = {2016},
        url       = {http://arxiv.org/abs/1604.04382},
        timestamp = {Mon, 02 May 2016 18:22:52 +0200},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/LiW16b},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                This paper proposes Markovian Generative Adversarial Networks
                (MGANs), a method for training generative neural networks for
                efficient texture synthesis. While deep neural network
                approaches have recently demonstrated remarkable results in
                terms of synthesis quality, they still come at considerable
                computational costs (minutes of run-time for low-res images).
                Our paper addresses this efficiency issue. Instead of a
                numerical deconvolution in previous work, we precompute a
                feed-forward, strided convolutional network that captures the
                feature statistics of Markovian patches and is able to directly
                generate outputs of arbitrary dimensions. Such network can
                directly decode brown noise to realistic texture, or photos to
                artistic paintings. With adversarial training, we obtain
                quality comparable to recent neural texture synthesis methods.
                As no optimization is required any longer at generation time,
                our run-time performance (0.25M pixel images at 25Hz) surpasses
                previous neural texture synthesizers by a significant margin
                (at least 500 times faster). We apply this idea to texture
                synthesis, style transfer, and video stylization.
        }
}

@article{DBLP:journals/corr/RonnebergerFB15,
        author    = {Olaf Ronneberger and
                     Philipp Fischer and
                     Thomas Brox},
        title     = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
        journal   = {CoRR},
        volume    = {abs/1505.04597},
        year      = {2015},
        url       = {http://arxiv.org/abs/1505.04597},
        timestamp = {Mon, 01 Jun 2015 14:13:54 +0200},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/RonnebergerFB15},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                There is large consent that successful training of deep
                networks requires many thousand annotated training samples. In
                this paper, we present a network and training strategy that
                relies on the strong use of data augmentation to use the
                available annotated samples more efficiently. The architecture
                consists of a contracting path to capture context and a
                symmetric expanding path that enables precise localization. We
                show that such a network can be trained end-to-end from very
                few images and outperforms the prior best method (a
                sliding-window convolutional network) on the ISBI challenge for
                segmentation of neuronal structures in electron microscopic
                stacks. Using the same network trained on transmitted light
                microscopy images (phase contrast and DIC) we won the ISBI cell
                tracking challenge 2015 in these categories by a large margin.
                Moreover, the network is fast. Segmentation of a 512x512 image
                takes less than a second on a recent GPU. The full
                implementation (based on Caffe) and the trained networks are
                available at this http URL .
        }
}

@article{DBLP:journals/corr/LarsenSW15,
        author    = {Anders Boesen Lindbo Larsen and
                     S{\o}ren Kaae S{\o}nderby and
                     Ole Winther},
        title     = {Autoencoding beyond pixels using a learned similarity metric},
        journal   = {CoRR},
        volume    = {abs/1512.09300},
        year      = {2015},
        url       = {http://arxiv.org/abs/1512.09300},
        timestamp = {Sat, 02 Jan 2016 11:38:49 +0100},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/LarsenSW15},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                We present an autoencoder that leverages learned
                representations to better measure similarities in data space.
                By combining a variational autoencoder with a generative
                adversarial network we can use learned feature representations
                in the GAN discriminator as basis for the VAE reconstruction
                objective. Thereby, we replace element-wise errors with
                feature-wise errors to better capture the data distribution
                while offering invariance towards e.g. translation. We apply
                our method to images of faces and show that it outperforms VAEs
                with element-wise similarity measures in terms of visual
                fidelity. Moreover, we show that the method learns an embedding
                in which high-level abstract visual features (e.g. wearing
                glasses) can be modified using simple arithmetic.
        }
}

@article{DBLP:journals/corr/UlyanovVL16,
        author    = {Dmitry Ulyanov and
                     Andrea Vedaldi and
                     Victor S. Lempitsky},
        title     = {Instance Normalization: The Missing Ingredient for Fast Stylization},
        journal   = {CoRR},
        volume    = {abs/1607.08022},
        year      = {2016},
        url       = {http://arxiv.org/abs/1607.08022},
        timestamp = {Tue, 02 Aug 2016 12:59:27 +0200},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/UlyanovVL16},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                It this paper we revisit the fast stylization method introduced
                in Ulyanov et. al. (2016). We show how a small change in the
                stylization architecture results in a significant qualitative
                improvement in the generated images. The change is limited to
                swapping batch normalization with instance normalization, and
                to apply the latter both at training and testing times. The
                resulting method can be used to train high-performance
                architectures for real-time image generation. The code will is
                made available on github.
        }
}

@article{DBLP:journals/corr/CordtsORREBFRS16,
        author    = {Marius Cordts and
                     Mohamed Omran and
                     Sebastian Ramos and
                     Timo Rehfeld and
                     Markus Enzweiler and
                     Rodrigo Benenson and
                     Uwe Franke and
                     Stefan Roth and
                     Bernt Schiele},
        title     = {The Cityscapes Dataset for Semantic Urban Scene Understanding},
        journal   = {CoRR},
        volume    = {abs/1604.01685},
        year      = {2016},
        url       = {http://arxiv.org/abs/1604.01685},
        timestamp = {Mon, 02 May 2016 18:22:52 +0200},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/CordtsORREBFRS16},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                Visual understanding of complex urban street scenes is an
                enabling factor for a wide range of applications. Object
                detection has benefited enormously from large-scale datasets,
                especially in the context of deep learning. For semantic urban
                scene understanding, however, no current dataset adequately
                captures the complexity of real-world urban scenes.

                To address this, we introduce Cityscapes, a benchmark suite and
                large-scale dataset to train and test approaches for
                pixel-level and instance-level semantic labeling. Cityscapes is
                comprised of a large, diverse set of stereo video sequences
                recorded in streets from 50 different cities. 5000 of these
                images have high quality pixel-level annotations; 20000
                additional images have coarse annotations to enable methods
                that leverage large volumes of weakly-labeled data. Crucially,
                our effort exceeds previous attempts in terms of dataset size,
                annotation richness, scene variability, and complexity. Our
                accompanying empirical study provides an in-depth analysis of
                the dataset characteristics, as well as a performance
                evaluation of several state-of-the-art approaches based on our
                benchmark.
        }
}

@article{DBLP:journals/corr/RussakovskyDSKSMHKKBBF14,
        author    = {Olga Russakovsky and
                     Jia Deng and
                     Hao Su and
                     Jonathan Krause and
                     Sanjeev Satheesh and
                     Sean Ma and
                     Zhiheng Huang and
                     Andrej Karpathy and
                     Aditya Khosla and
                     Michael S. Bernstein and
                     Alexander C. Berg and
                     Fei{-}Fei Li},
        title     = {ImageNet Large Scale Visual Recognition Challenge},
        journal   = {CoRR},
        volume    = {abs/1409.0575},
        year      = {2014},
        url       = {http://arxiv.org/abs/1409.0575},
        timestamp = {Mon, 01 Jun 2015 08:26:31 +0200},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/RussakovskyDSKSMHKKBBF14},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                The ImageNet Large Scale Visual Recognition Challenge is a
                benchmark in object category classification and
                detection on hundreds of object categories and millions
                of images. The challenge has been run annually from
                2010 to present, attracting participation from more
                than fifty institutions.

                This paper describes the creation of this benchmark dataset and
                the advances in object recognition that have been possible as a
                result. We discuss the challenges of collecting large-scale
                ground truth annotation, highlight key breakthroughs in
                categorical object recognition, provide a detailed analysis of
                the current state of the field of large-scale image
                classification and object detection, and compare the
                state-of-the-art computer vision accuracy with human accuracy.
                We conclude with lessons learned in the five years of the
                challenge, and propose future directions and improvements.
        }
}

@InProceedings{fine-grained,
        author = {A. Yu and K. Grauman},
        title = {{F}ine-{G}rained {V}isual {C}omparisons with {L}ocal {L}earning},
        booktitle = {Computer Vision and Pattern Recognition (CVPR)},
        month = {June},
        year = {2014},
        abstract = {
                Given two images, we want to predict which exhibits a
                particular visual attribute more than the other — even
                when the two images are quite similar. Existing
                relative attribute methods rely on global ranking
                functions; yet rarely will the visual cues relevant to
                a comparison be constant for all data, nor will humans’
                perception of the attribute necessarily permit a global
                ordering. To address these issues, we propose a local
                learning approach for fine-grained visual comparisons.
                Given a novel pair of images, we learn a local ranking
                model on the fly, using only analogous training
                comparisons. We show how to identify these analogous
                pairs using learned metrics. With results on three
                challenging datasets — including a large newly curated
                dataset for fine-grained comparisons — our method
                outperforms stateof-the-art methods for relative
                attribute prediction.
        }
}

@article{Eitz:2012:HSO:2185520.2185540,
        author = {Eitz, Mathias and Hays, James and Alexa, Marc},
        title = {How Do Humans Sketch Objects?},
        journal = {ACM Trans. Graph.},
        issue_date = {July 2012},
        volume = {31},
        number = {4},
        month = jul,
        year = {2012},
        issn = {0730-0301},
        pages = {44:1--44:10},
        articleno = {44},
        numpages = {10},
        url = {http://doi.acm.org/10.1145/2185520.2185540},
        doi = {10.1145/2185520.2185540},
        acmid = {2185540},
        publisher = {ACM},
        address = {New York, NY, USA},
        keywords = {crowd-sourcing, learning, recognition, sketch},
        abstract = {
                Humans have used sketching to depict our visual world since
                prehistoric times. Even today, sketching is possibly
                the only rendering technique readily available to all
                humans. This paper is the first large scale exploration
                of human sketches. We analyze the distribution of
                non-expert sketches of everyday objects such as
                'teapot' or 'car'. We ask humans to sketch objects of a
                given category and gather 20,000 unique sketches evenly
                distributed over 250 object categories. With this
                dataset we perform a perceptual study and find that
                humans can correctly identify the object category of a
                sketch 73% of the time. We compare human performance
                against computational recognition methods. We develop a
                bag-of-features sketch representation and use
                multi-class support vector machines, trained on our
                sketch dataset, to classify sketches. The resulting
                recognition method is able to identify unknown sketches
                with 56% accuracy (chance is 0.4%). Based on the
                computational model, we demonstrate an interactive
                sketch recognition system. We release the complete
                crowd-sourced dataset of sketches to the community.
        }
}

@article {Laffont14,
        title = {Transient Attributes for High-Level Understanding and Editing of Outdoor Scenes},
        author = {Pierre-Yves Laffont and Zhile Ren and Xiaofeng Tao and Chao Qian and James Hays},
        journal = {ACM Transactions on Graphics (proceedings of SIGGRAPH)},
        volume = {33},
        number = {4},
        year = {2014},
        abstract = {
                We live in a dynamic visual world where the appearance of
                scenes changes dramatically from hour to hour or season to
                season. In this work we study “transient scene attributes” –
                high level properties which affect scene appearance, such as
                “snow”, “autumn”, “dusk”, “fog”. We define 40 transient
                attributes and use crowdsourcing to annotate thousands of
                images from 101 webcams. We use this “transient attribute
                database” to train regressors that can predict the presence of
                attributes in novel images. We demonstrate a photo organization
                method based on predicted attributes. Finally we propose a
                high-level image editing method which allows a user to adjust
                the attributes of a scene, e.g. change a scene to be “snowy” or
                “sunset”. To support attribute manipulation we introduce a
                novel appearance transfer technique which is simple and fast
                yet competitive with the state-of-the-art. We show that we can
                convincingly modify many transient attributes in outdoor
                scenes.
        }
}

@article{DBLP:journals/corr/WuSH16e,
        author    = {Zifeng Wu and
                     Chunhua Shen and
                     Anton van den Hengel},
        title     = {Wider or Deeper: Revisiting the ResNet Model for Visual Recognition},
        journal   = {CoRR},
        volume    = {abs/1611.10080},
        year      = {2016},
        url       = {http://arxiv.org/abs/1611.10080},
        timestamp = {Thu, 01 Dec 2016 19:32:08 +0100},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/WuSH16e},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                The trend towards increasingly deep neural networks has been
                driven by a general observation that increasing depth
                increases the performance of a network. Recently,
                however, evidence has been amassing that simply increasing
                depth may not be the best way to increase performance,
                particularly given other limitations. Investigations into deep
                residual networks have also suggested that they may not
                in fact be operating as a single deep network, but
                rather as an ensemble of many relatively shallow
                networks. We examine these issues, and in doing so
                arrive at a new interpretation of the unravelled view
                of deep residual networks which explains some of the
                behaviours that have been observed experimentally. As a
                result, we are able to derive a new, shallower,
                architecture of residual networks which significantly
                outperforms much deeper models such as ResNet-200 on
                the ImageNet classification dataset. We also show that
                this performance is transferable to other problem
                domains by developing a semantic segmentation approach
                which outperforms the state-of-the-art by a remarkable
                margin on datasets including PASCAL VOC, PASCAL
                Context, and Cityscapes. The architecture that we
                propose thus outperforms its comparators, including
                very deep ResNets, and yet is more efficient in memory
                use and sometimes also in training time. The code and
                models are available at this https URL
        }
}

@article{DBLP:journals/corr/BaoCWLH17,
        author    = {Jianmin Bao and
                     Dong Chen and
                     Fang Wen and
                     Houqiang Li and
                     Gang Hua},
        title     = {{CVAE-GAN:} Fine-Grained Image Generation through Asymmetric Training},
        journal   = {CoRR},
        volume    = {abs/1703.10155},
        year      = {2017},
        url       = {http://arxiv.org/abs/1703.10155},
        timestamp = {Mon, 03 Apr 2017 12:41:34 +0200},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/BaoCWLH17},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                We present variational generative adversarial networks, a
                general learning framework that combines a variational
                auto-encoder with a generative adversarial network, for
                synthesizing images of fine-grained categories, such as
                faces of a specific person or objects in a category.
                Our approach models an image as a composition of label
                and latent attributes in a probabilistic model. By
                varying the fine-grained category label fed to the
                resulting generative model, we can generate images in a
                specific category by randomly drawn values on a latent
                attribute vector. The novelty of our approach comes
                from two aspects. Firstly, we propose to adopt a cross
                entropy loss for the discriminative and classifier
                network, but a mean discrepancy objective for the
                generative network. This kind of asymmetric loss
                function makes the training of the GAN more stable.
                Secondly, we adopt an encoder network to learn the
                relationship between the latent space and the real
                image space, and use pairwise feature matching to keep
                the structure of generated images. We experiment with
                natural images of faces, flowers, and birds, and
                demonstrate that the proposed models are capable of
                generating realistic and diverse samples with
                fine-grained category labels. We further show that our
                models can be applied to other tasks, such as image
                inpainting, super-resolution, and data augmentation for
                training better face recognition models.
        },
        comments = {
                Uses a conditional VAE-GAN architecture with feature-matching
                losses. L2 losses are placed on the discriminator and
                classifier features of generated samples. In total the
                introduced loss function has six terms:
                L = L_{KL} + L_G + L_{GD} + L_{GC} + L_D + L_C.

                L_{KL} is KL divergence between a sample from a prior
                distribution of latent space, and the statistics q(z | x_r, c)
                produced by the VAE.

                L_{GD} and L_{GC} are mean feature matching between the
                generated feature maps and the real feature maps. L_{GD} is
                using a moving average, while L_{GD} is just over the current
                batch.
        }
}

@article{DBLP:journals/corr/HuangLPHB16,
        author    = {Xun Huang and
                     Yixuan Li and
                     Omid Poursaeed and
                     John E. Hopcroft and
                     Serge J. Belongie},
        title     = {Stacked Generative Adversarial Networks},
        journal   = {CoRR},
        volume    = {abs/1612.04357},
        year      = {2016},
        url       = {http://arxiv.org/abs/1612.04357},
        timestamp = {Mon, 02 Jan 2017 11:09:15 +0100},
        biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/HuangLPHB16},
        bibsource = {dblp computer science bibliography, http://dblp.org},
        abstract = {
                In this paper, we propose a novel generative model named
                Stacked Generative Adversarial Networks (SGAN), which
                is trained to invert the hierarchical representations
                of a bottom-up discriminative network. Our model
                consists of a top-down stack of GANs, each learned to
                generate lower-level representations conditioned on
                higher-level representations. A representation
                discriminator is introduced at each feature hierarchy
                to encourage the representation manifold of the
                generator to align with that of the bottom-up
                discriminative network, leveraging the powerful
                discriminative representations to guide the generative
                model. In addition, we introduce a conditional loss
                that encourages the use of conditional information from
                the layer above, and a novel entropy loss that
                maximizes a variational lower bound on the conditional
                entropy of generator outputs. We first train each stack
                independently, and then train the whole model
                end-to-end. Unlike the original GAN that uses a single
                noise vector to represent all the variations, our SGAN
                decomposes variations into multiple levels and
                gradually resolves uncertainties in the top-down
                generative process. Based on visual inspection,
                Inception scores and visual Turing test, we demonstrate that
                SGAN is able to generate images of much higher quality
                than GANs without stacking.
        },
        comments = {
                Trains a stacked GAN with a different discriminator at each
                level of the feature hierarchy, which discriminates between
                feature maps from a classifier and the generated feature maps
                at each level.

                Also includes an entropy loss (via mean squared error on
                reconstructed noise from a "Q-Net").

                There is also a conditional (MSE?) loss on E_i being able to
                reconstruct h_{i + 1} from h_i.
        }
}

@article{carreira2017quo,
  title={Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset},
  author={Carreira, Joao and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1705.07750},
  year={2017},
  abstract = {
        The paucity of videos in current action classification datasets
        (UCF-101 and HMDB-51) has made it difficult to identify good
        video architectures, as most methods obtain similar performance
        on existing small-scale benchmarks. This paper re-evaluates
        state-of-the-art architectures in light of the new Kinetics
        Human Action Video dataset. Kinetics has two orders of
        magnitude more data, with 400 human action classes and over 400
        clips per class, and is collected from realistic, challenging
        YouTube videos. We provide an analysis on how current
        architectures fare on the task of action classification on this
        dataset and how much performance improves on the smaller
        benchmark datasets after pre-training on Kinetics. 

        We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is
        based on 2D ConvNet inflation: filters and pooling kernels of very deep
        image classification ConvNets are expanded into 3D, making it possible
        to learn seamless spatio-temporal feature extractors from video while
        leveraging successful ImageNet architecture designs and even their
        parameters. We show that, after pre-training on Kinetics, I3D models
        considerably improve upon the state-of-the-art in action
        classification, reaching 80.7% on HMDB-51 and 98.0% on UCF-101.
  }
}

@article{DBLP:journals/corr/TranBFTP14,
  author    = {Du Tran and
               Lubomir D. Bourdev and
               Rob Fergus and
               Lorenzo Torresani and
               Manohar Paluri},
  title     = {{C3D:} Generic Features for Video Analysis},
  journal   = {CoRR},
  volume    = {abs/1412.0767},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.0767},
  timestamp = {Thu, 01 Jan 2015 19:51:08 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/TranBFTP14},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  abstract = {
        We propose a simple, yet effective approach for spatiotemporal feature learning
        using deep 3-dimensional convolutional networks (3D ConvNets) trained
        on a large scale supervised video dataset. Our findings are three-fold:
        1) 3D ConvNets are more suitable for spatiotemporal feature learning
        compared to 2D ConvNets; 2) A homogeneous architecture with small 3x3x3
        convolution kernels in all layers is among the best performing
        architectures for 3D ConvNets; and 3) Our learned features, namely C3D
        (Convolutional 3D), with a simple linear classifier outperform
        state-of-the-art methods on 4 different benchmarks and are comparable
        with current best methods on the other 2 benchmarks. In addition, the
        features are compact: achieving 52.8% accuracy on UCF101 dataset with
        only 10 dimensions and also very efficient to compute due to the fast
        inference of ConvNets. Finally, they are conceptually very simple and
        easy to train and use.
  }
}

@article{DBLP:journals/corr/abs-1212-0402,
  author    = {Khurram Soomro and
               Amir Roshan Zamir and
               Mubarak Shah},
  title     = {{UCF101:} {A} Dataset of 101 Human Actions Classes From Videos in
               The Wild},
  journal   = {CoRR},
  volume    = {abs/1212.0402},
  year      = {2012},
  url       = {http://arxiv.org/abs/1212.0402},
  timestamp = {Wed, 02 Jan 2013 09:49:04 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1212-0402},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  abstract = {
        We introduce UCF101 which is currently the largest dataset of human actions. It
        consists of 101 action classes, over 13k clips and 27 hours of video
        data. The database consists of realistic user uploaded videos
        containing camera motion and cluttered background. Additionally, we
        provide baseline action recognition results on this new dataset using
        standard bag of words approach with overall performance of 44.5%. To
        the best of our knowledge, UCF101 is currently the most challenging
        dataset of actions due to its large number of classes, large number of
        clips and also unconstrained nature of such clips.
  }
}

@inproceedings{KarpathyCVPR14,
  title     = {Large-scale Video Classification with Convolutional Neural Networks},
  author    = {Andrej Karpathy and George Toderici and Sanketh Shetty and
               Thomas Leung and Rahul Sukthankar and Li Fei-Fei},
  year      = {2014},
  booktitle = {CVPR},
  abstract = {
        Convolutional Neural Networks (CNNs) have been established as a powerful class
        of models for image recognition problems. Encouraged by these results, we
        provide an extensive empirical evaluation of CNNs on large-scale video
        classification using a new dataset of 1 million YouTube videos belonging to 487
        classes. We study multiple approaches for extending the connectivity of a CNN
        in time domain to take advantage of local spatio-temporal information and
        suggest a multiresolution, foveated architecture as a promising way of speeding
        up the training. Our best spatio-temporal networks display significant
        performance improvements compared to strong feature-based baselines (55.3% to
        63.9%), but only a surprisingly modest improvement compared to single-frame
        models (59.3% to 60.9%). We further study the generalization performance of our
        best model by retraining the top layers on the UCF-101 Action Recognition
        dataset and observe significant performance improvements compared to the
        UCF-101 baseline model (63.3% up from 43.9%).
  }
}

@article{DBLP:journals/corr/NgHVVMT15,
  author    = {Joe Yue{-}Hei Ng and
               Matthew J. Hausknecht and
               Sudheendra Vijayanarasimhan and
               Oriol Vinyals and
               Rajat Monga and
               George Toderici},
  title     = {Beyond Short Snippets: Deep Networks for Video Classification},
  journal   = {CoRR},
  volume    = {abs/1503.08909},
  year      = {2015},
  url       = {http://arxiv.org/abs/1503.08909},
  timestamp = {Thu, 09 Apr 2015 11:33:20 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/NgHVVMT15},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  abstract = {
        Convolutional neural networks (CNNs) have been extensively applied for
        image recognition problems giving state-of-the-art results on
        recognition, detection, segmentation and retrieval. In this work we
        propose and evaluate several deep neural network architectures to
        combine image information across a video over longer time periods than
        previously attempted. We propose two methods capable of handling full
        length videos. The first method explores various convolutional temporal
        feature pooling architectures, examining the various design choices
        which need to be made when adapting a CNN for this task. The second
        proposed method explicitly models the video as an ordered sequence of
        frames. For this purpose we employ a recurrent neural network that uses
        Long Short-Term Memory (LSTM) cells which are connected to the output
        of the underlying CNN. Our best networks exhibit significant
        performance improvements over previously published results on the
        Sports 1 million dataset (73.1% vs. 60.9%) and the UCF-101 datasets
        with (88.6% vs. 88.0%) and without additional optical flow information
        (82.6% vs. 72.8%).
  }
}

@INPROCEEDINGS{Wang2013,
  author={Heng Wang and Cordelia Schmid},
  title={Action Recognition with Improved Trajectories},
  booktitle= {IEEE International Conference on Computer Vision},
  year={2013},
  address={Sydney, Australia},
  url={http://hal.inria.fr/hal-00873267},
  abstract = {
        Recently dense trajectories were shown to be an efficient video
        representation for action recognition and achieved state-of-the-art
        results on a variety of datasets. This paper improves their performance
        by taking into account camera motion to correct them. To estimate
        camera motion, we match feature points between frames using SURF
        descriptors and dense optical flow, which are shown to be
        complementary. These matches are, then, used to robustly estimate a
        homography with RANSAC. Human motion is in general different from
        camera motion and generates inconsistent matches. To improve the
        estimation, a human detector is employed to remove these matches. Given
        the estimated camera motion, we remove trajectories consistent with it.
        We also use this estimation to cancel out camera motion from the
        optical flow. This significantly improves motion-based descriptors,
        such as HOF and MBH. Experimental results on four challenging action
        datasets (i.e., Hollywood2, HMDB51, Olympic Sports and UCF50)
        significantly outperform the current state of the art.
  }
}

@inproceedings{Le:2011:LHI:2191740.2192108,
 author = {Le, Q. V. and Zou, W. Y. and Yeung, S. Y. and Ng, A. Y.},
 title = {Learning Hierarchical Invariant Spatio-temporal Features for Action
 Recognition with Independent Subspace Analysis},
 booktitle = {Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition},
 series = {CVPR '11},
 year = {2011},
 isbn = {978-1-4577-0394-2},
 pages = {3361--3368},
 numpages = {8},
 url = {http://dx.doi.org/10.1109/CVPR.2011.5995496},
 doi = {10.1109/CVPR.2011.5995496},
 acmid = {2192108},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {YouTube action recognition dataset, hierarchical invariant
 spatio-temporal feature learning technique, action recognition, hand-designed
 local feature, SIFT, HOG, static image, video domain, unsupervised feature
 learning, video data, independent subspace analysis algorithm, hierarchical
 representation, UCF, KTH},
 abstract = {
        Previous work on action recognition has focused on adapting
        hand-designed local features, such as SIFT or HOG, from static images
        to the video domain. In this paper, we propose using unsupervised
        feature learning as a way to learn features directly from video data.
        More specifically, we present an extension of the Independent Subspace
        Analysis algorithm to learn invariant spatio-temporal features from
        unlabeled video data. We discovered that, despite its simplicity, this
        method performs surprisingly well when combined with deep learning
        techniques such as stacking and convolution to learn hierarchical
        representations. By replacing hand-designed features with our learned
        features, we achieve classification results superior to all previous
        published results on the Hollywood2, UCF, KTH and YouTube action
        recognition datasets. On the challenging Hollywood2 and YouTube action
        datasets we obtain 53.3% and 75.8% respectively, which are
        approximately 5% better than the current best published results.
        Further benefits of this method, such as the ease of training and the
        efficiency of training and prediction, will also be discussed. You can
        download our code and learned spatio-temporal features here:
        http://ai.stanford.edu/~wzou/.
 }
}

@article{DBLP:journals/corr/SimonyanZ14,
  author    = {Karen Simonyan and
               Andrew Zisserman},
  title     = {Two-Stream Convolutional Networks for Action Recognition in Videos},
  journal   = {CoRR},
  volume    = {abs/1406.2199},
  year      = {2014},
  url       = {http://arxiv.org/abs/1406.2199},
  timestamp = {Tue, 01 Jul 2014 11:58:08 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/SimonyanZ14},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  abstract = {
        We investigate architectures of discriminatively trained deep
        Convolutional Networks (ConvNets) for action recognition in video. The
        challenge is to capture the complementary information on appearance
        from still frames and motion between frames. We also aim to generalise
        the best performing hand-crafted features within a data-driven learning
        framework. 
        Our contribution is three-fold. First, we propose a two-stream ConvNet
        architecture which incorporates spatial and temporal networks. Second,
        we demonstrate that a ConvNet trained on multi-frame dense optical flow
        is able to achieve very good performance in spite of limited training
        data. Finally, we show that multi-task learning, applied to two
        different action classification datasets, can be used to increase the
        amount of training data and improve the performance on both. 
        Our architecture is trained and evaluated on the standard video actions
        benchmarks of UCF-101 and HMDB-51, where it is competitive with the
        state of the art. It also exceeds by a large margin previous attempts
        to use deep nets for video classification.
  }
}

@article{kay2017kinetics,
  title={The Kinetics Human Action Video Dataset},
  author={Kay, Will and Carreira, Joao and Simonyan, Karen and Zhang, Brian and
          Hillier, Chloe and Vijayanarasimhan, Sudheendra and Viola, Fabio and
          Green, Tim and Back, Trevor and Natsev, Paul and others},
  journal={arXiv preprint arXiv:1705.06950},
  year={2017},
  abstract = {
        We describe the DeepMind Kinetics human action video dataset. The
        dataset contains 400 human action classes, with at least 400 video
        clips for each action. Each clip lasts around 10s and is taken from a
        different YouTube video. The actions are human focussed and cover a
        broad range of classes including human-object interactions such as
        playing instruments, as well as human-human interactions such as
        shaking hands. We describe the statistics of the dataset, how it was
        collected, and give some baseline performance figures for neural
        network architectures trained and tested for human action
        classification on this dataset. We also carry out a preliminary
        analysis of whether imbalance in the dataset leads to bias in the
        classifiers.
  }
}

@InProceedings{Kuehne11,
   author= "Kuehne, H. and Jhuang, H. and Garrote, E. and Poggio, T. and Serre, T.",
   title = "{HMDB}: a large video database for human motion recognition",
   booktitle = "Proceedings of the International Conference on Computer Vision (ICCV)",
   year = "2011",
   abstract = {
        With nearly one billion online videos viewed everyday, an emerging new
        frontier in computer vision research is recognition and search in
        video. While much effort has been devoted to the collection and
        annotation of large scalable static image datasets containing thousands
        of image categories, human action datasets lag far behind. Current
        action recognition databases contain on the order of ten different
        action categories collected under fairly controlled conditions.
        State-of-the-art performance on these datasets is now near ceiling and
        thus there is a need for the design and creation of new benchmarks. To
        address this issue we collected the largest action video database
        to-date with 51 action categories, which in total contain around 7,000
        manually annotated clips extracted from a variety of sources ranging
        from digitized movies to YouTube. The goal of this effort is to provide
        a tool to evaluate the performance of computer vision systems for
        action recognition and explore the robustness of these methods under
        various conditions such as camera motion, viewpoint, video quality and
        occlusion.
   }
}

@article{DBLP:journals/corr/FeichtenhoferPZ16,
  author    = {Christoph Feichtenhofer and
               Axel Pinz and
               Andrew Zisserman},
  title     = {Convolutional Two-Stream Network Fusion for Video Action Recognition},
  journal   = {CoRR},
  volume    = {abs/1604.06573},
  year      = {2016},
  url       = {http://arxiv.org/abs/1604.06573},
  timestamp = {Mon, 02 May 2016 18:22:52 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/FeichtenhoferPZ16},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  abstract = {
        Recent applications of Convolutional Neural Networks (ConvNets) for
        human action recognition in videos have proposed different solutions
        for incorporating the appearance and motion information. We study a
        number of ways of fusing ConvNet towers both spatially and temporally
        in order to best take advantage of this spatio-temporal information. We
        make the following findings: (i) that rather than fusing at the softmax
        layer, a spatial and temporal network can be fused at a convolution
        layer without loss of performance, but with a substantial saving in
        parameters; (ii) that it is better to fuse such networks spatially at
        the last convolutional layer than earlier, and that additionally fusing
        at the class prediction layer can boost accuracy; finally (iii) that
        pooling of abstract convolutional features over spatiotemporal
        neighbourhoods further boosts performance. Based on these studies we
        propose a new ConvNet architecture for spatiotemporal fusion of video
        snippets, and evaluate its performance on standard benchmarks where
        this architecture achieves state-of-the-art results.
  }
}

@INPROCEEDINGS{Zach07aduality,
    author = {C. Zach and T. Pock and H. Bischof},
    title = {A duality based approach for realtime tv-l1 optical flow},
    booktitle = {In Ann. Symp. German Association Patt. Recogn},
    year = {2007},
    pages = {214--223},
    abstract = {
        Variational methods are among the most successful approaches to
        calculate the optical flow between two image frames. A particularly
        appealing formulation is based on total variation (TV) regularization
        and the robust L 1 norm in the data fidelity term. This formulation can
        preserve discontinuities in the flow field and offers an increased
        robustness against illumination changes, occlusions and noise. In this
        work we present a novel approach to solve the TV-L 1 formulation. Our
        method results in a very efficient numerical scheme, which is based on
        a dual formulation of the TV energy and employs an efficient point-wise
        thresholding step. Additionally, our approach can be accelerated by
        modern graphics processing units. We demonstrate the real-time
        performance (30 fps) of our approach for video inputs at a resolution
        of 320 × 240 pixel.
    }
}

@article{DBLP:journals/corr/ShouCZMC17,
  author    = {Zheng Shou and
               Jonathan Chan and
               Alireza Zareian and
               Kazuyuki Miyazawa and
               Shih{-}Fu Chang},
  title     = {{CDC:} Convolutional-De-Convolutional Networks for Precise Temporal
               Action Localization in Untrimmed Videos},
  journal   = {CoRR},
  volume    = {abs/1703.01515},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.01515},
  timestamp = {Wed, 07 Jun 2017 14:41:04 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ShouCZMC17},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{DBLP:journals/corr/IdreesZJGLSS16,
  author    = {Haroon Idrees and
               Amir Roshan Zamir and
               Yu{-}Gang Jiang and
               Alex Gorban and
               Ivan Laptev and
               Rahul Sukthankar and
               Mubarak Shah},
  title     = {The {THUMOS} Challenge on Action Recognition for Videos "in the Wild"},
  journal   = {CoRR},
  volume    = {abs/1604.06182},
  year      = {2016},
  url       = {http://arxiv.org/abs/1604.06182},
  timestamp = {Wed, 07 Jun 2017 14:40:12 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/IdreesZJGLSS16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@InProceedings{Heilbron_2015_CVPR,
author = {Caba Heilbron, Fabian and Escorcia, Victor and Ghanem, Bernard and Carlos Niebles, Juan},
title = {ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015}
}

@article{DBLP:journals/corr/TranBFTP15,
  author    = {Du Tran and
               Lubomir D. Bourdev and
               Rob Fergus and
               Lorenzo Torresani and
               Manohar Paluri},
  title     = {Deep End2End Voxel2Voxel Prediction},
  journal   = {CoRR},
  volume    = {abs/1511.06681},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.06681},
  timestamp = {Wed, 07 Jun 2017 14:41:18 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/TranBFTP15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{DBLP:journals/corr/ShouWC16,
  author    = {Zheng Shou and
               Dongang Wang and
               Shih{-}Fu Chang},
  title     = {Action Temporal Localization in Untrimmed Videos via Multi-stage CNNs},
  journal   = {CoRR},
  volume    = {abs/1601.02129},
  year      = {2016},
  url       = {http://arxiv.org/abs/1601.02129},
  timestamp = {Wed, 07 Jun 2017 14:41:27 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ShouWC16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{DBLP:journals/corr/YeungRJAML15,
  author    = {Serena Yeung and
               Olga Russakovsky and
               Ning Jin and
               Mykhaylo Andriluka and
               Greg Mori and
               Fei{-}Fei Li},
  title     = {Every Moment Counts: Dense Detailed Labeling of Actions in Complex
               Videos},
  journal   = {CoRR},
  volume    = {abs/1507.05738},
  year      = {2015},
  url       = {http://arxiv.org/abs/1507.05738},
  timestamp = {Wed, 07 Jun 2017 14:42:02 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/YeungRJAML15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
