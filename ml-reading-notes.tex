% TODO(brendan): Question to answer: what is the effect of attaching multiple
% losses on multiple heads of a network, in terms of their respective influence
% on the update of weights in lower layers of the network during
% backpropagation?
% Are the relative magnitudes of the losses from the different heads important
% in terms of their influence on weight updates? Will a loss with a magnitude
% 10^3 times larger dominate a 10^3 times smaller loss in influencing weight
% updates?

\documentclass[a4paper, 12pt]{article}

\usepackage{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{fullpage}
\usepackage{url}
\usepackage{graphicx}
\usepackage{inputenc}
\usepackage{titling}
\usepackage{hyperref}
\usepackage{txfonts}
\usepackage{bbold}
\usepackage{mathtools}

\newcommand\phantomlabel[1]{\phantomsection\label{#1}}

\DeclarePairedDelimiter\norm{\lVert}{\rVert}%

\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
        #1\;\delimsize\|\;#2%
}
\newcommand{\infdiv}{D_{KL}\infdivx}

\date{\today}
\title{Machine Learning Reading Notes}

\author{Brendan Duke}

\begin{document}

\maketitle

\section{Definitions}

% TODO(brendan):
\phantomlabel{backprop}
\textbf{Back propagation}

\phantomlabel{batchnorm}
\textbf{Batch Normalization}

\textbf{Deep Neural Networks (DNNs)} are engineered systems inspired by the
biological brain\cite{Goodfellow-et-al-2016-Book}.

\phantomlabel{kl}
\textbf{KL divergence} is given in Equation~\ref{kleqn}\cite[Chapter~3]{Goodfellow-et-al-2016-Book}.

\begin{equation}
        \infdiv{P}{Q} = \varmathbb{E}_{x \sim P}\left[\log P(x) - \log Q(x)\right]
        \label{kleqn}
\end{equation}

\textbf{Cross-entropy} is related to \hyperref[kl]{KL divergence} by
$H(P, Q) = H(P) + \infdiv{P}{Q}$, where $H(P)$, the \textbf{Shannon entropy}, is
$H(P) = -\varmathbb{E}_{x \sim P} \left[\log P(x)\right]$.

\phantomlabel{LSTM}
% TODO(brendan): Diagrams and equations for LSTM. Definition for RNNs. Should
% be a distillation of Chapter 10 of Goodfellow book.
\textbf{LSTM} (Long Short Term
Memory) neural networks are a type of recurrent neural network whose
characteristic feature is the presence of a gated self-loop that allows
retention of its ``cell state'', which are the pre-non-linearity activations of
the previous time step\cite[Chapter~10]{Goodfellow-et-al-2016-Book}.

Cell state is updated at each time step according to
Equation~\ref{lstm_cell_state_update}.

\begin{equation}
        s_i^{(t)} = f_i^{(t)} s_i^{(t - 1)} + g_i^{(t)}
                \sigma \left( b_i + \sum_j U_{i, j} x_j^{(t)} + \sum_j W_{i, j} h_j^{(t - 1)}\right)
        \label{lstm_cell_state_update}
\end{equation}

The vectors $\boldsymbol{f^{(t)}}$ and $\boldsymbol{g^{(t)}}$ in
Equation~\ref{lstm_cell_state_update} also take inputs from $\boldsymbol{x^{(t)}}$
and $\boldsymbol{h^{(t - 1)}}$, with their own weight tensors and bias vectors
$\boldsymbol{U}^f$, $\boldsymbol{W}^f$ and $\boldsymbol{b}^f$, $\boldsymbol{U}^g$,
$\boldsymbol{W}^g$ and $\boldsymbol{b}^f$, respectively.

Similar gate functions exist to gate the inputs and outputs to the LSTM, as
well.

\textbf{Mahalanobis Distance}

\phantomlabel{multilayer_perceptron}
\textbf{Multi-Layer Perceptrons (MLPs)} are mathematical functions mapping some
set of input values to some set of output
values\cite{Goodfellow-et-al-2016-Book}.

\textbf{Neighbourhood Components Analysis (NCA)} is a method of learning a
Mahalanobis distance metric, and can also be used in linear dimensionality
reduction\cite{NIPS2004_2566}.

The \textbf{PCKh} metric, used by the MPII Human Pose Dataset, defines a joint
estimate as matching the ground truth if the estimate lies within 50\% of the
head segment length\cite{andriluka-2d-2014-853}. The head segment length is
defined as the diagonal across the annotated head rectangle in the MPII data,
multiplied by a factor of 0.6. Details can be found by examining the MATLAB
\href{http://human-pose.mpi-inf.mpg.de/results/mpii_human_pose/evalMPII.zip}{evaluation script}
provided with the MPII dataset.

\phantomlabel{nonmax_supression}
\textbf{Non-maximum suppression} in object detection, in general, is a set of
methods used to prune an initial set of object bounding boxes that may be
uncorrelated with the actual object detections in an image, down to a subset
that are\cite{DBLP:conf/accv/RotheGG14}. In edge detection, non-maximum
suppression is used to suppress any pixels (i.e.\ not include them in the set of
detected edges) that are not the maximum response in their neighbourhood.

The \textbf{softmax function} is a continuous differentiable version of the
argmax function, where the result is represented as a one-hot
vector\cite[Chapter~6]{Goodfellow-et-al-2016-Book}. Softmax is a way of
representing probability distributions over a discrete variable that can take
on $n$ possible values.

Formally, softmax is given by Equation~\ref{softmax_eqn}.

\begin{equation}
        \textrm{softmax}{(\boldsymbol{z})}_i = \frac{e^{z_i}}{\sum_je^{z_j}}
        \label{softmax_eqn}
\end{equation}

\phantomlabel{rectified_linear_units}
\textbf{Rectified linear units (ReLUs)}\cite{icml2010_NairH10}

\textbf{Leaky rectified linear units (LReLs)}\cite{maas_rectified_nonlinearities}

\begin{equation}
        h^{(i)} = \max\left(w^{(i)T}x, 0.01w^{(i)T}x\right) =
        \begin{cases}
                w^{(i)T}x & \textrm{if } w^{(i)T}x > 0 \\
                0.01w^{(i)T}x & \textrm{otherwise}
        \end{cases}
\end{equation}

Leaky ReLUs have non-zero gradient over their domain, and were therefore
motivated in reducing the vanishing gradient problem.

\section{Datasets}

\subsection{UCF-101\cite{DBLP:journals/corr/abs-1212-0402}}
\label{ucf101}

101 classes, 13k clips and 27 hours of video data in total.

\section{Paper Summaries}

\subsection{DeepPose: Human Pose Estimation via Deep Neural
            Networks\cite{DBLP:journals/corr/ToshevS13}}

This paper uses DNNs as a method for human pose estimation, based on the
success of~\cite{NIPS2013_5207} and~\cite{DBLP:journals/corr/GirshickDDM13} for
object detection using DNNs.

This is in contrast to the existing work in human pose estimation at the time,
which focused on explicitly designed pose models. Papers about these methods
can be found in the ``Related Work'' section of
\cite{DBLP:journals/corr/ToshevS13}.

The input to the 7-layered convolutional DNN (based on
AlexNet\cite{NIPS2012_4824}) is the full image.

\subsection{Dropout: A Simple Way to Prevent Neural Networks from
            Overfitting\cite{Srivastava:2014:DSW:2627435.2670313}}
\label{dropout}

\textbf{Dropout} is a technique used to overcome the problem of overfitting in
deep neural nets with large numbers of parameters. The idea is to train using
many ``thinned'' networks, chosen by randomly removing subsets of units and
their connections. The predictions from the thinned networks are approximately
averaged at test time by using a single, unthinned, network with reduced
weights.

\begin{itemize}
        \item Existing regularization methods: stopping training as soon as
                validation error stops improving, L1 and L2 regularization, and
                weight sharing\cite{Nowlan:1992:SNN:148167.148169}.
\end{itemize}

\subsection{End-to-end people detection in crowded
            scenes\cite{DBLP:journals/corr/StewartA15}}

This paper is focused on jointly creating a set of bounding-box predictions for
people in crowded scenes using GoogLeNet and a
\hyperref[LSTM]{recurrent LSTM layer} as a controller. Since bounding-box
predictions are generated jointly, common post-processing steps such as
\hyperref[nonmax_supression]{non-maximum suppression} are unnecessary.  All
components of the system are trained end-to-end using back propagation.

\subsubsection{Motivation}

The end-to-end people detection method is contrasted with the object detection
methods of R-CNN in~\cite{DBLP:journals/corr/GirshickDDM13} and OverFeat in
\cite{DBLP:journals/corr/SermanetEZMFL13}.
\cite{DBLP:journals/corr/GirshickDDM13} and
\cite{DBLP:journals/corr/SermanetEZMFL13} rely on non-maximum suppression,
which does not use access to image information to infer bounding box positions
since non-maximum suppression acts only on bounding boxes. Also, in end-to-end
people detection, the decoding stage is learned using LSTMs, instead of using
specialized methods as in~\cite{VisualPhrases} and~\cite{TaAnSc_14:occluded}.

Early related work can be found in~\cite{Felzenszwalb:2010:ODD:1850486.1850574}
and~\cite{Leibe:2005:PDC:1068507.1069006}. Best performing object detectors at
the time were~\cite{DBLP:journals/corr/GirshickDDM13},
\cite{DBLP:journals/corr/SermanetEZMFL13}, \cite{Uijlings13},
\cite{DBLP:journals/corr/ZhangBS15} and~\cite{DBLP:journals/corr/SzegedyREA14}.

Sequence modeling is done using LSTMs as in
\cite{DBLP:journals/corr/SutskeverVL14} (used for machine translation) and
\cite{DBLP:journals/corr/KarpathyF14} (used for image captioning). The loss
function is similar to the loss function proposed in
\cite{Graves06connectionisttemporal} in that the loss function encourages the
model to make predictions in descending order of confidence.

\subsubsection{Data}

A new training set collected from public webcams, called ``Brainwash'', is
produced. Brainwash consists of 11917 images with 91146 labelled people. 1000
images are allocated for testing and validation, hence training, test and
validation sets contain 82906, 4922 and 3318 labels, respectively.

\subsubsection{Model}

A pre-trained GoogLeNet\cite{going-deeper-szegedy43022} is used to produce
encoded features as input to the LSTM\@. The GoogLeNet features are further
fine-tuned by the training process.  Using GoogLeNet, a feature vector of
length 1024 is produced for each region over a $(15, 20)$ grid of regions that
covers the entire $(480, 640)$ input image. Each cell in the grid has a
receptive field of $(139, 139)$, and is trained to produce a set (with fixed
cardinality five) of distinct bounding boxes in the center $(64, 64)$ region.

$L_2$ regularization of weights in the network was removed entirely.

GoogLeNet activations are scaled down by a factor of 100 before being input to
the decoder, since decoder weights are initialized according to a uniform
distribution in $[-0.1, 0.1]$, while GoogLeNet activations are in $[-80, 80]$.
Regression predictions from GoogLeNet are scaled up by 100 before comparing
with ground truth locations (which are in $[-64, 64]$).

At each step, the LSTM for each grid cell, of which there are 300 in total,
produces a new bounding box and corresponding confidence that the bounding box
contains a person $\boldsymbol{b} = \{\boldsymbol{b}_{pos}, b_c\}$, where
$\boldsymbol{b}_{pos} = (b_x, b_y, b_w, b_h) \in \varmathbb{R}^4$ and
$b_c \in [0, 1]$. The prediction algorithm stops when the confidence drops
below a set threshold. The LSTM units have 250 memory states, no bias units,
and no output non-linearities. Each LSTM unit adds its output to the image
representation, and feeds the result into the next LSTM unit. Comparable
results are found by only presenting the image representation as input to the
first LSTM unit.

Dropout with probability 0.15 is used on the output of each LSTM\@.

\subsubsection{Inference}

The system is trained with learning rate 0.2, decreased by a factor of 0.8
every 100 000 iterations (with convergence occurring after 500 000 iterations),
and momentum 0.5. Gradient clipping is done at 2-norm of 0.1.

Images are jittered by up to 32 pixels in horizontal and vertical directions,
and scaled by a factor between 0.9 and 1.1.

At test time, per-region predictions are merged by adding a new region at each
iteration, and destroying any new bounding boxes that overlap previously
accepted bounding boxes, under the constraint that any given bounding box can
destroy at most one other bounding box. An ordering function
$\Delta': A \times C \rightarrow \varmathbb{N} \times \varmathbb{R}$ given by
$\Delta'(\boldsymbol{b}_i, \tilde{\boldsymbol{b}}_j) = (m_{ij}, d_{ij})$ where
$m_{ij}$ denotes intersection of boxes and $d_{ij}$ is $L_1$ displacement, is
minimized using the Hungarian algorithm in order to find a bipartite matching.
At each step, any new candidate that is not intersecting in the matching is
added to the set of accepted candidates.

\subsubsection{Criticism}

A new loss function that operates on sets of bounding-box predictions is
introduced. Denoting bounding boxes generated by the model as
$C = \{\tilde{\boldsymbol{b}}_i\}$, and ground truth bounding boxes by
$G = \{\boldsymbol{b}_i\}$, the loss function is given by
Equation~\ref{loss-eqn}.

\begin{equation}
        L(G, C, f) = \alpha\sum_i^{|G|}
                             l_{pos}\left(\tilde{\boldsymbol{b}}_{pos}^i, \boldsymbol{b}_{pos}^{f(i)}\right) +
                     \sum_j^{|C|} l_c\left(\tilde{b}_c^j, y_j\right)
        \label{loss-eqn}
\end{equation}

In Equation~\ref{loss-eqn}, $f(i)$ is an injective function $G \rightarrow C$
that assigns one ground truth to each index $i$ up to the number of ground
truths, $l_{pos}$ is the $L_1$ displacement between bounding boxes, and $l_c$
is a cross-entropy loss on a candidate's confidence that a bounding box exists,
where $y_j = \mathbb{1}\{f^{-1}(j) \neq \varnothing\}$. $\alpha$ is set to 0.03
from cross-validation.

In creating $f(i)$ in Equation~\ref{loss-eqn} to assign candidate predictions
to ground truths, the
$G \times C \rightarrow \varmathbb{R} \times \varmathbb{N} \times \varmathbb{N}$
function
$\Delta\left(\boldsymbol{b}^i, \tilde{\boldsymbol{b}}^j\right) = (o_{ij}, r_i, d_{ij})$
is used to lexicographically order pairs first by $o$, then $r$, then $d$,
where $o$ is one if there is sufficient overlap between candidate and ground
truth and zero otherwise, $r$ is the prediction's confidence, and $d$ is the
$L_1$ displacement between candidate and ground truth bounding boxes.

\subsubsection{Experiments}

With an AP (average precision) of 0.78 and EER (equal error rate) of 0.81, the
$f(i)$ produced by minimizing $\Delta$, using the
\href{https://en.wikipedia.org/wiki/Hungarian_algorithm}{Hungarian algorithm},
is found to improve on AP and EER compared with a fixed assignment of $f(i)$,
or selecting the first $k$ highest ranked ($L_\textrm{firstk}$). COUNT
(Absolute difference between number of predicted and ground truth detections)
for $f(i)$ with Hungarian was 0.76 compared with 0.74 for $L_\textrm{firstk}$.
As a baseline, Overfeat-GoogLeNet (bounding-box regression on each cell,
followed by non-maximum suppression, as in
\cite{DBLP:journals/corr/SermanetEZMFL13}) achieved 0.67, 0.71 and 1.05 AP, EER
and COUNT, respectively.

Training without finetuning GoogLeNet reduces AP by 0.29.

Removal of dropout from the output of each LSTM decreases AP by 0.011.

When using the original $2^{-4}$ $L_2$ weights regularization multiplier on
GoogLeNet only, the network was unable to train.  An $L_2$ regularization
multiplier on GoogLeNet of $10^{-6}$ reduced AP by 0.03.

It is found that AP (on the validation set) increases from 0.82 to 0.85 when
using separate weights connecting each of the LSTM outputs to predicted
candidates.

\subsection{Deep Residual Learning for Image
            Recognition\cite{DBLP:journals/corr/HeZRS15}}

A technique, training of residual functions, is presented for deep neural
network architecture design, which allows training of deeper networks with
improved accuracy compared to not training residual functions.

\cite{going-deeper-szegedy43022} and~\cite{DBLP:journals/corr/SimonyanZ14a} are
referred to as motivating ``very deep'' models.

% TODO(brendan): Read [1], [9] for vanishing gradient, R-CNN series for
% localization.

\subsection{Generative Adversarial Networks\cite{NIPS2014_5423}}
\label{gan}

A method of generating probability distributions is presented that can be
trained end-to-end when used with \hyperref[multilayer_perceptron]{MLPs}. In
the given method, a discriminator network $D$ is optimized to distinguish from
ground truth the samples from the generated distribution $G$. $G$ is optimized
to cause $D$ to become $\frac{1}{2}$ everywhere.

Motivation points to~\cite{deepSpeechReviewSPM2012} and~\cite{NIPS2012_4824} as
successful uses of deep discriminative networks for classification based on
\hyperref[backprop]{back propagation}, \hyperref[dropout]{dropout} and
piecewise linear units (such as \hyperref[rectified_linear_units]{ReLUs}).

\subsection{Image-to-Image Translation with Conditional Adversarial
            Networks\cite{DBLP:journals/corr/IsolaZZE16}}

A loss function is presented for image-to-image translation that can be applied
to different tasks, such as colourizing images and reconstructing photos from
label maps or edges.

\href{https://github.com/phillipi/pix2pix}{Code} is available.

\subsubsection{Motivation}

\cite{DBLP:journals/corr/LarsenSW15}, \cite{DBLP:journals/corr/PathakKDDE16}
and~\cite{DBLP:journals/corr/ZhangIE16} are noted as evidence that applying
Euclidean distance loss alone on generated images produces blurry results.

\cite{DBLP:journals/corr/PathakKDDE16} also noted that it was helpful to mix
the GAN objective with a pixel-wise loss such as an L2 loss.

\cite{NIPS2014_5423}, \cite{DBLP:journals/corr/DentonCSF15},
\cite{DBLP:journals/corr/RadfordMC15},
\cite{DBLP:journals/corr/SalimansGZCRC16}, and
\cite{DBLP:journals/corr/ZhaoML16} are mentioned as prior work in
\hyperref[gan]{GANs}, and specifically conditional GANs are explored, as
suggested by~\cite{NIPS2014_5423}.

As opposed to image modelling losses where pixels are conditionally independent
from each other (e.g.~\cite{DBLP:journals/corr/ShelhamerLD16},
\cite{DBLP:journals/corr/XieT15}, \cite{IizukaSIGGRAPH2016},
\cite{DBLP:journals/corr/LarssonMS16} and \cite{DBLP:journals/corr/ZhangIE16}),
a ``structured loss'' is used. Other examples of structured losses are in
\cite{DBLP:journals/corr/ChenPKMY14} (conditional random fields),
\cite{DBLP:journals/corr/DosovitskiyB16} (feature matching),
\cite{DBLP:journals/corr/LiW16} (non-parametric losses),
\cite{DBLP:journals/corr/XieHT15} (pseudo-priors) and
\cite{DBLP:journals/corr/JohnsonAL16} (losses based on matching covariance
statistics).

Previous work on conditional GANs exists in~\cite{DBLP:journals/corr/MirzaO14}
(generating MNIST digits from discrete labels) and
\cite{DBLP:journals/corr/ReedAYLSL16} (image generation from text). Work on
image generation with conditional GANs has focused on image
inpainting\cite{DBLP:journals/corr/PathakKDDE16}, image prediction from a
normal map\cite{DBLP:journals/corr/WangG16}, generating images from user
input\cite{DBLP:journals/corr/WangG16}, predicting future
frames\cite{DBLP:journals/corr/MathieuCL15}, predicting future states based on
time-lapses of objects\cite{DBLP:journals/corr/ZhouB16b}, generating photos of
clothing from input images of clothed people\cite{DBLP:journals/corr/YooKPPK16}
and style transfer\cite{DBLP:journals/corr/LiW16b}.

The generator and discriminator architectures are motivated by
\cite{DBLP:journals/corr/RadfordMC15}.

Instance normalization is introduced in~\cite{DBLP:journals/corr/UlyanovVL16}.

\subsubsection{Data}

The cityscapes\cite{DBLP:journals/corr/CordtsORREBFRS16} (semantic labels
$\leftrightarrow$ photo), CMP Facades (architectural labels $\rightarrow$
photo), Google maps (map $\leftrightarrow$ aerial photo),
ImageNet\cite{DBLP:journals/corr/RussakovskyDSKSMHKKBBF14} (BW $\rightarrow$
colour), \cite{zhu2016generative} and~\cite{fine-grained} (edges $\rightarrow$
photo using the HED edge detector\cite{DBLP:journals/corr/XieT15}),
\cite{Eitz:2012:HSO:2185520.2185540} (sketch $\rightarrow$ photo) and
\cite{Laffont14} (day $\rightarrow$ night).

\subsubsection{Model}

A U-Net\cite{DBLP:journals/corr/RonnebergerFB15} architecture is used for the
generator. A PatchGAN architecture\cite{DBLP:journals/corr/LiW16b} is used for
the discriminator.

Generator architecture: encoder: $C64-C128-C256-C512-C512-C512-C512-C512$,
decoder: $CD512-CD512-CD512-C512-C512-C256-C128-C64$, where $Ck$ denotes
Convolution-BatchNorm-ReLU and $CDk$ denotes
Convolution-BatchNorm-Dropout-ReLU with a dropout rate of $50\%$. After the
last layer, a convolution maps to the number of output channels, followed by a
$\tanh$ function.

In the case of the U-Net, there are skip connections between the $i$th level of
the encoder and the $(n - i)$th layer of the decoder.

The $(70, 70)$ discriminator architecture is $C64-C128-C256-C512$, $(1, 1)$ and
$(16, 16)$ are $C64-C128$, and $(256, 256)$ is $C64-C128-C256-C512-C512-C512$.

All ReLUs in the encoder are leaky, with slope 0.2.

\subsubsection{Inference}

Network weights were initialized from a Gaussian distribution with mean zero
and standard deviation 0.02.

Noise was supplied in the form of dropout, which is applied both at training
and test time.

\hyperref[batchnorm]{Batch normalization} is applied using the statistics of
the test batch, instead of the aggregated training data statistics. Doing so
with a batch size of one is instance
normalization\cite{DBLP:journals/corr/UlyanovVL16}. Batch sizes of one and four
were used.

Mini-batch SGD is used along with the Adam optimizer. One gradient descent step
on G is used, followed by a gradient descent step on D.

Random jitter was applied by resizing the $(256, 256)$ input to $(286, 286)$
and random cropping back down to $(256, 256)$.

Refer to Appendix 5.1.2 of~\cite{DBLP:journals/corr/IsolaZZE16} for specific
training details for each dataset.

\subsubsection{Criticism}

The objective function of a conditional GAN is shown in Equation~\ref{discrim_condition_eqn}.

\begin{equation}
        \mathcal{L}_{cGAN} = \varmathbb{E}_{x, y \sim p_{data}(x, y)}\left[\log D(x, y)\right] +
                             \varmathbb{E}_{x \sim p_{data}(x), y \sim p_z (z)}\left[\log \left(1 - D\left(x, G(x, z)\right)\right)\right]
        \label{discrim_condition_eqn}
\end{equation}

Equation~\ref{discrim_no_condition_eqn} is the objective function where the
discriminator has no prior information about $x$.

\begin{equation}
        \mathcal{L}_{cGAN}\left(G, D\right) =
                \varmathbb{E}_{y \sim p_{data}(y)}\left[\log D(y)\right] +
                \varmathbb{E}_{x \sim p_{data}(x), z \sim p_z (z)}\left[\log \left(1 - D\left(G(x, z)\right)\right)\right]
        \label{discrim_no_condition_eqn}
\end{equation}

An $L1$ loss is attached, as given in Equation~\ref{l1_loss}.

\begin{equation}
        \mathcal{L}_{L1}\left(G\right) = \varmathbb{E}_{x, y \sim p_{data}(x, y)}\left[\norm{y - G(x, z)}_1\right]
        \label{l1_loss}
\end{equation}

The final objective function is
$G^* = \arg\min_G \max_D \mathcal{L}_{cGAN}\left(G, D\right) + \lambda \mathcal{L}_{L1}\left(G\right)$.

\subsubsection{Experiments}

It was found in initial experiments that $G$ learned to ignore its input noise
$z$.

Only minor variation due to the dropout noise applied is observed in the
generated samples.

Engineering generative networks that produce stochastic dependence on noise
input is left as an open research problem.

Little difference was found between using batch sizes of one and four.

Inputs and outputs in all experiments are 1--3 channel images.

Amazon Mechanical Turk was used to show participants a ground truth or
generated image at $(256, 256)$ resolution for one second, after which the
participant had to respond whether the shown image was real or fake.

$6.1\% \pm 1.3\%$ and $18.9\% \pm 2.5\%$ of participants labelled photo
$\rightarrow$ map and map $\rightarrow$ photo generated images as real,
respectively, with L1 + cGAN loss improving over L1 alone.

On colourization, the cGAN achieved $22.5\% \pm 1.6\%$ of responses as
``real'', as compared with $27.8\% \pm 2.7\%$ in
\cite{DBLP:journals/corr/ZhangIE16}.

FCN-8 was trained for semantic classification on a real dataset, after which
its accuracy on the $(70, 70)$ PatchGAN on cityscapes hit 0.63 per-pixel, 0.21 per
class and 0.16 class IoU, higher than $(1, 1)$ and $(256, 256)$ PatchGAN
discriminators.

% NOTE(brendan): 1x1, 70x70 and 256x256 used differing numbers of layers in
% their architectures, so there is no control for number of layers vs.
% receptive field causing the improved performance.

L1 + cGAN loss is found to perform better overall than L1 + GAN or cGAN
objective functions, with the FCN-8 metric scores given above.

Colour distributions in lab colour space are compared for different objective
functions, with L1 loss producing a narrower colour distribution, and cGAN loss
producing a colour distribution closer to that of the ground truth.

The U-Net qualitatively achieves better generated results with both cGAN and L1
loss than an encoder-decoder without skips, with the latter collapsing to
nearly identical results for all label maps.

A generator is trained on $(256, 256)$ images, and evaluated on $(512, 512)$
map $\leftrightarrow$ aerial images.

A cGAN is trained on semantic segmentation labelling of
Cityscapes\cite{DBLP:journals/corr/CordtsORREBFRS16} and achieves 0.22 class
IoU with cGAN loss alone, compared with 0.35 class IoU with L1 loss and 0.80
class IoU using the wide ResNets of~\cite{DBLP:journals/corr/WuSH16e}.

\subsection{Quo Vadis, Action Recognition? A New Model and the Kinetics
            Dataset\cite{carreira2017quo}}

\subsubsection{Data}

The Kinetics Human Action Video Dataset\cite{kay2017kinetics}, which has 400
human action classes each with more than 400 examples. The classes are focused
on human actions, such as pouring or kissing, as opposed to activities (such as
tennis or baseball). The clips are 10s long.

The Kinetics test set is 100 clips for each class.

A miniKinetics version of the dataset was used in the paper, and for
experimentation, with 213 classes and 120k clips split into three subsets: one
for training with 150 to 1000 clips per class, and one split each for
validation and test, containing 25 and 75 clips per class each, respectively.

\hyperref[ucf101]{UCF-101}.

HMDB-51\cite{Kuehne11}, which contains 7000 clips extracted from movies and
YouTube then manually annotated with 51 class labels.

Transfer learning is done on UCF-101 and HMDB-51, these two smaller datasets,
from the larger Kinetics dataset.

\subsubsection{Model}

Inception-V1 with batch normalization is used as a common ``backbone''
architecture for all models.

\textbf{Two-Stream Inflated 3D ConvNets (I3D)}. All kernels in the Inception-V1
model are inflated, i.e. $(N, N)$ kernels become $(N, N, N)$ kernels.

Weights of $2D$ filters are tiled $N$ times across the time dimension.

The first two max-pooling layers of Inception-V1 were altered to be
$(1, 3, 3)$, and have unity stride in time.

\textbf{2D ConvNets with LSTMs on top}. An LSTM layer with batch normalization,
and 512 hidden units, is placed after the last average pooling layer in
Inception-V1. A fully connected layer is added on top for classification.

\textbf{Two-stream networks with different stream fusion techniques}.
Predictions from an RGB frame are averaged with predictions from pre-computed
optical flow for ten frames surrounding the RGB frame.

The flow stream has twice as many input channels as flow frames: one for each
of up and down flow directions.

The two streams are fused after the last convolutional layer.

Inputs to the two-stream model are a sequence of five frames, sampled ten
frames apart from a 25 fps video, as well as the corresponding optical flow
features.

Features from the $(5, 7, 7)$ activations before the last average pooling layer
of Inception-V1 are passed through a $C_{512} \rightarrow P_3 \rightarrow FC_{?}$ 3D
convolutional network, where $P_3$ is a $(3, 3, 3)$ max-pooling layer. These
layers are initialized with Gaussians.

The averaging process is learnable.

\textbf{C3D}. A network with eight convolutional layers, five pooling layers
and two fully connected layers at the top is used. Inputs are 16-frame,
$(112, 112)$ clips, as in~\cite{DBLP:journals/corr/TranBFTP14}. Batch
normalization is used after each (convolutional and fully connected) layer. In
the first layer, a temporal stride of two (compared with one) is used, in order
to fit 15 videos per batch per K40 GPU\@.

\subsubsection{Inference}

Video streams are decoded at 25 fps.

Standard SGD with momentum set to 0.9 is used, with synchronous data
parallelization across 32 GPUs for all models except C3D, for which data
parallelization across 64 GPUs was used.

Models are trained on miniKinetics for 35k steps, and for 100k steps on
Kinetics, with a 10x reduction on learning rate when validation loss plateaued.

Models were trained for up to 5k steps on UCF-101 and HMDB-51, using 16 GPUs.

\textbf{Data augmentations} used are random cropping, by resizing the smaller
video side to 256 pixels then randomly cropping a $(224, 224)$ patch, and
temporal random cropping. Random left-right flipping was applied.

Shorter videos are looped to satisfy each model's input requirements.

Training with \textbf{Two-Stream Inflated 3D ConvNets (I3D)} is done with
64-frame snippets, and test is done using entire videos.

Test inference is done by taking $(224, 224)$ center crops.

Optical flow is computed using the TV-$L^1$ algorithm
from~\cite{Zach07aduality}, which runs at 30 fps on $(320, 240)$ videos (on
decade-old GPU technology).

\subsubsection{Criticism}

For \textbf{2D ConvNets with LSTMs on top}, a cross-entropy loss is placed on
the outputs at all time steps.

\subsubsection{Results}

78.7\% on miniKinetics with Two-Stream I3D, compared with 74.0\% for 3D-Fused,
72.9\% for Two-Stream, 60.0\% for C3D and 69.9\% for Conv2D with LSTM\@.

Using pre-training on Kinetics, 98.0\% and 80.7\% is achieved, compared with
state of the art of 94.6\% and 70.3\% respectively on UCF-101 and HMDB-51.

% TODO(brendan): A couple more interesting findings from the results, e.g. with
% regards to optical flow.

\subsubsection{Questions}

\begin{itemize}
        \item \textbf{Future work?} Action tubes, or attention mechanisms to
                focus in on human actors.

                The authors suggest that the optical flow streams produce
                significant improvements, compared to the RGB stream alone, due
                to their optical flow algorithm's recurrent nature
                (optimization is done iteratively).  This could be disproved by
                using purely feed-forward optical flow computations, e.g.
                FlowNet 2.0.

                The authors mention specifically that they will repeat all
                experiments with Kinetics instead of miniKinetics, and compare
                with and without ImageNet pre-training. They will also compare
                inflating of different 2D ConvNet architectures (besides
                Inception-V1).
\end{itemize}

\subsubsection{Similar Work}

Convolutional Two-Stream Network Fusion for Video Action
Recognition~\cite{DBLP:journals/corr/FeichtenhoferPZ16} builds on two-stream
networks by fusing the streams' respective features earlier.

\subsection{Learning Spatiotemporal Features with 3D Convolutional
            Networks\cite{DBLP:journals/corr/TranBFTP14}}

\subsubsection{Data}

UCF-101~\ref{ucf101}.

Sports-1M~\cite{KarpathyCVPR14} (1.1 million sports videos, with 487 classes).

I380K (internal dataset, finetuned from).

\subsubsection{Model}

A Conv3D model is used, with $(3, 3, 3)$ kernels and $(2, 2, 2)$ max-pooling
layers, with the exception of the first max-pooling layer, which is
$(1, 2, 2)$.

Full model structure (testing effect of kernel size):
$C_{64} \rightarrow P_1 \rightarrow C_{128} \rightarrow P_2 \rightarrow
{(C_{256} \rightarrow P_2)}^3 \rightarrow {(FC_{2048})}^2 \rightarrow
\textrm{softmax}$, where $P_1$ refers to the $(1, 2, 2)$ max-pooling layer, and
$P_2$ refers to the $(2, 2, 2)$ max-pooling layer.

Full model structure (best results):
$C_{64} \rightarrow P_1 \rightarrow C_{128} \rightarrow P_2 \rightarrow
{(C_{256})}^2 \rightarrow P_2 \rightarrow
{({(C_{512})}^2 \rightarrow P_2)}^2 \rightarrow {(FC_{4096})}^2 \rightarrow
\textrm{softmax}$.

The last fully connected layer above goes to an $L2$ normalization, followed by
a linear SVM\@.

\subsubsection{Inference}

Inputs were resized to $(171, 128)$ then randomly cropped to $(112, 112)$.

Non-overlapping 16-frame clips were used during training, therefore batches of
$(\verb|batch_size|, 16, 112, 112, 3)$ were input to the model.

Hyper-parameters: batch size of 30, learning rate of 0.003 divided by four
every four epochs, for sixteen in total.

Five two-second long clips were extracted from each video and trained on.

For predictions on UCF101, a trained C3D model was used to extract features
from 16-frame clips, with overlap of 8-frames between subsequences. A linear
SVM was then trained on these C3D features.

\subsubsection{Criticism}

Cross-entropy on activity label?

\subsubsection{Results}

90.4\% on UCF-101, compared with 89.1\% state-of-the-art at the time.

84.4\% vs. $> 90\%$ on Sports-1M, although the technique
from\cite{DBLP:journals/corr/NgHVVMT15} could be applied on top of C3D
features.

On UCF-101, homogeneous temporal depth of kernels in the model were able to
beat increasing or decreasing temporal depth.

\subsubsection{Questions}

\begin{itemize}
\item \textbf{Any comparison with frame-frame ImageNet vectors?} None. The only
        comparison was with ImageNet features averaged over all frames.

\item \textbf{Which features complement C3D?} iDT\@? Optical flow?
\end{itemize}

\subsubsection{Similar Work}

Improved Dense Trajectories\cite{Wang2013}.

Learning Hierarchical Invariant Spatio-temporal Features for Action Recognition
with Independent Subspace Analysis\cite{Le:2011:LHI:2191740.2192108}.
Unsupervised video feature learning using stacking.

Two-stream networks\cite{DBLP:journals/corr/SimonyanZ14}.

Beyond Short Snippets: Deep Networks for Video
Classification\cite{DBLP:journals/corr/NgHVVMT15}. Connects Conv2D to LSTM, and
explores other temporal feature pooling models that work on Conv2D-extracted
features.

\subsection{CDC\@: Convolutional-De-Convolutional Networks for Precise Temporal
            Action Localization in Untrimmed
            Videos\cite{DBLP:journals/corr/ShouCZMC17}}

\subsubsection{Data}

THUMOS~2014~\cite{DBLP:journals/corr/IdreesZJGLSS16} consists of 101 action
classes. THUMOS action classes are from UCF-101~\ref{ucf101}. All videos are
from YouTube, and are labelled with action class and temporal span of the
action. THUMOS~2014 contains 20 action classes, 2755 trimmed training videos
and 1010 untrimmed validation videos containing 3007 action instances. 213 test
videos, with 3358 action instances, exist that are not entirely background.

The THUMOS~2015 dataset extends THUMOS~2014 with a total of 5613 positive and
background untrimmed videos.

ActivityNet challenge 2016~\cite{Heilbron_2015_CVPR} has 203 activity
categories, with 137 untrimmed videos per class on average. ActivityNet 2016
has an average of 1.41 activity instances per video, and 849 hours of video in
total. Activitynet 2016 encompasses untrimmed video classification, trimmed
video classification and activity detection.

\subsubsection{Model}

The C3D architecture
from~\cite{DBLP:journals/corr/TranBFTP15, DBLP:journals/corr/TranBFTP14} up to
and including layer \verb|pool5| is built upon. \verb|pool5| is changed to only
pool in spatial dimensions.

\verb|pool5| is then followed by \verb|conv6|, which is a layer with filter
size $[4, 4, 4]$, with strides $[2, 1, 1]$, and the temporal part of the filter
doing an up-convolution while the spatial part does down-convolution (?). No
spatial padding, and ``same'' temporal padding is used, such that the input
$[L/8, 4, 4]$ feature map becomes a $[L/4, 1, 1]$ feature map.

This layer is called \verb|CDC6|, after ``conv-deconv'' layers, and is followed
by two temporal up-convolutional layers (both of filter size $[4, 1, 1]$ and
stride $[2, 1, 1]$) for a final output of shape $[L, 1, 1]$.  The pre-logits
layers \verb|CDC6| and \verb|CDC7| have 4096 channel outputs.

Dropout is used for all \verb|CDC| layers, with 0.5 dropout ratio.

\subsubsection{Inference}

Non-overlapping segments of 32 frames are extracted and fed to the CDC network.
During training, only segments with at least one frame with a non-background
label are used.

C3D is pre-trained on Sports-1M~\cite{KarpathyCVPR14}, and after this the CDC
network converges within four epochs.

Both CDC and C3D layers are trained jointly, with a learning rate of
$10^{-5}$ used for all layers except \verb|CDC8|, for which a learning rate
of $10^{-4}$ is used.

At test time, segment proposals from~\cite{DBLP:journals/corr/ShouWC16} are
fed to the CDC network, with the proposals expanded by $1/8$ of the original
proposal length on either end.

Using the per-frame predictions on the segment proposal, Gaussian kernel
density estimation is used to obtain $\mu$ and $\sigma$, and a refined segment
from $[\mu - \sigma, \mu + \sigma]$ is produced. The predicted class scores are
the average class scores over this refined segment.

To make segment proposals for the temporal localization task, non-maximum
suppression is used as
in~\cite{DBLP:journals/corr/YeungRJAML15, DBLP:journals/corr/ShouWC16}.

\subsubsection{Criticism}

A cross-entropy per-frame loss is used on the class predictions.

As an evaluation metric, mean average precision is computed over the
predictions for each frame.

For evaluation on the temporal localization task, mAP is computed over
different IoU thresholds.

\subsubsection{Results}

Results on THUMOS~2014 are given below.

Per-frame: 44.4 mAP compared with 41.3 mAP from
MultiLSTM~\cite{DBLP:journals/corr/YeungRJAML15}, and 41.7 mAP using
conv-deconv instead of CDC\@.

Temporal localization at 0.5 IoU\@: 23.3 mAP, as compared with 19.0 mAP
in~\cite{DBLP:journals/corr/ShouWC16}.

On ActivityNet (at 0.75 IoU): 26.0 when based on the segment proposals
of Wang and Tao in their ActivityNet 2016 challenge submission, as compared to
4.1 mAP without CDC\@. At 0.5 IoU, CDC only improves the method of Wang and Tao
from 45.1 to 45.3 mAP\@.

\subsubsection{Questions}

Tried conv-deconv vs. CDC, but what about deconv-conv? Or a series of 3d
convolutions with spatial stride fixed, until the last layer? I.e.\ test
whether the CDC filter is special when compared to simply adding more
parameters to the temporal up-convolutional filters.

\bibliographystyle{apalike}
\bibliography{ml-reading-notes}

\end{document}
