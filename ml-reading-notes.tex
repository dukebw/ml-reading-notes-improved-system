\documentclass[a4paper, 12pt]{article}

\usepackage{cite}
\usepackage{amsmath}
\usepackage{fullpage}
\usepackage{url}
\usepackage{graphicx}
\usepackage{inputenc}
\usepackage{titling}
\usepackage{hyperref}

\date{\today}
\title{Machine Learning Reading Notes} 

\author{Brendan Duke}

\begin{document}

\maketitle

\section{Definitions}

\textbf{Deep Neural Networks (DNNs)} are engineered systems inspired by the
biological brain\cite{Goodfellow-et-al-2016-Book}.

\textbf{Mahalanobis Distance}

\textbf{Neighbourhood Components Analysis (NCA)} is a method of learning a
Mahalanobis distance metric, and can also be used in linear dimensionality
reduction\cite{NIPS2004_2566}.

The \textbf{PCKh} metric, used by the MPII Human Pose Dataset, defines a joint
estimate as matching the ground truth if the estimate lies within 50\% of the
head segment length\cite{andriluka-2d-2014-853}. The head segment length is
defined as the diagonal across the annotated head rectangle in the MPII data,
multiplied by a factor of 0.6. Details can be found by examining the MATLAB
\href{http://human-pose.mpi-inf.mpg.de/results/mpii_human_pose/evalMPII.zip}{evaluation script}
provided with the MPII dataset.

\section{Paper Summaries}
 
\subsection{DeepPose: Human Pose Estimation via Deep Neural
            Networks\cite{DBLP:journals/corr/ToshevS13}}

This paper uses DNNs as a method for human pose estimation, based on the
success of \cite{NIPS2013_5207} and \cite{DBLP:journals/corr/GirshickDDM13} for
object detection using DNNs.

This is in contrast to the existing work in human pose estimation at the time,
which focused on explicitly designed pose models. Papers about these methods
can be found in the ``Related Work'' section of
\cite{DBLP:journals/corr/ToshevS13}.

The input to the 7-layered convolutional DNN (based on
AlexNet\cite{NIPS2012_4824}) is the full image.

\subsection{Dropout: A Simple Way to Prevent Neural Networks from
            Overfitting\cite{Srivastava:2014:DSW:2627435.2670313}}

\textbf{Dropout} is a technique used to overcome the problem of overfitting in
deep neural nets with large numbers of parameters. The idea is to train using
many ``thinned'' networks, chosen by randomly removing subsets of units and
their connections. The predictions from the thinned networks are approximately
averaged at test time by using a single, unthinned, network with reduced
weights.

\begin{itemize}
        \item Existing regularization methods: stopping training as soon as
                validation error stops improving, L1 and L2 regularization, and
                weight sharing\cite{Nowlan:1992:SNN:148167.148169}.
\end{itemize}

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,ml-reading-notes}

\end{document}
