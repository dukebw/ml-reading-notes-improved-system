% TODO(brendan): Question to answer: what is the effect of attaching multiple
% losses on multiple heads of a network, in terms of their respective influence
% on the update of weights in lower layers of the network during
% backpropagation?
% Are the relative magnitudes of the losses from the different heads important
% in terms of their influence on weight updates? Will a loss with a magnitude
% 10^3 times larger dominate a 10^3 times smaller loss in influencing weight
% updates?

\documentclass[a4paper, 12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbold}
\usepackage{bm}
\usepackage{cite}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{inputenc}
\usepackage{mathtools}
\usepackage{natbib}
\usepackage{titling}
\usepackage{siunitx}
\usepackage{txfonts}
\usepackage{url}

\newcommand{\expect}{\operatorname{E}\expectarg}
\DeclarePairedDelimiterX{\expectarg}[1]{[}{]}{%
  \ifnum\currentgrouptype=16 \else\begingroup\fi
  \activatebar#1
  \ifnum\currentgrouptype=16 \else\endgroup\fi
}

\newcommand{\innermid}{\nonscript\;\delimsize\vert\nonscript\;}
\newcommand{\activatebar}{%
  \begingroup\lccode`\~=`\|
  \lowercase{\endgroup\let~}\innermid 
  \mathcode`|=\string"8000
}

\newcommand\phantomlabel[1]{\phantomsection\label{#1}}

\DeclarePairedDelimiter\norm{\lVert}{\rVert}%

\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
        #1\;\delimsize\|\;#2%
}
\newcommand{\infdiv}{D_{KL}\infdivx}

\date{\today}
\title{Machine Learning Reading Notes}

\author{Brendan Duke}

\begin{document}

\maketitle


\part{Definitions}


% TODO(brendan):
\phantomlabel{backprop}
\textbf{Back propagation}


\phantomlabel{batchnorm}
\textbf{Batch Normalization}


\textbf{Deep Neural Networks (DNNs)} are engineered systems inspired by the
biological brain\citet{Goodfellow-et-al-2016-Book}.

\phantomlabel{fisher-info}
The \textbf{Fisher information matrix} is the expected value of the observed
information matrix, which is the gradient of the negative score function, or
equivalently the Hessian of the negative log-likelihood. I.e.,

\begin{align*}
        F(\hat{\theta} | \theta^*) &= \expect{J(\hat{\theta} | D)} \\
                                   &= -\nabla s(\hat{\theta}) \\
                                   &= -\nabla^2 \log p(D \mid \hat{\theta})
\end{align*}

where $s(\hat{\theta})$ is the score function.

\phantomlabel{kl}
\textbf{KL divergence} is given in Equation~\ref{kleqn}\citet[Chapter~3]{Goodfellow-et-al-2016-Book}.

\begin{equation}
        \infdiv{P}{Q} = \varmathbb{E}_{x \sim P}\left[\log P(x) - \log Q(x)\right]
        \label{kleqn}
\end{equation}

\textbf{Cross-entropy} is related to \hyperref[kl]{KL divergence} by
$H(P, Q) = H(P) + \infdiv{P}{Q}$, where $H(P)$, the \textbf{Shannon entropy}, is
$H(P) = -\varmathbb{E}_{x \sim P} \left[\log P(x)\right]$.


\phantomlabel{LSTM}
% TODO(brendan): Diagrams and equations for LSTM. Definition for RNNs. Should
% be a distillation of Chapter 10 of Goodfellow book.
\textbf{LSTM} (Long Short Term
Memory) neural networks are a type of recurrent neural network whose
characteristic feature is the presence of a gated self-loop that allows
retention of its ``cell state'', which are the pre-non-linearity activations of
the previous time step\citet[Chapter~10]{Goodfellow-et-al-2016-Book}.

Cell state is updated at each time step according to
Equation~\ref{lstm_cell_state_update}.

\begin{equation}
        s_i^{(t)} = f_i^{(t)} s_i^{(t - 1)} + g_i^{(t)}
                \sigma \left( b_i + \sum_j U_{i, j} x_j^{(t)} + \sum_j W_{i, j} h_j^{(t - 1)}\right)
        \label{lstm_cell_state_update}
\end{equation}

The vectors $\boldsymbol{f^{(t)}}$ and $\boldsymbol{g^{(t)}}$ in
Equation~\ref{lstm_cell_state_update} also take inputs from $\boldsymbol{x^{(t)}}$
and $\boldsymbol{h^{(t - 1)}}$, with their own weight tensors and bias vectors
$\boldsymbol{U}^f$, $\boldsymbol{W}^f$ and $\boldsymbol{b}^f$, $\boldsymbol{U}^g$,
$\boldsymbol{W}^g$ and $\boldsymbol{b}^f$, respectively.

Similar gate functions exist to gate the inputs and outputs to the LSTM, as
well.


\textbf{Mahalanobis Distance}


\phantomlabel{multilayer_perceptron}
\textbf{Multi-Layer Perceptrons (MLPs)} are mathematical functions mapping some
set of input values to some set of output
values\citet{Goodfellow-et-al-2016-Book}.


\textbf{Neighbourhood Components Analysis (NCA)} is a method of learning a
Mahalanobis distance metric, and can also be used in linear dimensionality
reduction\citet{NIPS2004_2566}.

The \textbf{PCKh} metric, used by the MPII Human Pose Dataset, defines a joint
estimate as matching the ground truth if the estimate lies within 50\% of the
head segment length\citet{andriluka-2d-2014-853}. The head segment length is
defined as the diagonal across the annotated head rectangle in the MPII data,
multiplied by a factor of 0.6. Details can be found by examining the MATLAB
\href{http://human-pose.mpi-inf.mpg.de/results/mpii_human_pose/evalMPII.zip}{evaluation script}
provided with the MPII dataset.


\phantomlabel{nonmax_supression}
\textbf{Non-maximum suppression} in object detection, in general, is a set of
methods used to prune an initial set of object bounding boxes that may be
uncorrelated with the actual object detections in an image, down to a subset
that are\citet{DBLP:conf/accv/RotheGG14}. In edge detection, non-maximum
suppression is used to suppress any pixels (i.e.\ not include them in the set of
detected edges) that are not the maximum response in their neighbourhood.

The \textbf{softmax function} is a continuous differentiable version of the
argmax function, where the result is represented as a one-hot
vector\citet[Chapter~6]{Goodfellow-et-al-2016-Book}. Softmax is a way of
representing probability distributions over a discrete variable that can take
on $n$ possible values.

Formally, softmax is given by Equation~\ref{softmax_eqn}.

\begin{equation}
        \textrm{softmax}{(\boldsymbol{z})}_i = \frac{e^{z_i}}{\sum_je^{z_j}}
        \label{softmax_eqn}
\end{equation}


\phantomlabel{rectified_linear_units}
\textbf{Rectified linear units (ReLUs)}\citet{icml2010_NairH10}


\textbf{Leaky rectified linear units (LReLUs)}\citet{maas_rectified_nonlinearities}

\begin{equation}
        h^{(i)} = \max\left(w^{(i)T}x, 0.01w^{(i)T}x\right) =
        \begin{cases}
                w^{(i)T}x & \textrm{if } w^{(i)T}x > 0 \\
                0.01w^{(i)T}x & \textrm{otherwise}
        \end{cases}
\end{equation}

Leaky ReLUs have non-zero gradient over their domain, and were therefore
motivated in reducing the vanishing gradient problem.


\phantomlabel{dilated_convolutions}
\textbf{Dilated Convolutions}~\citet{DBLP:journals/corr/YuK15} can be defined as
follows.

Let $F: \mathbb{Z}^2 \rightarrow \mathbb{R}$ be a discrete
function, let $\Omega_r = {[-r, r]}^2 \cap \mathbb{Z}^2$ and let
$k: \Omega_r \rightarrow \mathbb{R}$ be a discrete filter of size
${(2r + 1)}^2$. Then the convolution operator is defined by
Equation~\ref{conv-op}.

\begin{equation}
        (F * k)(\mathbf{p}) = \sum_{\mathbf{s} + \mathbf{t} = \mathbf{p}} F(\mathbf{s}) + k(\mathbf{t})
        \label{conv-op}
\end{equation}

Furthermore, the dilated convolution operator, denoted by $*_l$, is defined by
Equation~\ref{dilated-conv-op}.

\begin{equation}
        (F *_l k)(\mathbf{p}) = \sum_{\mathbf{s} + l\mathbf{t} = \mathbf{p}} F(\mathbf{s}) + k(\mathbf{t})
        \label{dilated-conv-op}
\end{equation}


\part{Book Summaries}


\section{Machine Learning: A Probabilistic Perspective}
% TODO(brendan): citation


\subsection{Probability}

Bayes rule is:

\begin{equation}
        p(Y = y | X = x) = \frac{p(Y = y, X = x)}{p(X = x)}
                = \frac{p(X = x) p(Y = y | X = x)}
                       {\sum_{x'} p(X = x') p(Y = y | X = x')}
\end{equation}


\subsection{Generative Models for Discrete Data}

The \textbf{maximum a posteriori (MAP)} estimate is defined as
$\hat{y} = \textrm{argmax}_{c} p(y = c | \mathbf{x}, D)$.

A \textbf{prior} is a probability distribution assigned to each hypothesis $h
\in \mathcal{H}$ in the hypothesis space, based only on knowledge external to
the training data. E.g.\ in the space of related numbers under 100, a strong
prior may be assigned to ``odd numbers'' or ``even numbers'', while a weak
prior would be assigned to unintuitive concepts such as ``all powers of 2
except 32''.

The \textbf{likelihood} of a hypothesis given a set of training data, is the
probability of randomly sampling exactly that set of training data, given the
hypothesis, i.e. $p{(\mathcal{D} | h)} = {(1 / |h|)}^N$.

The \textbf{posterior} is:

\begin{equation}
        p(h | \mathcal{D})
                = \frac{p(h) p(\mathcal{D} | h)}
                       {\sum_{h' \in \mathcal{H}} p(\mathcal{D} | h')}
\end{equation}

where $p(\mathcal{D} | h)$ is ${(1 / |h|)}^N$ if the training data match the
hypothesis $h$, otherwise zero.

The MAP is
$\textrm{argmax}_h p(h) p(\mathcal{D} | h) = \textrm{argmax}_h \left[ \log{p(h)} + \log{p(\mathcal{D} | h)} \right]$.
In the limit of infinite training data, the term exponential in $N$ in
$\log{p(\mathcal{D} | h)}$ will dominate. Therefore the
\textbf{maximum likelihood estimator (MLE)}
$\textrm{argmax}_h \log{p(\mathcal{D} | h)}$ is converged to by the MAP in the
limit of infinite data, justifying the MLE's use as an objective function.


\section{Deep Learning~\citet{Goodfellow-et-al-2016-Book}}


\subsection{Machine Learning Basics}

Maximum Likelihood Estimation (MLE) is maximization of the log-likelihood
$\log p_{model}(y | \mathbf{x}; \mathbf{\theta})$, where $y$ is a ground truth
example, $\mathbf{x}$ is an input feature vector, and $\mathbf{\theta}$ are
model parameters.

Principal Component Analysis (PCA) involves projecting input feature vectors
$\mathbf{x}$ into a reduced-dimensionality space via multiplication by
$D \in \mathbb{R}^{n \times l}$. The PCA projection minimizes the $L_2$
reconstruction error $||\mathbf{x} - r(\mathbf{x})||_2$.

Stocastic Gradient Descent (SGD) is gradient descent over minibatches. For an
objective function $L = f(y; x, \theta)$, at each step $t$ we have
$\theta_t = \frac{1}{m'} \sum_{i = 0}^{m'} \theta_{t - 1} - \epsilon \nabla f$.


\part{Datasets}


\section{CIFAR-10~\citet{cifar10-website}}
\label{cifar10}

CIFAR-10~\citet{cifar10-website} consists of \num{60000} colour images of $32
\times 32$ resolution, has 10 classes and \num{6000} images per class.


\section{HMDB-51~\citet{Kuehne11}}
\label{hmdb51}

HMDB-51 contains 7000 clips extracted from movies and YouTube then manually
annotated with 51 class labels.


\section{Kinetics~\citet{kay2017kinetics}}
\label{kinetics}

The Kinetics Human Action Video Dataset\citet{kay2017kinetics}, which has 400
human action classes each with more than 400 examples. The classes are focused
on human actions, such as pouring or kissing, as opposed to activities (such as
tennis or baseball). The clips are 10s long. There are roughly \num{300000}
labelled clips in total in the dataset.

The Kinetics test set is 100 clips for each class.


\section{THUMOS~2014~\citet{DBLP:journals/corr/IdreesZJGLSS16}}
\label{thumos}

THUMOS~2014~\citet{DBLP:journals/corr/IdreesZJGLSS16} consists of 101 action
classes. THUMOS action classes are from UCF-101~\ref{ucf101}. All videos are
from YouTube, and are labelled with action class and temporal span of the
action. THUMOS~2014 contains 20 action classes, 2755 trimmed training videos
and 1010 untrimmed validation videos containing 3007 action instances. 213 test
videos, with 3358 action instances, exist that are not entirely background.

The THUMOS~2015 dataset extends THUMOS~2014 with a total of 5613 positive and
background untrimmed videos.


\section{UCF-101\citet{DBLP:journals/corr/abs-1212-0402}}
\label{ucf101}

101 classes, 13k clips and 27 hours of video data in total.


\section{WMT~2014~\citet{wmt14-translation-website}}
\label{wmt2014}

WMT~2014 is a machine translation task with five language pairs:
French-English, Hindi-English, German-English, Czech-English and
Russian-English. A number of parallel and monolingual corpora are included as
training data.

The majority of the training data is taken
from~\href{http://www.statmt.org/europarl/}{Europarl}~v7, from which about
50~million words per language are used in WMT~2014. Additional training data is
taken from the~\href{http://www.casmacat.eu/corpus/news-commentary.html}{News
Commentary Parallel Corpus}, at about three million words per language.

Test data from previous years is provided, with on the order of~\num{3000}
sentences per test set.


\part{Paper Summaries}


\section{Human Pose}


\subsection{DeepPose: Human Pose Estimation via Deep Neural
            Networks\citet{DBLP:journals/corr/ToshevS13}}

This paper uses DNNs as a method for human pose estimation, based on the
success of~\citet{NIPS2013_5207} and~\citet{DBLP:journals/corr/GirshickDDM13} for
object detection using DNNs.

This is in contrast to the existing work in human pose estimation at the time,
which focused on explicitly designed pose models. Papers about these methods
can be found in the ``Related Work'' section of
\citet{DBLP:journals/corr/ToshevS13}.

The input to the 7-layered convolutional DNN (based on
AlexNet\citet{NIPS2012_4824}) is the full image.


\subsection{End-to-end people detection in crowded
            scenes\citet{DBLP:journals/corr/StewartA15}}

This paper is focused on jointly creating a set of bounding-box predictions for
people in crowded scenes using GoogLeNet and a
\hyperref[LSTM]{recurrent LSTM layer} as a controller. Since bounding-box
predictions are generated jointly, common post-processing steps such as
\hyperref[nonmax_supression]{non-maximum suppression} are unnecessary.  All
components of the system are trained end-to-end using back propagation.

\subsubsection{Motivation}

The end-to-end people detection method is contrasted with the object detection
methods of R-CNN in~\citet{DBLP:journals/corr/GirshickDDM13} and OverFeat in
\citet{DBLP:journals/corr/SermanetEZMFL13}.
\citet{DBLP:journals/corr/GirshickDDM13} and
\citet{DBLP:journals/corr/SermanetEZMFL13} rely on non-maximum suppression,
which does not use access to image information to infer bounding box positions
since non-maximum suppression acts only on bounding boxes. Also, in end-to-end
people detection, the decoding stage is learned using LSTMs, instead of using
specialized methods as in~\citet{VisualPhrases} and~\citet{TaAnSc_14:occluded}.

Early related work can be found in~\citet{Felzenszwalb:2010:ODD:1850486.1850574}
and~\citet{Leibe:2005:PDC:1068507.1069006}. Best performing object detectors at
the time were~\citet{DBLP:journals/corr/GirshickDDM13},
\citet{DBLP:journals/corr/SermanetEZMFL13}, \citet{Uijlings13},
\citet{DBLP:journals/corr/ZhangBS15} and~\citet{DBLP:journals/corr/SzegedyREA14}.

Sequence modeling is done using LSTMs as in
\citet{DBLP:journals/corr/SutskeverVL14} (used for machine translation) and
\citet{DBLP:journals/corr/KarpathyF14} (used for image captioning). The loss
function is similar to the loss function proposed in
\citet{Graves06connectionisttemporal} in that the loss function encourages the
model to make predictions in descending order of confidence.

\subsubsection{Data}

A new training set collected from public webcams, called ``Brainwash'', is
produced. Brainwash consists of 11917 images with 91146 labelled people. 1000
images are allocated for testing and validation, hence training, test and
validation sets contain 82906, 4922 and 3318 labels, respectively.

\subsubsection{Model}

A pre-trained GoogLeNet\citet{going-deeper-szegedy43022} is used to produce
encoded features as input to the LSTM\@. The GoogLeNet features are further
fine-tuned by the training process.  Using GoogLeNet, a feature vector of
length 1024 is produced for each region over a $(15, 20)$ grid of regions that
covers the entire $(480, 640)$ input image. Each cell in the grid has a
receptive field of $(139, 139)$, and is trained to produce a set (with fixed
cardinality five) of distinct bounding boxes in the center $(64, 64)$ region.

$L_2$ regularization of weights in the network was removed entirely.

GoogLeNet activations are scaled down by a factor of 100 before being input to
the decoder, since decoder weights are initialized according to a uniform
distribution in $[-0.1, 0.1]$, while GoogLeNet activations are in $[-80, 80]$.
Regression predictions from GoogLeNet are scaled up by 100 before comparing
with ground truth locations (which are in $[-64, 64]$).

At each step, the LSTM for each grid cell, of which there are 300 in total,
produces a new bounding box and corresponding confidence that the bounding box
contains a person $\boldsymbol{b} = \{\boldsymbol{b}_{pos}, b_c\}$, where
$\boldsymbol{b}_{pos} = (b_x, b_y, b_w, b_h) \in \varmathbb{R}^4$ and
$b_c \in [0, 1]$. The prediction algorithm stops when the confidence drops
below a set threshold. The LSTM units have 250 memory states, no bias units,
and no output non-linearities. Each LSTM unit adds its output to the image
representation, and feeds the result into the next LSTM unit. Comparable
results are found by only presenting the image representation as input to the
first LSTM unit.

Dropout with probability 0.15 is used on the output of each LSTM\@.

\subsubsection{Inference}

The system is trained with learning rate 0.2, decreased by a factor of 0.8
every 100 000 iterations (with convergence occurring after 500 000 iterations),
and momentum 0.5. Gradient clipping is done at 2-norm of 0.1.

Images are jittered by up to 32 pixels in horizontal and vertical directions,
and scaled by a factor between 0.9 and 1.1.

At test time, per-region predictions are merged by adding a new region at each
iteration, and destroying any new bounding boxes that overlap previously
accepted bounding boxes, under the constraint that any given bounding box can
destroy at most one other bounding box. An ordering function
$\Delta': A \times C \rightarrow \varmathbb{N} \times \varmathbb{R}$ given by
$\Delta'(\boldsymbol{b}_i, \tilde{\boldsymbol{b}}_j) = (m_{ij}, d_{ij})$ where
$m_{ij}$ denotes intersection of boxes and $d_{ij}$ is $L_1$ displacement, is
minimized using the Hungarian algorithm in order to find a bipartite matching.
At each step, any new candidate that is not intersecting in the matching is
added to the set of accepted candidates.

\subsubsection{Criticism}

A new loss function that operates on sets of bounding-box predictions is
introduced. Denoting bounding boxes generated by the model as
$C = \{\tilde{\boldsymbol{b}}_i\}$, and ground truth bounding boxes by
$G = \{\boldsymbol{b}_i\}$, the loss function is given by
Equation~\ref{loss-eqn}.

\begin{equation}
        L(G, C, f) = \alpha\sum_i^{|G|}
                             l_{pos}\left(\tilde{\boldsymbol{b}}_{pos}^i, \boldsymbol{b}_{pos}^{f(i)}\right) +
                     \sum_j^{|C|} l_c\left(\tilde{b}_c^j, y_j\right)
        \label{loss-eqn}
\end{equation}

In Equation~\ref{loss-eqn}, $f(i)$ is an injective function $G \rightarrow C$
that assigns one ground truth to each index $i$ up to the number of ground
truths, $l_{pos}$ is the $L_1$ displacement between bounding boxes, and $l_c$
is a cross-entropy loss on a candidate's confidence that a bounding box exists,
where $y_j = \mathbb{1}\{f^{-1}(j) \neq \varnothing\}$. $\alpha$ is set to 0.03
from cross-validation.

In creating $f(i)$ in Equation~\ref{loss-eqn} to assign candidate predictions
to ground truths, the
$G \times C \rightarrow \varmathbb{R} \times \varmathbb{N} \times \varmathbb{N}$
function
$\Delta\left(\boldsymbol{b}^i, \tilde{\boldsymbol{b}}^j\right) = (o_{ij}, r_i, d_{ij})$
is used to lexicographically order pairs first by $o$, then $r$, then $d$,
where $o$ is one if there is sufficient overlap between candidate and ground
truth and zero otherwise, $r$ is the prediction's confidence, and $d$ is the
$L_1$ displacement between candidate and ground truth bounding boxes.

\subsubsection{Experiments}

With an AP (average precision) of 0.78 and EER (equal error rate) of 0.81, the
$f(i)$ produced by minimizing $\Delta$, using the
\href{https://en.wikipedia.org/wiki/Hungarian_algorithm}{Hungarian algorithm},
is found to improve on AP and EER compared with a fixed assignment of $f(i)$,
or selecting the first $k$ highest ranked ($L_\textrm{firstk}$). COUNT
(Absolute difference between number of predicted and ground truth detections)
for $f(i)$ with Hungarian was 0.76 compared with 0.74 for $L_\textrm{firstk}$.
As a baseline, Overfeat-GoogLeNet (bounding-box regression on each cell,
followed by non-maximum suppression, as in
\citet{DBLP:journals/corr/SermanetEZMFL13}) achieved 0.67, 0.71 and 1.05 AP, EER
and COUNT, respectively.

Training without finetuning GoogLeNet reduces AP by 0.29.

Removal of dropout from the output of each LSTM decreases AP by 0.011.

When using the original $2^{-4}$ $L_2$ weights regularization multiplier on
GoogLeNet only, the network was unable to train.  An $L_2$ regularization
multiplier on GoogLeNet of $10^{-6}$ reduced AP by 0.03.

It is found that AP (on the validation set) increases from 0.82 to 0.85 when
using separate weights connecting each of the LSTM outputs to predicted
candidates.


\section{Regularization}


\subsection{Dropout: A Simple Way to Prevent Neural Networks from
            Overfitting\citet{Srivastava:2014:DSW:2627435.2670313}}
\label{dropout}

\textbf{Dropout} is a technique used to overcome the problem of overfitting in
deep neural nets with large numbers of parameters. The idea is to train using
many ``thinned'' networks, chosen by randomly removing subsets of units and
their connections. The predictions from the thinned networks are approximately
averaged at test time by using a single, unthinned, network with reduced
weights.

\begin{itemize}
        \item Existing regularization methods: stopping training as soon as
                validation error stops improving, L1 and L2 regularization, and
                weight sharing\citet{Nowlan:1992:SNN:148167.148169}.
\end{itemize}


\section{CNN Architecture}


\subsection{Deep Residual Learning for Image
            Recognition\citet{DBLP:journals/corr/HeZRS15}}

A technique, training of residual functions, is presented for deep neural
network architecture design, which allows training of deeper networks with
improved accuracy compared to not training residual functions.

\citet{going-deeper-szegedy43022} and~\citet{DBLP:journals/corr/SimonyanZ14a} are
referred to as motivating ``very deep'' models.

% TODO(brendan): Read [1], [9] for vanishing gradient, R-CNN series for
% localization.


\section{GANs}


\subsection{Generative Adversarial Networks\citet{NIPS2014_5423}}
\label{gan}

A method of generating probability distributions is presented that can be
trained end-to-end when used with \hyperref[multilayer_perceptron]{MLPs}. In
the given method, a discriminator network $D$ is optimized to distinguish from
ground truth the samples from the generated distribution $G$. $G$ is optimized
to cause $D$ to become $\frac{1}{2}$ everywhere.

Motivation points to~\citet{deepSpeechReviewSPM2012} and~\citet{NIPS2012_4824} as
successful uses of deep discriminative networks for classification based on
\hyperref[backprop]{back propagation}, \hyperref[dropout]{dropout} and
piecewise linear units (such as \hyperref[rectified_linear_units]{ReLUs}).


\subsection{Image-to-Image Translation with Conditional Adversarial
            Networks\citet{DBLP:journals/corr/IsolaZZE16}}

A loss function is presented for image-to-image translation that can be applied
to different tasks, such as colourizing images and reconstructing photos from
label maps or edges.

\href{https://github.com/phillipi/pix2pix}{Code} is available.

\subsubsection{Motivation}

\citet{DBLP:journals/corr/LarsenSW15}, \citet{DBLP:journals/corr/PathakKDDE16}
and~\citet{DBLP:journals/corr/ZhangIE16} are noted as evidence that applying
Euclidean distance loss alone on generated images produces blurry results.

\citet{DBLP:journals/corr/PathakKDDE16} also noted that it was helpful to mix
the GAN objective with a pixel-wise loss such as an L2 loss.

\citet{NIPS2014_5423}, \citet{DBLP:journals/corr/DentonCSF15},
\citet{DBLP:journals/corr/RadfordMC15},
\citet{DBLP:journals/corr/SalimansGZCRC16}, and
\citet{DBLP:journals/corr/ZhaoML16} are mentioned as prior work in
\hyperref[gan]{GANs}, and specifically conditional GANs are explored, as
suggested by~\citet{NIPS2014_5423}.

As opposed to image modelling losses where pixels are conditionally independent
from each other (e.g.~\citet{DBLP:journals/corr/ShelhamerLD16},
\citet{DBLP:journals/corr/XieT15}, \citet{IizukaSIGGRAPH2016},
\citet{DBLP:journals/corr/LarssonMS16} and \citet{DBLP:journals/corr/ZhangIE16}),
a ``structured loss'' is used. Other examples of structured losses are in
\citet{DBLP:journals/corr/ChenPKMY14} (conditional random fields),
\citet{DBLP:journals/corr/DosovitskiyB16} (feature matching),
\citet{DBLP:journals/corr/LiW16} (non-parametric losses),
\citet{DBLP:journals/corr/XieHT15} (pseudo-priors) and
\citet{DBLP:journals/corr/JohnsonAL16} (losses based on matching covariance
statistics).

Previous work on conditional GANs exists in~\citet{DBLP:journals/corr/MirzaO14}
(generating MNIST digits from discrete labels) and
\citet{DBLP:journals/corr/ReedAYLSL16} (image generation from text). Work on
image generation with conditional GANs has focused on image
inpainting\citet{DBLP:journals/corr/PathakKDDE16}, image prediction from a
normal map\citet{DBLP:journals/corr/WangG16}, generating images from user
input\citet{DBLP:journals/corr/WangG16}, predicting future
frames\citet{DBLP:journals/corr/MathieuCL15}, predicting future states based on
time-lapses of objects\citet{DBLP:journals/corr/ZhouB16b}, generating photos of
clothing from input images of clothed people\citet{DBLP:journals/corr/YooKPPK16}
and style transfer\citet{DBLP:journals/corr/LiW16b}.

The generator and discriminator architectures are motivated by
\citet{DBLP:journals/corr/RadfordMC15}.

Instance normalization is introduced in~\citet{DBLP:journals/corr/UlyanovVL16}.

\subsubsection{Data}

The cityscapes\citet{DBLP:journals/corr/CordtsORREBFRS16} (semantic labels
$\leftrightarrow$ photo), CMP Facades (architectural labels $\rightarrow$
photo), Google maps (map $\leftrightarrow$ aerial photo),
ImageNet\citet{DBLP:journals/corr/RussakovskyDSKSMHKKBBF14} (BW $\rightarrow$
colour), \citet{zhu2016generative} and~\citet{fine-grained} (edges $\rightarrow$
photo using the HED edge detector\citet{DBLP:journals/corr/XieT15}),
\citet{Eitz:2012:HSO:2185520.2185540} (sketch $\rightarrow$ photo) and
\citet{Laffont14} (day $\rightarrow$ night).

\subsubsection{Model}

A U-Net\citet{DBLP:journals/corr/RonnebergerFB15} architecture is used for the
generator. A PatchGAN architecture\citet{DBLP:journals/corr/LiW16b} is used for
the discriminator.

Generator architecture: encoder: $C64-C128-C256-C512-C512-C512-C512-C512$,
decoder: $CD512-CD512-CD512-C512-C512-C256-C128-C64$, where $Ck$ denotes
Convolution-BatchNorm-ReLU and $CDk$ denotes
Convolution-BatchNorm-Dropout-ReLU with a dropout rate of $50\%$. After the
last layer, a convolution maps to the number of output channels, followed by a
$\tanh$ function.

In the case of the U-Net, there are skip connections between the $i$th level of
the encoder and the $(n - i)$th layer of the decoder.

The $(70, 70)$ discriminator architecture is $C64-C128-C256-C512$, $(1, 1)$ and
$(16, 16)$ are $C64-C128$, and $(256, 256)$ is $C64-C128-C256-C512-C512-C512$.

All ReLUs in the encoder are leaky, with slope 0.2.

\subsubsection{Inference}

Network weights were initialized from a Gaussian distribution with mean zero
and standard deviation 0.02.

Noise was supplied in the form of dropout, which is applied both at training
and test time.

\hyperref[batchnorm]{Batch normalization} is applied using the statistics of
the test batch, instead of the aggregated training data statistics. Doing so
with a batch size of one is instance
normalization\citet{DBLP:journals/corr/UlyanovVL16}. Batch sizes of one and four
were used.

Mini-batch SGD is used along with the Adam optimizer. One gradient descent step
on G is used, followed by a gradient descent step on D.

Random jitter was applied by resizing the $(256, 256)$ input to $(286, 286)$
and random cropping back down to $(256, 256)$.

Refer to Appendix 5.1.2 of~\citet{DBLP:journals/corr/IsolaZZE16} for specific
training details for each dataset.

\subsubsection{Criticism}

The objective function of a conditional GAN is shown in Equation~\ref{discrim_condition_eqn}.

\begin{equation}
        \mathcal{L}_{cGAN} = \varmathbb{E}_{x, y \sim p_{data}(x, y)}\left[\log D(x, y)\right] +
                             \varmathbb{E}_{x \sim p_{data}(x), y \sim p_z (z)}\left[\log \left(1 - D\left(x, G(x, z)\right)\right)\right]
        \label{discrim_condition_eqn}
\end{equation}

Equation~\ref{discrim_no_condition_eqn} is the objective function where the
discriminator has no prior information about $x$.

\begin{equation}
        \mathcal{L}_{cGAN}\left(G, D\right) =
                \varmathbb{E}_{y \sim p_{data}(y)}\left[\log D(y)\right] +
                \varmathbb{E}_{x \sim p_{data}(x), z \sim p_z (z)}\left[\log \left(1 - D\left(G(x, z)\right)\right)\right]
        \label{discrim_no_condition_eqn}
\end{equation}

An $L1$ loss is attached, as given in Equation~\ref{l1_loss}.

\begin{equation}
        \mathcal{L}_{L1}\left(G\right) = \varmathbb{E}_{x, y \sim p_{data}(x, y)}\left[\norm{y - G(x, z)}_1\right]
        \label{l1_loss}
\end{equation}

The final objective function is
$G^* = \arg\min_G \max_D \mathcal{L}_{cGAN}\left(G, D\right) + \lambda \mathcal{L}_{L1}\left(G\right)$.

\subsubsection{Experiments}

It was found in initial experiments that $G$ learned to ignore its input noise
$z$.

Only minor variation due to the dropout noise applied is observed in the
generated samples.

Engineering generative networks that produce stochastic dependence on noise
input is left as an open research problem.

Little difference was found between using batch sizes of one and four.

Inputs and outputs in all experiments are 1--3 channel images.

Amazon Mechanical Turk was used to show participants a ground truth or
generated image at $(256, 256)$ resolution for one second, after which the
participant had to respond whether the shown image was real or fake.

$6.1\% \pm 1.3\%$ and $18.9\% \pm 2.5\%$ of participants labelled photo
$\rightarrow$ map and map $\rightarrow$ photo generated images as real,
respectively, with L1 + cGAN loss improving over L1 alone.

On colourization, the cGAN achieved $22.5\% \pm 1.6\%$ of responses as
``real'', as compared with $27.8\% \pm 2.7\%$ in
\citet{DBLP:journals/corr/ZhangIE16}.

FCN-8 was trained for semantic classification on a real dataset, after which
its accuracy on the $(70, 70)$ PatchGAN on cityscapes hit 0.63 per-pixel, 0.21 per
class and 0.16 class IoU, higher than $(1, 1)$ and $(256, 256)$ PatchGAN
discriminators.

% NOTE(brendan): 1x1, 70x70 and 256x256 used differing numbers of layers in
% their architectures, so there is no control for number of layers vs.
% receptive field causing the improved performance.

L1 + cGAN loss is found to perform better overall than L1 + GAN or cGAN
objective functions, with the FCN-8 metric scores given above.

Colour distributions in lab colour space are compared for different objective
functions, with L1 loss producing a narrower colour distribution, and cGAN loss
producing a colour distribution closer to that of the ground truth.

The U-Net qualitatively achieves better generated results with both cGAN and L1
loss than an encoder-decoder without skips, with the latter collapsing to
nearly identical results for all label maps.

A generator is trained on $(256, 256)$ images, and evaluated on $(512, 512)$
map $\leftrightarrow$ aerial images.

A cGAN is trained on semantic segmentation labelling of
Cityscapes\citet{DBLP:journals/corr/CordtsORREBFRS16} and achieves 0.22 class
IoU with cGAN loss alone, compared with 0.35 class IoU with L1 loss and 0.80
class IoU using the wide ResNets of~\citet{DBLP:journals/corr/WuSH16e}.


\section{Human Activity Recognition}


\subsection{Quo Vadis, Action Recognition? A New Model and the Kinetics
            Dataset\citet{carreira2017quo}}

\subsubsection{Data}

\hyperref[kinetics]{Kinetics}.

A miniKinetics version of the dataset was used in the paper, and for
experimentation, with 213 classes and 120k clips split into three subsets: one
for training with 150 to 1000 clips per class, and one split each for
validation and test, containing 25 and 75 clips per class each, respectively.

\hyperref[ucf101]{UCF-101}.

\hyperref[hmdb51]{HMDB-51}.

Transfer learning is done on UCF-101 and HMDB-51, these two smaller datasets,
from the larger Kinetics dataset.

\subsubsection{Model}

Inception-V1 with batch normalization is used as a common ``backbone''
architecture for all models.

\textbf{Two-Stream Inflated 3D ConvNets (I3D)}. All kernels in the Inception-V1
model are inflated, i.e. $(N, N)$ kernels become $(N, N, N)$ kernels.

Weights of $2D$ filters are tiled $N$ times across the time dimension.

The first two max-pooling layers of Inception-V1 were altered to be
$(1, 3, 3)$, and have unity stride in time.

\textbf{2D ConvNets with LSTMs on top}. An LSTM layer with batch normalization,
and 512 hidden units, is placed after the last average pooling layer in
Inception-V1. A fully connected layer is added on top for classification.

\textbf{Two-stream networks with different stream fusion techniques}.
Predictions from an RGB frame are averaged with predictions from pre-computed
optical flow for ten frames surrounding the RGB frame.

The flow stream has twice as many input channels as flow frames: one for each
of up and down flow directions.

The two streams are fused after the last convolutional layer.

Inputs to the two-stream model are a sequence of five frames, sampled ten
frames apart from a 25 fps video, as well as the corresponding optical flow
features.

Features from the $(5, 7, 7)$ activations before the last average pooling layer
of Inception-V1 are passed through a $C_{512} \rightarrow P_3 \rightarrow FC_{?}$ 3D
convolutional network, where $P_3$ is a $(3, 3, 3)$ max-pooling layer. These
layers are initialized with Gaussians.

The averaging process is learnable.

\textbf{C3D}. A network with eight convolutional layers, five pooling layers
and two fully connected layers at the top is used. Inputs are 16-frame,
$(112, 112)$ clips, as in~\citet{DBLP:journals/corr/TranBFTP14}. Batch
normalization is used after each (convolutional and fully connected) layer. In
the first layer, a temporal stride of two (compared with one) is used, in order
to fit 15 videos per batch per K40 GPU\@.

\subsubsection{Inference}

Video streams are decoded at 25 fps.

Standard SGD with momentum set to 0.9 is used, with synchronous data
parallelization across 32 GPUs for all models except C3D, for which data
parallelization across 64 GPUs was used.

Models are trained on miniKinetics for 35k steps, and for 100k steps on
Kinetics, with a 10x reduction on learning rate when validation loss plateaued.

Models were trained for up to 5k steps on UCF-101 and HMDB-51, using 16 GPUs.

\textbf{Data augmentations} used are random cropping, by resizing the smaller
video side to 256 pixels then randomly cropping a $(224, 224)$ patch, and
temporal random cropping. Random left-right flipping was applied.

Shorter videos are looped to satisfy each model's input requirements.

Training with \textbf{Two-Stream Inflated 3D ConvNets (I3D)} is done with
64-frame snippets, and test is done using entire videos.

Test inference is done by taking $(224, 224)$ center crops.

Optical flow is computed using the TV-$L^1$ algorithm
from~\citet{Zach07aduality}, which runs at 30 fps on $(320, 240)$ videos (on
decade-old GPU technology).

\subsubsection{Criticism}

For \textbf{2D ConvNets with LSTMs on top}, a cross-entropy loss is placed on
the outputs at all time steps.

\subsubsection{Results}

78.7\% on miniKinetics with Two-Stream I3D, compared with 74.0\% for 3D-Fused,
72.9\% for Two-Stream, 60.0\% for C3D and 69.9\% for Conv2D with LSTM\@.

Using pre-training on Kinetics, 98.0\% and 80.7\% is achieved, compared with
state of the art of 94.6\% and 70.3\% respectively on UCF-101 and HMDB-51.

% TODO(brendan): A couple more interesting findings from the results, e.g. with
% regards to optical flow.

\subsubsection{Questions}

\begin{itemize}
        \item \textbf{Future work?} Action tubes, or attention mechanisms to
                focus in on human actors.

                The authors suggest that the optical flow streams produce
                significant improvements, compared to the RGB stream alone, due
                to their optical flow algorithm's recurrent nature
                (optimization is done iteratively).  This could be disproved by
                using purely feed-forward optical flow computations, e.g.
                FlowNet 2.0.

                The authors mention specifically that they will repeat all
                experiments with Kinetics instead of miniKinetics, and compare
                with and without ImageNet pre-training. They will also compare
                inflating of different 2D ConvNet architectures (besides
                Inception-V1).
\end{itemize}

\subsubsection{Similar Work}

Convolutional Two-Stream Network Fusion for Video Action
Recognition~\citet{DBLP:journals/corr/FeichtenhoferPZ16} builds on two-stream
networks by fusing the streams' respective features earlier.


\subsection{Learning Spatiotemporal Features with 3D Convolutional
            Networks\citet{DBLP:journals/corr/TranBFTP14}}

\subsubsection{Data}

UCF-101~\ref{ucf101}.

Sports-1M~\citet{KarpathyCVPR14} (1.1 million sports videos, with 487 classes).

I380K (internal dataset, finetuned from).

\subsubsection{Model}

A Conv3D model is used, with $(3, 3, 3)$ kernels and $(2, 2, 2)$ max-pooling
layers, with the exception of the first max-pooling layer, which is
$(1, 2, 2)$.

Full model structure (testing effect of kernel size):
$C_{64} \rightarrow P_1 \rightarrow C_{128} \rightarrow P_2 \rightarrow
{(C_{256} \rightarrow P_2)}^3 \rightarrow {(FC_{2048})}^2 \rightarrow
\textrm{softmax}$, where $P_1$ refers to the $(1, 2, 2)$ max-pooling layer, and
$P_2$ refers to the $(2, 2, 2)$ max-pooling layer.

Full model structure (best results):
$C_{64} \rightarrow P_1 \rightarrow C_{128} \rightarrow P_2 \rightarrow
{(C_{256})}^2 \rightarrow P_2 \rightarrow
{({(C_{512})}^2 \rightarrow P_2)}^2 \rightarrow {(FC_{4096})}^2 \rightarrow
\textrm{softmax}$.

The last fully connected layer above goes to an $L2$ normalization, followed by
a linear SVM\@.

\subsubsection{Inference}

Inputs were resized to $(171, 128)$ then randomly cropped to $(112, 112)$.

Non-overlapping 16-frame clips were used during training, therefore batches of
$(\verb|batch_size|, 16, 112, 112, 3)$ were input to the model.

Hyper-parameters: batch size of 30, learning rate of 0.003 divided by four
every four epochs, for sixteen in total.

Five two-second long clips were extracted from each video and trained on.

For predictions on UCF101, a trained C3D model was used to extract features
from 16-frame clips, with overlap of 8-frames between subsequences. A linear
SVM was then trained on these C3D features.

\subsubsection{Criticism}

Cross-entropy on activity label?

\subsubsection{Results}

90.4\% on UCF-101, compared with 89.1\% state-of-the-art at the time.

84.4\% vs. $> 90\%$ on Sports-1M, although the technique
from\citet{DBLP:journals/corr/NgHVVMT15} could be applied on top of C3D
features.

On UCF-101, homogeneous temporal depth of kernels in the model were able to
beat increasing or decreasing temporal depth.

\subsubsection{Questions}

\begin{itemize}
\item \textbf{Any comparison with frame-frame ImageNet vectors?} None. The only
        comparison was with ImageNet features averaged over all frames.

\item \textbf{Which features complement C3D?} iDT\@? Optical flow?
\end{itemize}

\subsubsection{Similar Work}

Improved Dense Trajectories\citet{Wang2013}.

Learning Hierarchical Invariant Spatio-temporal Features for Action Recognition
with Independent Subspace Analysis\citet{Le:2011:LHI:2191740.2192108}.
Unsupervised video feature learning using stacking.

Two-stream networks\citet{DBLP:journals/corr/SimonyanZ14}.

Beyond Short Snippets: Deep Networks for Video
Classification\citet{DBLP:journals/corr/NgHVVMT15}. Connects Conv2D to LSTM, and
explores other temporal feature pooling models that work on Conv2D-extracted
features.


\subsection{Revisiting the Effectiveness of Off-the-shelf Temporal Modeling
            approaches for Large-scale Video
            Classification\citet{2017arXiv170803805B}}

\subsubsection{Data}

\hyperref[kinetics]{Kinetics}.

\subsubsection{Model}

RGB, optical flow and audio streams are used. RGB and optical flow streams both
use Inception-ResNet-v2, where the RGB stream is pre-trained on ImageNet then
fine-tuned on Kinetics. The optical flow stream is initialized with the
fine-tuned weights from the RGB stream.

A VGG-16 ConvNet is used to model the audio data, which is preprocessed
following~\citet{DBLP:journals/corr/HersheyCEGJMPPS16}. Audio data is first
split into non-overlapping 960ms frames. Each frame is split into 25ms windows,
offset at every 10ms, for 96 windows in total. A Fourier transform is run on
each window, and frequencies are binned using Mel-scale into 64 bins. A
log-transform is applied to each bin. Therefore, the VGG-16 ConvNet receives
$96 \times 64$ inputs.

\subsubsection{Inference}

At test time, three segments are sampled from each trimmed video,
following~\citet{DBLP:journals/corr/WangXW0LTG16}.

\subsubsection{Criticism}

\subsubsection{Results}

\subsubsection{Questions}

\subsubsection{Similar Work}


\section{Temporal Action Localization}


\subsection{CDC\@: Convolutional-De-Convolutional Networks for Precise Temporal
            Action Localization in Untrimmed
            Videos~\citet{DBLP:journals/corr/ShouCZMC17}}

\subsubsection{Data}

THUMOS~2014~\ref{thumos} was trained and evaluated on.

ActivityNet challenge 2016~\citet{Heilbron_2015_CVPR} has 203 activity
categories, with 137 untrimmed videos per class on average. ActivityNet 2016
has an average of 1.41 activity instances per video, and 849 hours of video in
total. Activitynet 2016 encompasses untrimmed video classification, trimmed
video classification and activity detection.

\subsubsection{Model}

The C3D architecture
from~\citet{DBLP:journals/corr/TranBFTP15, DBLP:journals/corr/TranBFTP14} up to
and including layer \verb|pool5| is built upon. \verb|pool5| is changed to only
pool in spatial dimensions.

\verb|pool5| is then followed by \verb|conv6|, which is a layer with filter
size $[4, 4, 4]$, with strides $[2, 1, 1]$, and the temporal part of the filter
doing an up-convolution while the spatial part does down-convolution (?). No
spatial padding, and ``same'' temporal padding is used, such that the input
$[L/8, 4, 4]$ feature map becomes a $[L/4, 1, 1]$ feature map.

This layer is called \verb|CDC6|, after ``conv-deconv'' layers, and is followed
by two temporal up-convolutional layers (both of filter size $[4, 1, 1]$ and
stride $[2, 1, 1]$) for a final output of shape $[L, 1, 1]$.  The pre-logits
layers \verb|CDC6| and \verb|CDC7| have 4096 channel outputs.

Dropout is used for all \verb|CDC| layers, with 0.5 dropout ratio.

\subsubsection{Inference}

Non-overlapping segments of 32 frames are extracted and fed to the CDC network.
During training, only segments with at least one frame with a non-background
label are used.

C3D is pre-trained on Sports-1M~\citet{KarpathyCVPR14}, and after this the CDC
network converges within four epochs.

Both CDC and C3D layers are trained jointly, with a learning rate of
$10^{-5}$ used for all layers except \verb|CDC8|, for which a learning rate
of $10^{-4}$ is used.

At test time, segment proposals from~\citet{DBLP:journals/corr/ShouWC16} are
fed to the CDC network, with the proposals expanded by $1/8$ of the original
proposal length on either end.

Using the per-frame predictions on the segment proposal, Gaussian kernel
density estimation is used to obtain $\mu$ and $\sigma$, and a refined segment
from $[\mu - \sigma, \mu + \sigma]$ is produced. The predicted class scores are
the average class scores over this refined segment.

To make segment proposals for the temporal localization task, non-maximum
suppression is used as
in~\citet{DBLP:journals/corr/YeungRJAML15, DBLP:journals/corr/ShouWC16}.

\subsubsection{Criticism}

A cross-entropy per-frame loss is used on the class predictions.

As an evaluation metric, mean average precision is computed over the
predictions for each frame.

For evaluation on the temporal localization task, mAP is computed over
different IoU thresholds.

\subsubsection{Results}

Results on THUMOS~2014 are given below.

Per-frame: 44.4 mAP compared with 41.3 mAP from
MultiLSTM~\citet{DBLP:journals/corr/YeungRJAML15}, and 41.7 mAP using
conv-deconv instead of CDC\@.

Temporal localization at 0.5 IoU\@: 23.3 mAP, as compared with 19.0 mAP
in~\citet{DBLP:journals/corr/ShouWC16}.

On ActivityNet (at 0.75 IoU): 26.0 when based on the segment proposals
of Wang and Tao in their ActivityNet 2016 challenge submission, as compared to
4.1 mAP without CDC\@. At 0.5 IoU, CDC only improves the method of Wang and Tao
from 45.1 to 45.3 mAP\@.

\subsubsection{Questions}

Tried conv-deconv vs. CDC, but what about deconv-conv? Or a series of 3d
convolutions with spatial stride fixed, until the last layer? I.e.\ test
whether the CDC filter is special when compared to simply adding more
parameters to the temporal up-convolutional filters.


\subsection{Temporal Action Localization in Untrimmed Videos via Multi-stage
            CNNs~\citet{DBLP:journals/corr/ShouWC16}}

\subsubsection{Data}

THUMOS~2014~\ref{thumos} was trained and evaluated on.

MEXaction2~\citet{MEXaction2} is a dataset with two action classes: horseback
riding, and ``bull charging cape''. The dataset is made up of YouTube, UCF101
and INA videos, of which only INA videos are untrimmed. The untrimmed videos
are 77 hours in total. The training set is 1336 instances, validation set is
310 instances and test set is 329 instances.

\subsubsection{Model}

The model consists of three separate 3D convolutional networks based
on C3D~\citet{DBLP:journals/corr/TranBFTP14}: proposal, classification and
localization networks. C3D is pre-trained on Sports-1M.

The proposal network has a binary ``action or not?'' output, the classification
network outputs the class scores (plus background), and the localization
network outputs class scores as well, except is trained with a different loss
function.

\subsubsection{Inference}

Chunks of 16, 32, 64, 128, 256 and 512 frames are taken from the video at
intervals overlapping by 75\% of the chunk size. 16 frames are then sampled
uniformly from each of these chunks, and the resulting 16 frames are input to
the proposal network.

The classification network is trained first, then used to initialize the
localization network.

During evaluation, segments with proposal scores $\geq 0.7$ are kept.
Post-processing is done to remove segments predicted as background, as well as
to scale up confidence scores by the class distribution in the training set.
Non-maximum suppression is done to remove redundant detections, with overlap
threshold $\theta - 0.1$, where $\theta$ is the IoU overlap threshold.

\subsubsection{Criticism}

Ground truth labels for the proposal network are assigned as follows. Segments
from trimmed videos are all assigned as ``action''. Segments with greater than
0.7 IoU with a class are labelled as positive, whereas if segments have less
than 0.3 IoU with any given class those segments are labelled as background.
Finally, for each ground truth instance, if the instance has no segments
assigned to it, a segment with greater than 0.5 IoU with that ground truth
instance is chosen as a positively labelled segment. A number of background
segments equal to the total number of positively labelled segments are randomly
sampled.

Ground truth labels for the classification network are gathered in a similar
way as for the proposal network, except labels are action classes, and
background class labels are reduced to match the average number of labels per
class.

An overlap loss is used to artificially increase the scores of predictions with
higher overlap with the ground truth instance.

\begin{equation}
        \mathcal{L}_{\textrm{overlap}} = \frac{1}{N} \sum_n
                \left(\frac{1}{2} \cdot
                        \left(
                                \frac{{\left( P_n^{k_n} \right)}^2}{v_n^\alpha}
                                - 1
                        \right)
                        \cdot \left[ k_n > 0 \right]
                \right)
\end{equation}

Where $\left[ k_n > 0 \right]$ is the indicator function for the true class
label $k_n$ being positive, $v_n$ is the fraction of overlap of the segment
with the ground truth instance, and $\alpha = 0.25$.

\subsubsection{Results}

At IoU threshold 0.5:

7.4 mAP on MEXaction2, compared with 1.7 mAP baseline provided with the
dataset.

19.0 mAP on THUMOS~2014, compared with 15.0 mAP from the THUMOS~2014 challenge
submission of~\citet{LearSubmissionThumos2014}.

\subsubsection{Questions}

Does uniform sampling mean random uniform sampling, or sampling at fixed,
uniform intervals?


\subsection{Temporal Convolutional Networks for Action Segmentation and
            Detection\citet{DBLP:journals/corr/LeaFVRH16}}

\subsubsection{Data}

50 salads~\citet{Stein:2013:CEA:2493432.2493482} is a dataset of 50 sequences of
food preparation video, with about 30 action annotations, such as ``cutting a
tomato'', per sequence. RGB and depth maps at $640 \times 480$ and 30Hz, 3-axis
accelerometer data from a device attached to various cooking utensils, as well
as synchronization parameters between the accelerometer and RGB-D data are
provided.

Annotations in 50 salads consist of ``pre-'', ``core'', and ``post-'' phases of
each action in a recipe.

25 different people in total are in the 50 salads videos.

The MERL shopping dataset~\citet{merl-shopping-singh} is a human action dataset
consisting of 96 two-minute surveillance-style videos, with a single shopper
per video.  The shoppers perform one of five actions, plus a background class:
``reach to shelf'', ``retract hand from shelf'', ``hand in shelf'', ``inspect
product'', and ``inspect shelf'', each action normally being a few seconds
long.

Georgia Tech Egocentric Activities (GTEA)~\citet{Fathi:2011:LRO:2191740.2191834}
contains 28 videos of household activities, such as making coffee, taken from a
head-mounted camera.  Videos are on average a minute long, and on average
contain 19 action instances per video.

\subsubsection{Model}

Two models are investigated, one called an ``Encoder-Decoder Temporal
Convolutional Network (ED-TCN)'', and the other called a ``Dilated TCN''.

In the TCN encoder, layers consist of a convolution plus bias, a non-linear
unit, followed by a max-pool. TCN decoder layers are a nearest-neighbour
upsample, followed by a convolution plus bias then a non-linear unit.

The dilated TCN consists of a series of blocks, where each block consists of a
sequence of layers. In each subsequent dilated convolutional layer within the
same block, each unit at time $t$ takes weighted input from the two previous
units at time $t$ and $t - s$, where $s$ is the same as the $l$-dilation
parameter as defined in~\citet{DBLP:journals/corr/YuK15}.

The set of outputs from all of the blocks are summed, followed by a ReLU, then
a fully-connected layer and another ReLU\@, which is input to a softmax
function, the output of which forms the frame-wise predictions.

For ED-TCN, each layer $l$ has $96 + 32l$ filters. For dilated TCN, each layer
has 128 filters.

Causal and decausal models are evaluated. For ED-TCN, models convolve from $t -
d$ to $t$ in the causal case, and from $t - d/2$ to $t + d/2$ in the acausal
case. For dilated TCN, units at time $t$ take the additional input from $t + s$
in the acausal case.

\subsubsection{Inference}

Adam optimizer~\citet{DBLP:journals/corr/KingmaB14} is used.

\subsubsection{Criticism}

``Action segmentation'' metrics use frame-wise mAP, while ``action detection''
metrics use segment-wise mAP for given IoU overlap fractions.

It is pointed out that the way confidence evaluated heavily affects the score
for a given type of metric. E.g.\ on MERL shopping, the mAP scores
from~\citet{merl-shopping-singh} increase from 50.9 to 69.8 by using maximum
instead of average prediction score over a given interval.

A new evaluation metric is proposed, which computes true positives, false
positives and false negatives over segments at a given IoU threshold, and defines
$F1 = 2\frac{prec * recall}{prec + recall}$.

\subsubsection{Results}

On GTEA, ED-TCN achieves an average F1 score of 64.0 over IoU thresholds of
0.1, 0.25 and 0.5 and dilated TCN achieves an average F1 score of 60.6. A
baseline method, with better descriptors, achieves 64.6.

On 50 salads, different activation units are compared, with normalized ReLUs
out-performing all with a F1@25 score of 58.4, compared with 40.4 using
standard ReLUs. Activation function choice is not found to affect dilated TCN
performance.

ED-TCN performed best with two layers and filter size of 15 (44 frame receptive
field). Dilated TCN performed best with four blocks and five layers per block
(128 frame receptive field), and similar performance with 96 frame receptive
field.

\subsubsection{Questions}

Dilated convolutions with larger filter sizes? Dilated convolutions over 3D
data?


\subsection{Deep Temporal Linear Encoding
            Networks\citet{DBLP:journals/corr/DibaSG16}}

A method called Temporal Linear Encoding (TLE) is proposed to encode entire
videos into vectors in a single feature space.

\subsubsection{Data}

\hyperref[ucf101]{UCF-101} and \hyperref[hmdb51]{HMDB-51}.

\subsubsection{Model}

The deep temporal linear encoding layer takes in a set $\{S_i\}$ of feature
maps extracted from clips in the video, and does,

\begin{enumerate}
        \item Aggregation of the features $\{S_i\}$.
        \item Encoding of the aggregated features.
\end{enumerate}

Aggregation operators include element-wise average, maximum and multiplication
of segments, with element-wise multiplication yielding the best results.

Encoding methods include,

\begin{enumerate}
        \item Bilinear combination given by $y = W[X \otimes X']$, where $X \in
                \mathbb{R}^{(hw) \times c}$ and in this case $X = X'$.
                Presumably, $W$ is element-wise multiplication of the of the
                resulting $[X \otimes X'] \in \mathbb{R}^{(cc')}$. The square
                brackets mean concatenation into a vector.

        \item Fully connected pooling
\end{enumerate}

AlexNet, VGG-16 and BN-Inception are used as two-stream ConvNet base models,
pre-trained on ImageNet.

C3D models are also used as a base feature extractor.

\subsubsection{Inference}

The Tensor Sketch algorithm of~\citet{Pham:2013:FSP:2487575.2487591} is used to
approximate the outer product for the bilinear encoding method by projecting
the two input tensors directly to a lower-dimensional space, without explicitly
computing the outer product.

Fully-connected layers are dropped from the pre-trained ConvNet models and the
feature maps from the last convolutional layers are fed into the bilinear
model. E.g. BN-Inception produces features maps of dimension $14 \times 14
\times 1024$, leading to bilinear outputs of dimension $1024 \times 1024$, and
corresponding compact bilinear representation of size $8196$.

The ConvNet models are then fine-tuned, first fine-tuning the last layer, then
the entire ConvNet.

Features output from the bilinear model have a signed square root and
L2-normalization applied before being input to a softmax layer.

A similar procedure to above is applied for the fully-connected pooling models.

They first split each video into three equal segments. Then, for two-stream,
one RGB frame and corresponding stack of optical flow frames is extracted for
each segment, and the frames extracted from all three segments are input to the
TLE model. Similarly, for C3D, one clip of 16 frames is extracted from each
segment and the three sets of extracted features are input to the TLE model.

For the two-stream model, the above three-segment extraction-prediction
sequence is completed five times and the resulting predictions are averaged.
For the C3D model, the above is repeated three times.

\subsubsection{Criticism}

Presumably cross-entropy loss on the output of the softmax function.

\subsubsection{Results}

Element-wise multiplication (94.8/70.4) beat average (92.6/68.1) and maximum
(91.3/67.4) aggregation functions on UCF101/HMDB51.

With two-stream ConvNets, TLE\@: bilinear achieved 95.6/71.1 on UCF101/HMDB51,
compared with $94.0/68.5$ in~\citet{DBLP:journals/corr/WangXW0LTG16}. With C3D,
TLE\@: bilinear scores $86.3/60.3$.

An experiment was run to include a fourth segment extracted from a VGG-16
network pre-trained on Places365, a so-called ``context'' feature map, in
addition to a three-segment spatial ConvNet TLE\@. This setup scored
$83.8/63.6$, as compared to $81.5/60.9$ using only spatial ConvNets\@.

\subsubsection{Questions}

Would knowledge transfrom from Places365 still work for the two-stream
ConvNets?


\section{Reinforcement Learning}


\subsection{Designing Neural Network Architectures using Reinforcement
            Learning~\citet{DBLP:journals/corr/BakerGNR16}}

Q-Learning~\citet{Watkins1992} is used to build neural network architectures for
image classification, and tested on MNIST, SVHN and CIFAR-10. A single such
``MetaQNN'' model (the one that did best on the validation set) achieves 0.44\%
error on the MNIST test set. The best model for CIFAR-10 achieves 6.92\% error
on the test set.

$\epsilon$-greedy exploration is used with a training schedule that starts with
\num{1500} updates at $\epsilon = 1.0$ to allow for sufficient exploration,
followed by either \num{100} or \num{150} updates at each step
$\epsilon = 0.9, 0.8, \dots$.

Only convolutional, fully connected, pooling and softmax layers are allowed.
Various constraints to reduce the state space are used, such as only allowing
fully-connected layers to transition to fully-connected or termination states.

The following steps are taken in defining the learning algorithm:

\begin{enumerate}
        \item Reduce CNN layer definitions to state tuples.

        \item Define a set of actions that the agent may taken from a given
                state (i.e.\ layer definition).

        \item The size of the action space is balanced with the amount of
                exploration needed by the agent for the algorithm to converge.
\end{enumerate}

The action space is forced to terminate after a finite number of layers (12 for
MNIST and 18 for CIFAR-10).

\subsubsection{Extension Ideas}

\begin{itemize}
        \item In general, using Q-function approximation to allow increasing of
                the state space.

        \item Attaching a regularization term to the agent's cost function,
                e.g.\ $\log{N}$ in the number of parameters.
\end{itemize}

\subsubsection{Questions}

\begin{itemize}
        \item Is convergence to the global minimum really guaranteed by
                Q-learning?
\end{itemize}


\subsection{Neural Optimizer Search with Reinforcement
            Learning~\citet{neural-optimizer-search-46114}}

\subsubsection{Data}

An optimizer is learned on CIFAR-10~\ref{cifar10}.

The optimizer is transferred to WMT~2014~\ref{wmt2014}.

The \href{https://en.wikipedia.org/wiki/Rosenbrock_function}{Rosenbrock
function} is trained on using the learned update rule.

\subsubsection{Model}

% TODO(brendan): TikZ diagram of model controller RNN and computation graph.

The controller is an RNN, which samples strings of length $5n$ from the DSL
during training. Values of $n$ equal to 1, 2 and 3 are used in the experiments.

The controller is a single-layer LSTM with a \num{150}-unit hidden layer and
weights uniformly initialized in $[-0.08, 0.08]$. An entropy penalty on the
weights is used, set to $0.0015$.

A domain specific language (DSL) is cooked up to write a binary expression
tree, which represents the learned update function. The DSL uses postfix
notation.

Refer to section 4.1 of the paper for a list of operands, unary and binary
functions used in the DSL\@. Adam and RMSProp are among the operand primitives.

Weight updates are computed as
$\Delta w = \lambda * b(u_1({op}_1), u_2({op}_2))$.

A two-layer $3 \times 3$ convolutional network, with ReLU activations and batch
normalization after each layer, is used as the model to optimize over for the
CIFAR-10 dataset. The learned update rule is then transferred to train a Wide
ResNet model.

The Google Neural Machine Translation (GNMT) model from~\citet{gnmt-45610} is
used for the WMT~2014 experiments.

\subsubsection{Inference}

Samples generated by the controller RNN are added to a queue, from which a
worker from a distributed set of workers dequeues, trains, and returns the
accuracy $R(\Delta)$ to the controller.

After workers receive an optimizer from the controller, the worker does a
hyperparameter sweep for learning rates given by $10^i$, with $i \in [-5, 1]$.
The best learning rate is then trained for five epochs before evaluating.

Child networks have a batch size of 100.

Update rule hyperparameters are: $\beta_1 = 0.9$, $\beta_2 = \beta_3 = 0.999$,
and $\epsilon = 10^{-8}$.

Trust Region Policy Optimization~\citet{DBLP:journals/corr/SchulmanLMJA15} is
found to improve over REINFORCE~\citet{Williams1992} for training the controller
RNN\@. The baseline function used for TRPO is an exponential moving average of
previous rewards.

Adam optimizer is used to train the RNN controller, with a learning rate of
$10^{-5}$ and a minibatch size of five.

\subsubsection{Criticism}

The training objective for the RNN controller is
$\mathbb{E}_{\Delta \sim p_\theta(.)}[R(\Delta)]$, where $R(\Delta)$ is the
accuracy of the model on a held-out validation set after training the
model-to-optimize using the sampled optimizer.

\subsubsection{Results}

Of a number of discovered optimizers, \verb|Optimizer_1|, given by $e^{sign(g)
* sign(m)} * g$, performs nearly as well as SGD with momentum on the Rosenbrock
function, and better than all other compared optimizers (Adam, RMSProp, and
SGD).

The best discovered optimizers outperform SGD with momentum on CIFAR-10 using a
Wide ResNet, scoring 93.2 compared with 92.3 for momentum.

On WMT~2014, the best optimizer achieves an improved BLEU score of 25.2, as
compared with 24.5 with Adam.

\subsubsection{Questions}

Can a learned optimizer improve upon existing standard optimizers such as Adam
and SGD with momentum?

Will optimizers learned on one task transfer well to other tasks?

What is the computational cost of doing a neural optimizer search, and how
could said cost be alleviated?

The neural optimizer search was distributed over \num{100} CPU servers over a
period of less than a day.

How can the neural architecture search be efficiently extended to check for
optimizers that perform well over the entire training cycle, but perform
relatively poorly during the first $N$ epochs?

\subsubsection{Similar Work}

Trust Region Policy Optimization~\citet{DBLP:journals/corr/SchulmanLMJA15}
(background).


\subsection{Asynchronous Methods for Deep Reinforcement
            Learning~\citet{DBLP:journals/corr/MnihBMGLHSK16}}

Four different asynchronous methods are explored for reinforcement learning
using neural networks. The methods are tested on Atari 2600 games, a car
simulator, Mujoco and a new 3-D maze game. In most games, asynchronous
advantage actor-critic is found to converge much faster (in terms of wall-clock
time) and to a higher score than the other asynchronous methods. In general,
n-step return methods converge faster than one-step methods.

Asynchronous advantage actor-critic (Algorithm~S3
of~\citet{DBLP:journals/corr/MnihBMGLHSK16}) involves using the value function
as a learned baseline in policy gradient. Hence the value-function baseline is
the ``critic'', while the policy is the ``actor''.

Entropy is added to the policy $\pi$ \citet{williams1991function} to avoid
convergence to sub-optimal deterministic policies (for both A3C and value
methods).

For value methods, a different exploration policy is used per thread, by
sampling $\epsilon$ from different distributions used in $\epsilon$-greedy
exploration policies.

RMSProp with statistics shared across thread is shown to be more robust
compared to using thread-specific statistics.

One-step Q-learning and SARSA both achieve super-linear performance
improvements in the number of threads, hypothesized to be due to bias
reduction.

For Mujoco, low-dimensional inputs are mapped to a 200-dimensional vector,
which is then used as input to an LSTM\@.

For continuous action domains, the neural network outputs are the mean and
scalar variance of a multi-dimensional Gaussian with spherical covariance
(i.e.\ the covariance matrix $\Sigma = \lambda I$).

\subsubsection{Future work}

Experience replay, e.g.\ to improve sample efficiency for slow environments
such as TORCS\@.

Eligibility traces.

Generalized advantage estimation.

Reducing the over-estimation bias of Q-values.

True online temporal difference models with non-linear function
approximation~\citet{DBLP:journals/corr/SeijenMPMS15}.

\subsubsection{Questions}

Why is Algorithm~S2 unusual? What are eligibility traces (page 4)?

A small network (a handful of convolutional layers followed by fully connected
layers) is trained. What could be done to improve the ability to train a larger
network?


\subsection{Scalable trust-region method for deep reinforcement learning using
            Kronecker-factored
            approximation~\citet{DBLP:journals/corr/abs-1708-05144}}

A more sample-efficient algorithm for gradient descent is presented by using a
Kronecker-factored approximation to the natural gradient, which is applied to
OpenAI Gym discrete and continuous control tasks such as MuJoCo. The algorithm
improves over TRPO in sample-efficiency and computation time since TRPO is an
iterative method using conjugate gradient.

The Kronecker-factor approximation part of Actor-Critic using
Kronecker-Factored Trust Region (ACKTR) involves approximating the natural
gradient by approximating the
\hyperref[fisher-info]{Fisher information matrix} $F$. The method of
approximating $F$ is the same as in~\citet{DBLP:journals/corr/MartensG15}.

The trust region part of ACKTR refers to transforming the fixed $\eta$ in,

\begin{equation*}
        \theta \leftarrow \theta - \eta F^{-1} \nabla_\theta L
\end{equation*}

into,

\begin{equation*}
        \eta \leftarrow \min\left(
                \eta_{\max}, \sqrt{
                        \frac{2\delta}{\Delta\theta^{\top}
                                       \hat{F}
                                       \Delta\theta}}\right)
\end{equation*}

\subsubsection{Questions}

Supposedly $F$ is a local quadratic approximation to the KL divergence, which
measures the dissimilarity of probability distributions $P$ and $Q$. If this is
the case, what are $P$ and $Q$ when optimizing the policy of an RL agent?


\section{Natural Language Processing}


\subsection{Skip-Thought Vectors~\citet{DBLP:journals/corr/KirosZSZTUF15}}

Skip-thought vectors is a method of encoding a feature vector from a sentence,
in such a way that the feature vector can be re-used as a generic, continuous
sentence representation in a variety of tasks.

The skip-thought vector model is an encoder-decoder system. Sentences are input
to the encoder-decoder as a sequence of word representations from word2vec. In
the paper, RNNs with a reset (resetting the hidden state in the proposed state
computation) and update gate (updating the hidden state with the proposed state
if one, otherwise carrying over the previous hidden state) were used as both
encoder and decoder.

To generate the encoded representation for a given sentence, the encoder first
generates a hidden state for each word in the sentence, with the final hidden
state representing the entire sentence. At each timestep, the encoder takes as
input the word2vec representation of the current word, as well as its own
previous hidden state.

The decoder then takes that encoded state as an additional input (in addition
to its own hidden state and the word2vec representation of the previous
timestep's word) as it decodes, word-by-word, the next sentence.

The decoder is trained to predict both the previous and next sentence from a
given sentence representation, with a log-loss on the correct word at each
time-step.

In this way, the encoder is trained to produce a feature representation of the
preceding sentence that will be useful for the decoder to correctly predict the
subsequent word in the current sentence.


\subsection{Effective Approaches to Attention-based Neural Machine
            Translation~\citet{DBLP:journals/corr/LuongPM15}}

Different attention-based models for neural machine translation are used on the
WMT'15 English-German translation task in both directions, achieving 25.9 BLEU
points in the English to German direction, which is an improvement of 1.0 BLEU
points compared to the best system at the time of publication.

Two different attention mechanisms are considered: global attention, where a
context vector is formed from the weighted average of an attention mechanism
over the entire source sentence $\mathbf{s}$, and local attention, where first
a start position $p_t$ is predicted and then attention is only used in a
neighbourhood around $p_t$.

Different scoring mechanisms are used to compute attention, including the dot
product of the current target hidden state $\mathbf{h}_t$ with each source
hidden state $\overline{\mathbf{h}}_s$, i.e.\
$\mathbf{h}_t^\intercal \overline{\mathbf{h}}_s$. The generalized dot product
$\mathbf{h}_t^\intercal \mathbf{W}_\mathbf{a} \overline{\mathbf{h}}_s$ is also
used.

The align weights are then computed as the softmax of the score for each source
word $s$, and the context vector is computed as a weighted sum over the source
states $\overline{\mathbf{h}}_s$.

Locally-predicted attention with general scoring is shown to slightly
outperform global attention with dot product scoring in terms of BLEU score.

\subsubsection{Questions}

How is the loss function computed? Does it use cross-entropy with the closest
monogram or tri-gram in the reference translations?

The alignment weights output by the attention model (see Figure~7 alignment
visualizations) seem to be nonsensical, i.e.\ not mapping between source and
target word meanings. Is this normal for attention in machine translation
models?


\subsection{Reinforcement Learning for Bandit Neural Machine Translation with
            Simulated Human Feedback~\citet{DBLP:journals/corr/NguyenDB17}}

Advantage actor-critic is used to improve a pre-trained neural machine
translation system using only ``bandit rewards'', i.e.\ scalar rewards
indicating the quality of a given translation.

The \emph{granularity}, \emph{variance} and \emph{skew} of true human rating
distributions are simulated by fitting the distributions of these values using
kernel smoothing, then applying perturbations according to kernel-smoothed fits
to the output score from a BLEU system.

A neural encoder-decoder with global attention is used as both the ``actor''
and ``critic''.

The data used are parallel-translated TED talks from IWSLT 2014 and
2015~\citet{cettolo2015iwslt}.

An improvement by the NED-A2C method is shown in both per-sentence BLEU score
(apparently on the training data) and corpus BLEU score on a held-out test set.
Supervised training on the ``bandit set'' is shown to improve over A2C training
on the bandit set, and the authors attribute this to maximizing the
log-likelihood being a better approximation to corpus level BLEU than is
per-sentence level BLEU\@.

The method is shown to be more sensitive to variance in the bandit feedback
than to skew or granularity, with variance greater than 20\% causing results to
degrade.


\subsubsection{Questions}

What is the difference between corpus-level BLEU and sentence-level BLEU\@?

The authors mention that directly learning the Q-function with the critic is
unstable compared to advantage actor-critic. It would make sense as a sanity
check to also compare against other baselines, such as averaged reward over the
mini-batch (i.e.\ just using REINFORCE with a computed baseline).


\section{RNNs}

\subsection{Recurrent Batch
            Normalization~\citet{DBLP:journals/corr/CooijmansBLC16}}

Batch normalization (BN) is extended to the hidden-to-hidden connections in
RNNs (in particular, to LSTMs). Previously suggested vanishing gradient issues
when using BN in hidden-to-hidden connections are overcome by initializing the
BN $\gamma$ parameter to 0.1. The reason given for the vanishing gradients is
tanh saturation for unit variance features.

In the equation
$(\mathbf{f}_t, \mathbf{g}_t, \mathbf{i}_t, \mathbf{o}_t) =
\mathbf{W}_h \mathbf{h}_{t - 1} + \mathbf{W}_x \mathbf{x}_t + \mathbf{b}$,
BN layers are added after the matrix products
$\mathbf{W}_h \mathbf{h}_{t - 1}$ and $\mathbf{W}_x \mathbf{x}_t$ separately.
$\beta_h$ and $\beta_x$ are set to zero as they are redundant with
$\mathbf{b}$. Another BN layer is added in the hidden state output equation
$\mathbf{h}_t = \sigma{(\mathbf{o}_t)} \odot \tanh{(\mathbf{c}_t)}$,
right before the tanh squashing function. No BN layer is added to the
$\mathbf{c}_t$ update equation, supposedly so that the cell state can freely
propagate forward.

Separate BN statistics are kept for each timestep, supposedly to overcome
transience in the statistics of the initial sequences. BN statistics averaged
over entire sequences is found to degrade performance. At test time, the BN
statistics of the last timestep $T_{\max}$ of the longest sequence seen at
training time are extended to all timesteps $t > T_{\max}$.

As a trick for dealing with training sequences with long runs of constant
variance (namely black pixels in MNIST digits), hidden units are initialized
with Gaussian noise.


\section{Visual Question-Answering (VQA)}

\subsection{Tips and Tricks for Visual Question-Answering: Learnings from the
            2017 Challenge}

A~\num{3000} GPU-hour hyperparameter search over different VQA method
configurations.


\subsubsection{Data}

VQA~2.0~\citet{goyal2017making}. Visual Genome provides small
($\approx 0.8\%$?) improvement.


\subsubsection{Model}

Gated tanh gives a large performance increase ($\approx 5\%$ over tanh,
$\approx 3\%$ over ReLU) over other activations.

Faster-RCNN features trained on Visual Genome give a large performance boost
($\approx 4\%$).  See bottom-up attention paper~\citet{anderson2017bottom}.

Uni-directional GRU used, pre-trained with GloVe embeddings (small performance
boost over training embeddings from scratch).

Individual sigmoids provide a significant boost compared with using softmax.

Ensembles provide a large performance boost.


\subsubsection{Inference}

ResNet features subsampled to~$7 \times 7$ provides a~$\approx 2\%$ boost
over~$14 \times 14$ features.


\subsubsection{Criticism}

Criticizing using soft scores (part of VQA~2.0 annotations) gives a small
performance boost over using hard binary scores.

Individual sigmoids trained with cross-entropy on multiple annotations (i.e.,
treat VQA as a multi-label problem).


\bibliographystyle{apalike}
\bibliography{ml-reading-notes}

\end{document}
