% TODO(brendan): Question to answer: what is the effect of attaching multiple
% losses on multiple heads of a network, in terms of their respective influence
% on the update of weights in lower layers of the network during
% backpropagation?
% Are the relative magnitudes of the losses from the different heads important
% in terms of their influence on weight updates? Will a loss with a magnitude
% 10^3 times larger dominate a 10^3 times smaller loss in influencing weight
% updates?

\documentclass[a4paper, 12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbold}
\usepackage{bm}
\usepackage{cite}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{inputenc}
\usepackage{mathtools}
\usepackage{titling}
\usepackage{siunitx}
\usepackage{txfonts}
\usepackage{url}

\newcommand\phantomlabel[1]{\phantomsection\label{#1}}

\DeclarePairedDelimiter\norm{\lVert}{\rVert}%

\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
        #1\;\delimsize\|\;#2%
}
\newcommand{\infdiv}{D_{KL}\infdivx}

\date{\today}
\title{Machine Learning Reading Notes}

\author{Brendan Duke}

\begin{document}

\maketitle


\part{Definitions}


% TODO(brendan):
\phantomlabel{backprop}
\textbf{Back propagation}


\phantomlabel{batchnorm}
\textbf{Batch Normalization}


\textbf{Deep Neural Networks (DNNs)} are engineered systems inspired by the
biological brain\cite{Goodfellow-et-al-2016-Book}.


\phantomlabel{kl}
\textbf{KL divergence} is given in Equation~\ref{kleqn}\cite[Chapter~3]{Goodfellow-et-al-2016-Book}.

\begin{equation}
        \infdiv{P}{Q} = \varmathbb{E}_{x \sim P}\left[\log P(x) - \log Q(x)\right]
        \label{kleqn}
\end{equation}

\textbf{Cross-entropy} is related to \hyperref[kl]{KL divergence} by
$H(P, Q) = H(P) + \infdiv{P}{Q}$, where $H(P)$, the \textbf{Shannon entropy}, is
$H(P) = -\varmathbb{E}_{x \sim P} \left[\log P(x)\right]$.


\phantomlabel{LSTM}
% TODO(brendan): Diagrams and equations for LSTM. Definition for RNNs. Should
% be a distillation of Chapter 10 of Goodfellow book.
\textbf{LSTM} (Long Short Term
Memory) neural networks are a type of recurrent neural network whose
characteristic feature is the presence of a gated self-loop that allows
retention of its ``cell state'', which are the pre-non-linearity activations of
the previous time step\cite[Chapter~10]{Goodfellow-et-al-2016-Book}.

Cell state is updated at each time step according to
Equation~\ref{lstm_cell_state_update}.

\begin{equation}
        s_i^{(t)} = f_i^{(t)} s_i^{(t - 1)} + g_i^{(t)}
                \sigma \left( b_i + \sum_j U_{i, j} x_j^{(t)} + \sum_j W_{i, j} h_j^{(t - 1)}\right)
        \label{lstm_cell_state_update}
\end{equation}

The vectors $\boldsymbol{f^{(t)}}$ and $\boldsymbol{g^{(t)}}$ in
Equation~\ref{lstm_cell_state_update} also take inputs from $\boldsymbol{x^{(t)}}$
and $\boldsymbol{h^{(t - 1)}}$, with their own weight tensors and bias vectors
$\boldsymbol{U}^f$, $\boldsymbol{W}^f$ and $\boldsymbol{b}^f$, $\boldsymbol{U}^g$,
$\boldsymbol{W}^g$ and $\boldsymbol{b}^f$, respectively.

Similar gate functions exist to gate the inputs and outputs to the LSTM, as
well.


\textbf{Mahalanobis Distance}


\phantomlabel{multilayer_perceptron}
\textbf{Multi-Layer Perceptrons (MLPs)} are mathematical functions mapping some
set of input values to some set of output
values\cite{Goodfellow-et-al-2016-Book}.


\textbf{Neighbourhood Components Analysis (NCA)} is a method of learning a
Mahalanobis distance metric, and can also be used in linear dimensionality
reduction\cite{NIPS2004_2566}.

The \textbf{PCKh} metric, used by the MPII Human Pose Dataset, defines a joint
estimate as matching the ground truth if the estimate lies within 50\% of the
head segment length\cite{andriluka-2d-2014-853}. The head segment length is
defined as the diagonal across the annotated head rectangle in the MPII data,
multiplied by a factor of 0.6. Details can be found by examining the MATLAB
\href{http://human-pose.mpi-inf.mpg.de/results/mpii_human_pose/evalMPII.zip}{evaluation script}
provided with the MPII dataset.


\phantomlabel{nonmax_supression}
\textbf{Non-maximum suppression} in object detection, in general, is a set of
methods used to prune an initial set of object bounding boxes that may be
uncorrelated with the actual object detections in an image, down to a subset
that are\cite{DBLP:conf/accv/RotheGG14}. In edge detection, non-maximum
suppression is used to suppress any pixels (i.e.\ not include them in the set of
detected edges) that are not the maximum response in their neighbourhood.

The \textbf{softmax function} is a continuous differentiable version of the
argmax function, where the result is represented as a one-hot
vector\cite[Chapter~6]{Goodfellow-et-al-2016-Book}. Softmax is a way of
representing probability distributions over a discrete variable that can take
on $n$ possible values.

Formally, softmax is given by Equation~\ref{softmax_eqn}.

\begin{equation}
        \textrm{softmax}{(\boldsymbol{z})}_i = \frac{e^{z_i}}{\sum_je^{z_j}}
        \label{softmax_eqn}
\end{equation}


\phantomlabel{rectified_linear_units}
\textbf{Rectified linear units (ReLUs)}\cite{icml2010_NairH10}


\textbf{Leaky rectified linear units (LReLs)}\cite{maas_rectified_nonlinearities}

\begin{equation}
        h^{(i)} = \max\left(w^{(i)T}x, 0.01w^{(i)T}x\right) =
        \begin{cases}
                w^{(i)T}x & \textrm{if } w^{(i)T}x > 0 \\
                0.01w^{(i)T}x & \textrm{otherwise}
        \end{cases}
\end{equation}

Leaky ReLUs have non-zero gradient over their domain, and were therefore
motivated in reducing the vanishing gradient problem.


\phantomlabel{dilated_convolutions}
\textbf{Dilated Convolutions}~\cite{DBLP:journals/corr/YuK15} can be defined as
follows.

Let $F: \mathbb{Z}^2 \rightarrow \mathbb{R}$ be a discrete
function, let $\Omega_r = {[-r, r]}^2 \cap \mathbb{Z}^2$ and let
$k: \Omega_r \rightarrow \mathbb{R}$ be a discrete filter of size
${(2r + 1)}^2$. Then the convolution operator is defined by
Equation~\ref{conv-op}.

\begin{equation}
        (F * k)(\mathbf{p}) = \sum_{\mathbf{s} + \mathbf{t} = \mathbf{p}} F(\mathbf{s}) + k(\mathbf{t})
        \label{conv-op}
\end{equation}

Furthermore, the dilated convolution operator, denoted by $*_l$, is defined by
Equation~\ref{dilated-conv-op}.

\begin{equation}
        (F *_l k)(\mathbf{p}) = \sum_{\mathbf{s} + l\mathbf{t} = \mathbf{p}} F(\mathbf{s}) + k(\mathbf{t})
        \label{dilated-conv-op}
\end{equation}


\part{Book Summaries}


\section{Machine Learning: A Probabilistic Perspective}
% TODO(brendan): citation


\subsection{Probability}

Bayes rule is:

\begin{equation}
        p(Y = y | X = x) = \frac{p(Y = y, X = x)}{p(X = x)}
                = \frac{p(X = x) p(Y = y | X = x)}
                       {\sum_{x'} p(X = x') p(Y = y | X = x')}
\end{equation}


\subsection{Generative Models for Discrete Data}

The \textbf{maximum a posteriori (MAP)} estimate is defined as
$\hat{y} = \textrm{argmax}_{c} p(y = c | \mathbf{x}, D)$.

A \textbf{prior} is a probability distribution assigned to each hypothesis $h
\in \mathcal{H}$ in the hypothesis space, based only on knowledge external to
the training data. E.g.\ in the space of related numbers under 100, a strong
prior may be assigned to ``odd numbers'' or ``even numbers'', while a weak
prior would be assigned to unintuitive concepts such as ``all powers of 2
except 32''.

The \textbf{likelihood} of a hypothesis given a set of training data, is the
probability of randomly sampling exactly that set of training data, given the
hypothesis, i.e. $p{(\mathcal{D} | h)} = {(1 / |h|)}^N$.

The \textbf{posterior} is:

\begin{equation}
        p(h | \mathcal{D})
                = \frac{p(h) p(\mathcal{D} | h)}
                       {\sum_{h' \in \mathcal{H}} p(\mathcal{D} | h')}
\end{equation}

where $p(\mathcal{D} | h)$ is ${(1 / |h|)}^N$ if the training data match the
hypothesis $h$, otherwise zero.

The MAP is
$\textrm{argmax}_h p(h) p(\mathcal{D} | h) = \textrm{argmax}_h \left[ \log{p(h)} + \log{p(\mathcal{D} | h)} \right]$.
In the limit of infinite training data, the term exponential in $N$ in
$\log{p(\mathcal{D} | h)}$ will dominate. Therefore the
\textbf{maximum likelihood estimator (MLE)}
$\textrm{argmax}_h \log{p(\mathcal{D} | h)}$ is converged to by the MAP in the
limit of infinite data, justifying the MLE's use as an objective function.


\section{Deep Learning~\cite{Goodfellow-et-al-2016-Book}}


\subsection{Machine Learning Basics}

Maximum Likelihood Estimation (MLE) is maximization of the log-likelihood
$\log p_{model}(y | \mathbf{x}; \mathbf{\theta})$, where $y$ is a ground truth
example, $\mathbf{x}$ is an input feature vector, and $\mathbf{\theta}$ are
model parameters.

Principal Component Analysis (PCA) involves projecting input feature vectors
$\mathbf{x}$ into a reduced-dimensionality space via multiplication by
$D \in \mathbb{R}^{n \times l}$. The PCA projection minimizes the $L_2$
reconstruction error $||\mathbf{x} - r(\mathbf{x})||_2$.

Stocastic Gradient Descent (SGD) is gradient descent over minibatches. For an
objective function $L = f(y; x, \theta)$, at each step $t$ we have
$\theta_t = \frac{1}{m'} \sum_{i = 0}^{m'} \theta_{t - 1} - \epsilon \nabla f$.


\part{Datasets}


\section{CIFAR-10~\cite{cifar10-website}}
\label{cifar10}

CIFAR-10~\cite{cifar10-website} consists of \num{60000} colour images of $32
\times 32$ resolution, has 10 classes and \num{6000} images per class.


\section{HMDB-51~\cite{Kuehne11}}
\label{hmdb51}

HMDB-51 contains 7000 clips extracted from movies and YouTube then manually
annotated with 51 class labels.


\section{THUMOS~2014~\cite{DBLP:journals/corr/IdreesZJGLSS16}}
\label{thumos}

THUMOS~2014~\cite{DBLP:journals/corr/IdreesZJGLSS16} consists of 101 action
classes. THUMOS action classes are from UCF-101~\ref{ucf101}. All videos are
from YouTube, and are labelled with action class and temporal span of the
action. THUMOS~2014 contains 20 action classes, 2755 trimmed training videos
and 1010 untrimmed validation videos containing 3007 action instances. 213 test
videos, with 3358 action instances, exist that are not entirely background.

The THUMOS~2015 dataset extends THUMOS~2014 with a total of 5613 positive and
background untrimmed videos.


\section{UCF-101\cite{DBLP:journals/corr/abs-1212-0402}}
\label{ucf101}

101 classes, 13k clips and 27 hours of video data in total.


\section{WMT~2014~\cite{wmt14-translation-website}}
\label{wmt2014}

WMT~2014 is a machine translation task with five language pairs:
French-English, Hindi-English, German-English, Czech-English and
Russian-English. A number of parallel and monolingual corpora are included as
training data.

The majority of the training data is taken
from~\href{http://www.statmt.org/europarl/}{Europarl}~v7, from which about
50~million words per language are used in WMT~2014. Additional training data is
taken from the~\href{http://www.casmacat.eu/corpus/news-commentary.html}{News
Commentary Parallel Corpus}, at about three million words per language.

Test data from previous years is provided, with on the order of~\num{3000}
sentences per test set.


\part{Paper Summaries}


\section{Human Pose}


\subsection{DeepPose: Human Pose Estimation via Deep Neural
            Networks\cite{DBLP:journals/corr/ToshevS13}}

This paper uses DNNs as a method for human pose estimation, based on the
success of~\cite{NIPS2013_5207} and~\cite{DBLP:journals/corr/GirshickDDM13} for
object detection using DNNs.

This is in contrast to the existing work in human pose estimation at the time,
which focused on explicitly designed pose models. Papers about these methods
can be found in the ``Related Work'' section of
\cite{DBLP:journals/corr/ToshevS13}.

The input to the 7-layered convolutional DNN (based on
AlexNet\cite{NIPS2012_4824}) is the full image.


\subsection{End-to-end people detection in crowded
            scenes\cite{DBLP:journals/corr/StewartA15}}

This paper is focused on jointly creating a set of bounding-box predictions for
people in crowded scenes using GoogLeNet and a
\hyperref[LSTM]{recurrent LSTM layer} as a controller. Since bounding-box
predictions are generated jointly, common post-processing steps such as
\hyperref[nonmax_supression]{non-maximum suppression} are unnecessary.  All
components of the system are trained end-to-end using back propagation.

\subsubsection{Motivation}

The end-to-end people detection method is contrasted with the object detection
methods of R-CNN in~\cite{DBLP:journals/corr/GirshickDDM13} and OverFeat in
\cite{DBLP:journals/corr/SermanetEZMFL13}.
\cite{DBLP:journals/corr/GirshickDDM13} and
\cite{DBLP:journals/corr/SermanetEZMFL13} rely on non-maximum suppression,
which does not use access to image information to infer bounding box positions
since non-maximum suppression acts only on bounding boxes. Also, in end-to-end
people detection, the decoding stage is learned using LSTMs, instead of using
specialized methods as in~\cite{VisualPhrases} and~\cite{TaAnSc_14:occluded}.

Early related work can be found in~\cite{Felzenszwalb:2010:ODD:1850486.1850574}
and~\cite{Leibe:2005:PDC:1068507.1069006}. Best performing object detectors at
the time were~\cite{DBLP:journals/corr/GirshickDDM13},
\cite{DBLP:journals/corr/SermanetEZMFL13}, \cite{Uijlings13},
\cite{DBLP:journals/corr/ZhangBS15} and~\cite{DBLP:journals/corr/SzegedyREA14}.

Sequence modeling is done using LSTMs as in
\cite{DBLP:journals/corr/SutskeverVL14} (used for machine translation) and
\cite{DBLP:journals/corr/KarpathyF14} (used for image captioning). The loss
function is similar to the loss function proposed in
\cite{Graves06connectionisttemporal} in that the loss function encourages the
model to make predictions in descending order of confidence.

\subsubsection{Data}

A new training set collected from public webcams, called ``Brainwash'', is
produced. Brainwash consists of 11917 images with 91146 labelled people. 1000
images are allocated for testing and validation, hence training, test and
validation sets contain 82906, 4922 and 3318 labels, respectively.

\subsubsection{Model}

A pre-trained GoogLeNet\cite{going-deeper-szegedy43022} is used to produce
encoded features as input to the LSTM\@. The GoogLeNet features are further
fine-tuned by the training process.  Using GoogLeNet, a feature vector of
length 1024 is produced for each region over a $(15, 20)$ grid of regions that
covers the entire $(480, 640)$ input image. Each cell in the grid has a
receptive field of $(139, 139)$, and is trained to produce a set (with fixed
cardinality five) of distinct bounding boxes in the center $(64, 64)$ region.

$L_2$ regularization of weights in the network was removed entirely.

GoogLeNet activations are scaled down by a factor of 100 before being input to
the decoder, since decoder weights are initialized according to a uniform
distribution in $[-0.1, 0.1]$, while GoogLeNet activations are in $[-80, 80]$.
Regression predictions from GoogLeNet are scaled up by 100 before comparing
with ground truth locations (which are in $[-64, 64]$).

At each step, the LSTM for each grid cell, of which there are 300 in total,
produces a new bounding box and corresponding confidence that the bounding box
contains a person $\boldsymbol{b} = \{\boldsymbol{b}_{pos}, b_c\}$, where
$\boldsymbol{b}_{pos} = (b_x, b_y, b_w, b_h) \in \varmathbb{R}^4$ and
$b_c \in [0, 1]$. The prediction algorithm stops when the confidence drops
below a set threshold. The LSTM units have 250 memory states, no bias units,
and no output non-linearities. Each LSTM unit adds its output to the image
representation, and feeds the result into the next LSTM unit. Comparable
results are found by only presenting the image representation as input to the
first LSTM unit.

Dropout with probability 0.15 is used on the output of each LSTM\@.

\subsubsection{Inference}

The system is trained with learning rate 0.2, decreased by a factor of 0.8
every 100 000 iterations (with convergence occurring after 500 000 iterations),
and momentum 0.5. Gradient clipping is done at 2-norm of 0.1.

Images are jittered by up to 32 pixels in horizontal and vertical directions,
and scaled by a factor between 0.9 and 1.1.

At test time, per-region predictions are merged by adding a new region at each
iteration, and destroying any new bounding boxes that overlap previously
accepted bounding boxes, under the constraint that any given bounding box can
destroy at most one other bounding box. An ordering function
$\Delta': A \times C \rightarrow \varmathbb{N} \times \varmathbb{R}$ given by
$\Delta'(\boldsymbol{b}_i, \tilde{\boldsymbol{b}}_j) = (m_{ij}, d_{ij})$ where
$m_{ij}$ denotes intersection of boxes and $d_{ij}$ is $L_1$ displacement, is
minimized using the Hungarian algorithm in order to find a bipartite matching.
At each step, any new candidate that is not intersecting in the matching is
added to the set of accepted candidates.

\subsubsection{Criticism}

A new loss function that operates on sets of bounding-box predictions is
introduced. Denoting bounding boxes generated by the model as
$C = \{\tilde{\boldsymbol{b}}_i\}$, and ground truth bounding boxes by
$G = \{\boldsymbol{b}_i\}$, the loss function is given by
Equation~\ref{loss-eqn}.

\begin{equation}
        L(G, C, f) = \alpha\sum_i^{|G|}
                             l_{pos}\left(\tilde{\boldsymbol{b}}_{pos}^i, \boldsymbol{b}_{pos}^{f(i)}\right) +
                     \sum_j^{|C|} l_c\left(\tilde{b}_c^j, y_j\right)
        \label{loss-eqn}
\end{equation}

In Equation~\ref{loss-eqn}, $f(i)$ is an injective function $G \rightarrow C$
that assigns one ground truth to each index $i$ up to the number of ground
truths, $l_{pos}$ is the $L_1$ displacement between bounding boxes, and $l_c$
is a cross-entropy loss on a candidate's confidence that a bounding box exists,
where $y_j = \mathbb{1}\{f^{-1}(j) \neq \varnothing\}$. $\alpha$ is set to 0.03
from cross-validation.

In creating $f(i)$ in Equation~\ref{loss-eqn} to assign candidate predictions
to ground truths, the
$G \times C \rightarrow \varmathbb{R} \times \varmathbb{N} \times \varmathbb{N}$
function
$\Delta\left(\boldsymbol{b}^i, \tilde{\boldsymbol{b}}^j\right) = (o_{ij}, r_i, d_{ij})$
is used to lexicographically order pairs first by $o$, then $r$, then $d$,
where $o$ is one if there is sufficient overlap between candidate and ground
truth and zero otherwise, $r$ is the prediction's confidence, and $d$ is the
$L_1$ displacement between candidate and ground truth bounding boxes.

\subsubsection{Experiments}

With an AP (average precision) of 0.78 and EER (equal error rate) of 0.81, the
$f(i)$ produced by minimizing $\Delta$, using the
\href{https://en.wikipedia.org/wiki/Hungarian_algorithm}{Hungarian algorithm},
is found to improve on AP and EER compared with a fixed assignment of $f(i)$,
or selecting the first $k$ highest ranked ($L_\textrm{firstk}$). COUNT
(Absolute difference between number of predicted and ground truth detections)
for $f(i)$ with Hungarian was 0.76 compared with 0.74 for $L_\textrm{firstk}$.
As a baseline, Overfeat-GoogLeNet (bounding-box regression on each cell,
followed by non-maximum suppression, as in
\cite{DBLP:journals/corr/SermanetEZMFL13}) achieved 0.67, 0.71 and 1.05 AP, EER
and COUNT, respectively.

Training without finetuning GoogLeNet reduces AP by 0.29.

Removal of dropout from the output of each LSTM decreases AP by 0.011.

When using the original $2^{-4}$ $L_2$ weights regularization multiplier on
GoogLeNet only, the network was unable to train.  An $L_2$ regularization
multiplier on GoogLeNet of $10^{-6}$ reduced AP by 0.03.

It is found that AP (on the validation set) increases from 0.82 to 0.85 when
using separate weights connecting each of the LSTM outputs to predicted
candidates.


\section{Regularization}


\subsection{Dropout: A Simple Way to Prevent Neural Networks from
            Overfitting\cite{Srivastava:2014:DSW:2627435.2670313}}
\label{dropout}

\textbf{Dropout} is a technique used to overcome the problem of overfitting in
deep neural nets with large numbers of parameters. The idea is to train using
many ``thinned'' networks, chosen by randomly removing subsets of units and
their connections. The predictions from the thinned networks are approximately
averaged at test time by using a single, unthinned, network with reduced
weights.

\begin{itemize}
        \item Existing regularization methods: stopping training as soon as
                validation error stops improving, L1 and L2 regularization, and
                weight sharing\cite{Nowlan:1992:SNN:148167.148169}.
\end{itemize}


\section{CNN Architecture}


\subsection{Deep Residual Learning for Image
            Recognition\cite{DBLP:journals/corr/HeZRS15}}

A technique, training of residual functions, is presented for deep neural
network architecture design, which allows training of deeper networks with
improved accuracy compared to not training residual functions.

\cite{going-deeper-szegedy43022} and~\cite{DBLP:journals/corr/SimonyanZ14a} are
referred to as motivating ``very deep'' models.

% TODO(brendan): Read [1], [9] for vanishing gradient, R-CNN series for
% localization.


\section{GANs}


\subsection{Generative Adversarial Networks\cite{NIPS2014_5423}}
\label{gan}

A method of generating probability distributions is presented that can be
trained end-to-end when used with \hyperref[multilayer_perceptron]{MLPs}. In
the given method, a discriminator network $D$ is optimized to distinguish from
ground truth the samples from the generated distribution $G$. $G$ is optimized
to cause $D$ to become $\frac{1}{2}$ everywhere.

Motivation points to~\cite{deepSpeechReviewSPM2012} and~\cite{NIPS2012_4824} as
successful uses of deep discriminative networks for classification based on
\hyperref[backprop]{back propagation}, \hyperref[dropout]{dropout} and
piecewise linear units (such as \hyperref[rectified_linear_units]{ReLUs}).


\subsection{Image-to-Image Translation with Conditional Adversarial
            Networks\cite{DBLP:journals/corr/IsolaZZE16}}

A loss function is presented for image-to-image translation that can be applied
to different tasks, such as colourizing images and reconstructing photos from
label maps or edges.

\href{https://github.com/phillipi/pix2pix}{Code} is available.

\subsubsection{Motivation}

\cite{DBLP:journals/corr/LarsenSW15}, \cite{DBLP:journals/corr/PathakKDDE16}
and~\cite{DBLP:journals/corr/ZhangIE16} are noted as evidence that applying
Euclidean distance loss alone on generated images produces blurry results.

\cite{DBLP:journals/corr/PathakKDDE16} also noted that it was helpful to mix
the GAN objective with a pixel-wise loss such as an L2 loss.

\cite{NIPS2014_5423}, \cite{DBLP:journals/corr/DentonCSF15},
\cite{DBLP:journals/corr/RadfordMC15},
\cite{DBLP:journals/corr/SalimansGZCRC16}, and
\cite{DBLP:journals/corr/ZhaoML16} are mentioned as prior work in
\hyperref[gan]{GANs}, and specifically conditional GANs are explored, as
suggested by~\cite{NIPS2014_5423}.

As opposed to image modelling losses where pixels are conditionally independent
from each other (e.g.~\cite{DBLP:journals/corr/ShelhamerLD16},
\cite{DBLP:journals/corr/XieT15}, \cite{IizukaSIGGRAPH2016},
\cite{DBLP:journals/corr/LarssonMS16} and \cite{DBLP:journals/corr/ZhangIE16}),
a ``structured loss'' is used. Other examples of structured losses are in
\cite{DBLP:journals/corr/ChenPKMY14} (conditional random fields),
\cite{DBLP:journals/corr/DosovitskiyB16} (feature matching),
\cite{DBLP:journals/corr/LiW16} (non-parametric losses),
\cite{DBLP:journals/corr/XieHT15} (pseudo-priors) and
\cite{DBLP:journals/corr/JohnsonAL16} (losses based on matching covariance
statistics).

Previous work on conditional GANs exists in~\cite{DBLP:journals/corr/MirzaO14}
(generating MNIST digits from discrete labels) and
\cite{DBLP:journals/corr/ReedAYLSL16} (image generation from text). Work on
image generation with conditional GANs has focused on image
inpainting\cite{DBLP:journals/corr/PathakKDDE16}, image prediction from a
normal map\cite{DBLP:journals/corr/WangG16}, generating images from user
input\cite{DBLP:journals/corr/WangG16}, predicting future
frames\cite{DBLP:journals/corr/MathieuCL15}, predicting future states based on
time-lapses of objects\cite{DBLP:journals/corr/ZhouB16b}, generating photos of
clothing from input images of clothed people\cite{DBLP:journals/corr/YooKPPK16}
and style transfer\cite{DBLP:journals/corr/LiW16b}.

The generator and discriminator architectures are motivated by
\cite{DBLP:journals/corr/RadfordMC15}.

Instance normalization is introduced in~\cite{DBLP:journals/corr/UlyanovVL16}.

\subsubsection{Data}

The cityscapes\cite{DBLP:journals/corr/CordtsORREBFRS16} (semantic labels
$\leftrightarrow$ photo), CMP Facades (architectural labels $\rightarrow$
photo), Google maps (map $\leftrightarrow$ aerial photo),
ImageNet\cite{DBLP:journals/corr/RussakovskyDSKSMHKKBBF14} (BW $\rightarrow$
colour), \cite{zhu2016generative} and~\cite{fine-grained} (edges $\rightarrow$
photo using the HED edge detector\cite{DBLP:journals/corr/XieT15}),
\cite{Eitz:2012:HSO:2185520.2185540} (sketch $\rightarrow$ photo) and
\cite{Laffont14} (day $\rightarrow$ night).

\subsubsection{Model}

A U-Net\cite{DBLP:journals/corr/RonnebergerFB15} architecture is used for the
generator. A PatchGAN architecture\cite{DBLP:journals/corr/LiW16b} is used for
the discriminator.

Generator architecture: encoder: $C64-C128-C256-C512-C512-C512-C512-C512$,
decoder: $CD512-CD512-CD512-C512-C512-C256-C128-C64$, where $Ck$ denotes
Convolution-BatchNorm-ReLU and $CDk$ denotes
Convolution-BatchNorm-Dropout-ReLU with a dropout rate of $50\%$. After the
last layer, a convolution maps to the number of output channels, followed by a
$\tanh$ function.

In the case of the U-Net, there are skip connections between the $i$th level of
the encoder and the $(n - i)$th layer of the decoder.

The $(70, 70)$ discriminator architecture is $C64-C128-C256-C512$, $(1, 1)$ and
$(16, 16)$ are $C64-C128$, and $(256, 256)$ is $C64-C128-C256-C512-C512-C512$.

All ReLUs in the encoder are leaky, with slope 0.2.

\subsubsection{Inference}

Network weights were initialized from a Gaussian distribution with mean zero
and standard deviation 0.02.

Noise was supplied in the form of dropout, which is applied both at training
and test time.

\hyperref[batchnorm]{Batch normalization} is applied using the statistics of
the test batch, instead of the aggregated training data statistics. Doing so
with a batch size of one is instance
normalization\cite{DBLP:journals/corr/UlyanovVL16}. Batch sizes of one and four
were used.

Mini-batch SGD is used along with the Adam optimizer. One gradient descent step
on G is used, followed by a gradient descent step on D.

Random jitter was applied by resizing the $(256, 256)$ input to $(286, 286)$
and random cropping back down to $(256, 256)$.

Refer to Appendix 5.1.2 of~\cite{DBLP:journals/corr/IsolaZZE16} for specific
training details for each dataset.

\subsubsection{Criticism}

The objective function of a conditional GAN is shown in Equation~\ref{discrim_condition_eqn}.

\begin{equation}
        \mathcal{L}_{cGAN} = \varmathbb{E}_{x, y \sim p_{data}(x, y)}\left[\log D(x, y)\right] +
                             \varmathbb{E}_{x \sim p_{data}(x), y \sim p_z (z)}\left[\log \left(1 - D\left(x, G(x, z)\right)\right)\right]
        \label{discrim_condition_eqn}
\end{equation}

Equation~\ref{discrim_no_condition_eqn} is the objective function where the
discriminator has no prior information about $x$.

\begin{equation}
        \mathcal{L}_{cGAN}\left(G, D\right) =
                \varmathbb{E}_{y \sim p_{data}(y)}\left[\log D(y)\right] +
                \varmathbb{E}_{x \sim p_{data}(x), z \sim p_z (z)}\left[\log \left(1 - D\left(G(x, z)\right)\right)\right]
        \label{discrim_no_condition_eqn}
\end{equation}

An $L1$ loss is attached, as given in Equation~\ref{l1_loss}.

\begin{equation}
        \mathcal{L}_{L1}\left(G\right) = \varmathbb{E}_{x, y \sim p_{data}(x, y)}\left[\norm{y - G(x, z)}_1\right]
        \label{l1_loss}
\end{equation}

The final objective function is
$G^* = \arg\min_G \max_D \mathcal{L}_{cGAN}\left(G, D\right) + \lambda \mathcal{L}_{L1}\left(G\right)$.

\subsubsection{Experiments}

It was found in initial experiments that $G$ learned to ignore its input noise
$z$.

Only minor variation due to the dropout noise applied is observed in the
generated samples.

Engineering generative networks that produce stochastic dependence on noise
input is left as an open research problem.

Little difference was found between using batch sizes of one and four.

Inputs and outputs in all experiments are 1--3 channel images.

Amazon Mechanical Turk was used to show participants a ground truth or
generated image at $(256, 256)$ resolution for one second, after which the
participant had to respond whether the shown image was real or fake.

$6.1\% \pm 1.3\%$ and $18.9\% \pm 2.5\%$ of participants labelled photo
$\rightarrow$ map and map $\rightarrow$ photo generated images as real,
respectively, with L1 + cGAN loss improving over L1 alone.

On colourization, the cGAN achieved $22.5\% \pm 1.6\%$ of responses as
``real'', as compared with $27.8\% \pm 2.7\%$ in
\cite{DBLP:journals/corr/ZhangIE16}.

FCN-8 was trained for semantic classification on a real dataset, after which
its accuracy on the $(70, 70)$ PatchGAN on cityscapes hit 0.63 per-pixel, 0.21 per
class and 0.16 class IoU, higher than $(1, 1)$ and $(256, 256)$ PatchGAN
discriminators.

% NOTE(brendan): 1x1, 70x70 and 256x256 used differing numbers of layers in
% their architectures, so there is no control for number of layers vs.
% receptive field causing the improved performance.

L1 + cGAN loss is found to perform better overall than L1 + GAN or cGAN
objective functions, with the FCN-8 metric scores given above.

Colour distributions in lab colour space are compared for different objective
functions, with L1 loss producing a narrower colour distribution, and cGAN loss
producing a colour distribution closer to that of the ground truth.

The U-Net qualitatively achieves better generated results with both cGAN and L1
loss than an encoder-decoder without skips, with the latter collapsing to
nearly identical results for all label maps.

A generator is trained on $(256, 256)$ images, and evaluated on $(512, 512)$
map $\leftrightarrow$ aerial images.

A cGAN is trained on semantic segmentation labelling of
Cityscapes\cite{DBLP:journals/corr/CordtsORREBFRS16} and achieves 0.22 class
IoU with cGAN loss alone, compared with 0.35 class IoU with L1 loss and 0.80
class IoU using the wide ResNets of~\cite{DBLP:journals/corr/WuSH16e}.


\section{Human Activity Recognition}


\subsection{Quo Vadis, Action Recognition? A New Model and the Kinetics
            Dataset\cite{carreira2017quo}}

\subsubsection{Data}

The Kinetics Human Action Video Dataset\cite{kay2017kinetics}, which has 400
human action classes each with more than 400 examples. The classes are focused
on human actions, such as pouring or kissing, as opposed to activities (such as
tennis or baseball). The clips are 10s long.

The Kinetics test set is 100 clips for each class.

A miniKinetics version of the dataset was used in the paper, and for
experimentation, with 213 classes and 120k clips split into three subsets: one
for training with 150 to 1000 clips per class, and one split each for
validation and test, containing 25 and 75 clips per class each, respectively.

\hyperref[ucf101]{UCF-101}.

\hyperref[hmdb51]{HMDB-51}.

Transfer learning is done on UCF-101 and HMDB-51, these two smaller datasets,
from the larger Kinetics dataset.

\subsubsection{Model}

Inception-V1 with batch normalization is used as a common ``backbone''
architecture for all models.

\textbf{Two-Stream Inflated 3D ConvNets (I3D)}. All kernels in the Inception-V1
model are inflated, i.e. $(N, N)$ kernels become $(N, N, N)$ kernels.

Weights of $2D$ filters are tiled $N$ times across the time dimension.

The first two max-pooling layers of Inception-V1 were altered to be
$(1, 3, 3)$, and have unity stride in time.

\textbf{2D ConvNets with LSTMs on top}. An LSTM layer with batch normalization,
and 512 hidden units, is placed after the last average pooling layer in
Inception-V1. A fully connected layer is added on top for classification.

\textbf{Two-stream networks with different stream fusion techniques}.
Predictions from an RGB frame are averaged with predictions from pre-computed
optical flow for ten frames surrounding the RGB frame.

The flow stream has twice as many input channels as flow frames: one for each
of up and down flow directions.

The two streams are fused after the last convolutional layer.

Inputs to the two-stream model are a sequence of five frames, sampled ten
frames apart from a 25 fps video, as well as the corresponding optical flow
features.

Features from the $(5, 7, 7)$ activations before the last average pooling layer
of Inception-V1 are passed through a $C_{512} \rightarrow P_3 \rightarrow FC_{?}$ 3D
convolutional network, where $P_3$ is a $(3, 3, 3)$ max-pooling layer. These
layers are initialized with Gaussians.

The averaging process is learnable.

\textbf{C3D}. A network with eight convolutional layers, five pooling layers
and two fully connected layers at the top is used. Inputs are 16-frame,
$(112, 112)$ clips, as in~\cite{DBLP:journals/corr/TranBFTP14}. Batch
normalization is used after each (convolutional and fully connected) layer. In
the first layer, a temporal stride of two (compared with one) is used, in order
to fit 15 videos per batch per K40 GPU\@.

\subsubsection{Inference}

Video streams are decoded at 25 fps.

Standard SGD with momentum set to 0.9 is used, with synchronous data
parallelization across 32 GPUs for all models except C3D, for which data
parallelization across 64 GPUs was used.

Models are trained on miniKinetics for 35k steps, and for 100k steps on
Kinetics, with a 10x reduction on learning rate when validation loss plateaued.

Models were trained for up to 5k steps on UCF-101 and HMDB-51, using 16 GPUs.

\textbf{Data augmentations} used are random cropping, by resizing the smaller
video side to 256 pixels then randomly cropping a $(224, 224)$ patch, and
temporal random cropping. Random left-right flipping was applied.

Shorter videos are looped to satisfy each model's input requirements.

Training with \textbf{Two-Stream Inflated 3D ConvNets (I3D)} is done with
64-frame snippets, and test is done using entire videos.

Test inference is done by taking $(224, 224)$ center crops.

Optical flow is computed using the TV-$L^1$ algorithm
from~\cite{Zach07aduality}, which runs at 30 fps on $(320, 240)$ videos (on
decade-old GPU technology).

\subsubsection{Criticism}

For \textbf{2D ConvNets with LSTMs on top}, a cross-entropy loss is placed on
the outputs at all time steps.

\subsubsection{Results}

78.7\% on miniKinetics with Two-Stream I3D, compared with 74.0\% for 3D-Fused,
72.9\% for Two-Stream, 60.0\% for C3D and 69.9\% for Conv2D with LSTM\@.

Using pre-training on Kinetics, 98.0\% and 80.7\% is achieved, compared with
state of the art of 94.6\% and 70.3\% respectively on UCF-101 and HMDB-51.

% TODO(brendan): A couple more interesting findings from the results, e.g. with
% regards to optical flow.

\subsubsection{Questions}

\begin{itemize}
        \item \textbf{Future work?} Action tubes, or attention mechanisms to
                focus in on human actors.

                The authors suggest that the optical flow streams produce
                significant improvements, compared to the RGB stream alone, due
                to their optical flow algorithm's recurrent nature
                (optimization is done iteratively).  This could be disproved by
                using purely feed-forward optical flow computations, e.g.
                FlowNet 2.0.

                The authors mention specifically that they will repeat all
                experiments with Kinetics instead of miniKinetics, and compare
                with and without ImageNet pre-training. They will also compare
                inflating of different 2D ConvNet architectures (besides
                Inception-V1).
\end{itemize}

\subsubsection{Similar Work}

Convolutional Two-Stream Network Fusion for Video Action
Recognition~\cite{DBLP:journals/corr/FeichtenhoferPZ16} builds on two-stream
networks by fusing the streams' respective features earlier.


\subsection{Learning Spatiotemporal Features with 3D Convolutional
            Networks\cite{DBLP:journals/corr/TranBFTP14}}

\subsubsection{Data}

UCF-101~\ref{ucf101}.

Sports-1M~\cite{KarpathyCVPR14} (1.1 million sports videos, with 487 classes).

I380K (internal dataset, finetuned from).

\subsubsection{Model}

A Conv3D model is used, with $(3, 3, 3)$ kernels and $(2, 2, 2)$ max-pooling
layers, with the exception of the first max-pooling layer, which is
$(1, 2, 2)$.

Full model structure (testing effect of kernel size):
$C_{64} \rightarrow P_1 \rightarrow C_{128} \rightarrow P_2 \rightarrow
{(C_{256} \rightarrow P_2)}^3 \rightarrow {(FC_{2048})}^2 \rightarrow
\textrm{softmax}$, where $P_1$ refers to the $(1, 2, 2)$ max-pooling layer, and
$P_2$ refers to the $(2, 2, 2)$ max-pooling layer.

Full model structure (best results):
$C_{64} \rightarrow P_1 \rightarrow C_{128} \rightarrow P_2 \rightarrow
{(C_{256})}^2 \rightarrow P_2 \rightarrow
{({(C_{512})}^2 \rightarrow P_2)}^2 \rightarrow {(FC_{4096})}^2 \rightarrow
\textrm{softmax}$.

The last fully connected layer above goes to an $L2$ normalization, followed by
a linear SVM\@.

\subsubsection{Inference}

Inputs were resized to $(171, 128)$ then randomly cropped to $(112, 112)$.

Non-overlapping 16-frame clips were used during training, therefore batches of
$(\verb|batch_size|, 16, 112, 112, 3)$ were input to the model.

Hyper-parameters: batch size of 30, learning rate of 0.003 divided by four
every four epochs, for sixteen in total.

Five two-second long clips were extracted from each video and trained on.

For predictions on UCF101, a trained C3D model was used to extract features
from 16-frame clips, with overlap of 8-frames between subsequences. A linear
SVM was then trained on these C3D features.

\subsubsection{Criticism}

Cross-entropy on activity label?

\subsubsection{Results}

90.4\% on UCF-101, compared with 89.1\% state-of-the-art at the time.

84.4\% vs. $> 90\%$ on Sports-1M, although the technique
from\cite{DBLP:journals/corr/NgHVVMT15} could be applied on top of C3D
features.

On UCF-101, homogeneous temporal depth of kernels in the model were able to
beat increasing or decreasing temporal depth.

\subsubsection{Questions}

\begin{itemize}
\item \textbf{Any comparison with frame-frame ImageNet vectors?} None. The only
        comparison was with ImageNet features averaged over all frames.

\item \textbf{Which features complement C3D?} iDT\@? Optical flow?
\end{itemize}

\subsubsection{Similar Work}

Improved Dense Trajectories\cite{Wang2013}.

Learning Hierarchical Invariant Spatio-temporal Features for Action Recognition
with Independent Subspace Analysis\cite{Le:2011:LHI:2191740.2192108}.
Unsupervised video feature learning using stacking.

Two-stream networks\cite{DBLP:journals/corr/SimonyanZ14}.

Beyond Short Snippets: Deep Networks for Video
Classification\cite{DBLP:journals/corr/NgHVVMT15}. Connects Conv2D to LSTM, and
explores other temporal feature pooling models that work on Conv2D-extracted
features.


\section{Temporal Action Localization}


\subsection{CDC\@: Convolutional-De-Convolutional Networks for Precise Temporal
            Action Localization in Untrimmed
            Videos~\cite{DBLP:journals/corr/ShouCZMC17}}

\subsubsection{Data}

THUMOS~2014~\ref{thumos} was trained and evaluated on.

ActivityNet challenge 2016~\cite{Heilbron_2015_CVPR} has 203 activity
categories, with 137 untrimmed videos per class on average. ActivityNet 2016
has an average of 1.41 activity instances per video, and 849 hours of video in
total. Activitynet 2016 encompasses untrimmed video classification, trimmed
video classification and activity detection.

\subsubsection{Model}

The C3D architecture
from~\cite{DBLP:journals/corr/TranBFTP15, DBLP:journals/corr/TranBFTP14} up to
and including layer \verb|pool5| is built upon. \verb|pool5| is changed to only
pool in spatial dimensions.

\verb|pool5| is then followed by \verb|conv6|, which is a layer with filter
size $[4, 4, 4]$, with strides $[2, 1, 1]$, and the temporal part of the filter
doing an up-convolution while the spatial part does down-convolution (?). No
spatial padding, and ``same'' temporal padding is used, such that the input
$[L/8, 4, 4]$ feature map becomes a $[L/4, 1, 1]$ feature map.

This layer is called \verb|CDC6|, after ``conv-deconv'' layers, and is followed
by two temporal up-convolutional layers (both of filter size $[4, 1, 1]$ and
stride $[2, 1, 1]$) for a final output of shape $[L, 1, 1]$.  The pre-logits
layers \verb|CDC6| and \verb|CDC7| have 4096 channel outputs.

Dropout is used for all \verb|CDC| layers, with 0.5 dropout ratio.

\subsubsection{Inference}

Non-overlapping segments of 32 frames are extracted and fed to the CDC network.
During training, only segments with at least one frame with a non-background
label are used.

C3D is pre-trained on Sports-1M~\cite{KarpathyCVPR14}, and after this the CDC
network converges within four epochs.

Both CDC and C3D layers are trained jointly, with a learning rate of
$10^{-5}$ used for all layers except \verb|CDC8|, for which a learning rate
of $10^{-4}$ is used.

At test time, segment proposals from~\cite{DBLP:journals/corr/ShouWC16} are
fed to the CDC network, with the proposals expanded by $1/8$ of the original
proposal length on either end.

Using the per-frame predictions on the segment proposal, Gaussian kernel
density estimation is used to obtain $\mu$ and $\sigma$, and a refined segment
from $[\mu - \sigma, \mu + \sigma]$ is produced. The predicted class scores are
the average class scores over this refined segment.

To make segment proposals for the temporal localization task, non-maximum
suppression is used as
in~\cite{DBLP:journals/corr/YeungRJAML15, DBLP:journals/corr/ShouWC16}.

\subsubsection{Criticism}

A cross-entropy per-frame loss is used on the class predictions.

As an evaluation metric, mean average precision is computed over the
predictions for each frame.

For evaluation on the temporal localization task, mAP is computed over
different IoU thresholds.

\subsubsection{Results}

Results on THUMOS~2014 are given below.

Per-frame: 44.4 mAP compared with 41.3 mAP from
MultiLSTM~\cite{DBLP:journals/corr/YeungRJAML15}, and 41.7 mAP using
conv-deconv instead of CDC\@.

Temporal localization at 0.5 IoU\@: 23.3 mAP, as compared with 19.0 mAP
in~\cite{DBLP:journals/corr/ShouWC16}.

On ActivityNet (at 0.75 IoU): 26.0 when based on the segment proposals
of Wang and Tao in their ActivityNet 2016 challenge submission, as compared to
4.1 mAP without CDC\@. At 0.5 IoU, CDC only improves the method of Wang and Tao
from 45.1 to 45.3 mAP\@.

\subsubsection{Questions}

Tried conv-deconv vs. CDC, but what about deconv-conv? Or a series of 3d
convolutions with spatial stride fixed, until the last layer? I.e.\ test
whether the CDC filter is special when compared to simply adding more
parameters to the temporal up-convolutional filters.


\subsection{Temporal Action Localization in Untrimmed Videos via Multi-stage
            CNNs~\cite{DBLP:journals/corr/ShouWC16}}

\subsubsection{Data}

THUMOS~2014~\ref{thumos} was trained and evaluated on.

MEXaction2~\cite{MEXaction2} is a dataset with two action classes: horseback
riding, and ``bull charging cape''. The dataset is made up of YouTube, UCF101
and INA videos, of which only INA videos are untrimmed. The untrimmed videos
are 77 hours in total. The training set is 1336 instances, validation set is
310 instances and test set is 329 instances.

\subsubsection{Model}

The model consists of three separate 3D convolutional networks based
on C3D~\cite{DBLP:journals/corr/TranBFTP14}: proposal, classification and
localization networks. C3D is pre-trained on Sports-1M.

The proposal network has a binary ``action or not?'' output, the classification
network outputs the class scores (plus background), and the localization
network outputs class scores as well, except is trained with a different loss
function.

\subsubsection{Inference}

Chunks of 16, 32, 64, 128, 256 and 512 frames are taken from the video at
intervals overlapping by 75\% of the chunk size. 16 frames are then sampled
uniformly from each of these chunks, and the resulting 16 frames are input to
the proposal network.

The classification network is trained first, then used to initialize the
localization network.

During evaluation, segments with proposal scores $\geq 0.7$ are kept.
Post-processing is done to remove segments predicted as background, as well as
to scale up confidence scores by the class distribution in the training set.
Non-maximum suppression is done to remove redundant detections, with overlap
threshold $\theta - 0.1$, where $\theta$ is the IoU overlap threshold.

\subsubsection{Criticism}

Ground truth labels for the proposal network are assigned as follows. Segments
from trimmed videos are all assigned as ``action''. Segments with greater than
0.7 IoU with a class are labelled as positive, whereas if segments have less
than 0.3 IoU with any given class those segments are labelled as background.
Finally, for each ground truth instance, if the instance has no segments
assigned to it, a segment with greater than 0.5 IoU with that ground truth
instance is chosen as a positively labelled segment. A number of background
segments equal to the total number of positively labelled segments are randomly
sampled.

Ground truth labels for the classification network are gathered in a similar
way as for the proposal network, except labels are action classes, and
background class labels are reduced to match the average number of labels per
class.

An overlap loss is used to artificially increase the scores of predictions with
higher overlap with the ground truth instance.

\begin{equation}
        \mathcal{L}_{\textrm{overlap}} = \frac{1}{N} \sum_n
                \left(\frac{1}{2} \cdot
                        \left(
                                \frac{{\left( P_n^{k_n} \right)}^2}{v_n^\alpha}
                                - 1
                        \right)
                        \cdot \left[ k_n > 0 \right]
                \right)
\end{equation}

Where $\left[ k_n > 0 \right]$ is the indicator function for the true class
label $k_n$ being positive, $v_n$ is the fraction of overlap of the segment
with the ground truth instance, and $\alpha = 0.25$.

\subsubsection{Results}

At IoU threshold 0.5:

7.4 mAP on MEXaction2, compared with 1.7 mAP baseline provided with the
dataset.

19.0 mAP on THUMOS~2014, compared with 15.0 mAP from the THUMOS~2014 challenge
submission of~\cite{LearSubmissionThumos2014}.

\subsubsection{Questions}

Does uniform sampling mean random uniform sampling, or sampling at fixed,
uniform intervals?


\subsection{Temporal Convolutional Networks for Action Segmentation and
            Detection\cite{DBLP:journals/corr/LeaFVRH16}}

\subsubsection{Data}

50 salads~\cite{Stein:2013:CEA:2493432.2493482} is a dataset of 50 sequences of
food preparation video, with about 30 action annotations, such as ``cutting a
tomato'', per sequence. RGB and depth maps at $640 \times 480$ and 30Hz, 3-axis
accelerometer data from a device attached to various cooking utensils, as well
as synchronization parameters between the accelerometer and RGB-D data are
provided.

Annotations in 50 salads consist of ``pre-'', ``core'', and ``post-'' phases of
each action in a recipe.

25 different people in total are in the 50 salads videos.

The MERL shopping dataset~\cite{merl-shopping-singh} is a human action dataset
consisting of 96 two-minute surveillance-style videos, with a single shopper
per video.  The shoppers perform one of five actions, plus a background class:
``reach to shelf'', ``retract hand from shelf'', ``hand in shelf'', ``inspect
product'', and ``inspect shelf'', each action normally being a few seconds
long.

Georgia Tech Egocentric Activities (GTEA)~\cite{Fathi:2011:LRO:2191740.2191834}
contains 28 videos of household activities, such as making coffee, taken from a
head-mounted camera.  Videos are on average a minute long, and on average
contain 19 action instances per video.

\subsubsection{Model}

Two models are investigated, one called an ``Encoder-Decoder Temporal
Convolutional Network (ED-TCN)'', and the other called a ``Dilated TCN''.

In the TCN encoder, layers consist of a convolution plus bias, a non-linear
unit, followed by a max-pool. TCN decoder layers are a nearest-neighbour
upsample, followed by a convolution plus bias then a non-linear unit.

The dilated TCN consists of a series of blocks, where each block consists of a
sequence of layers. In each subsequent dilated convolutional layer within the
same block, each unit at time $t$ takes weighted input from the two previous
units at time $t$ and $t - s$, where $s$ is the same as the $l$-dilation
parameter as defined in~\cite{DBLP:journals/corr/YuK15}.

The set of outputs from all of the blocks are summed, followed by a ReLU, then
a fully-connected layer and another ReLU\@, which is input to a softmax
function, the output of which forms the frame-wise predictions.

For ED-TCN, each layer $l$ has $96 + 32l$ filters. For dilated TCN, each layer
has 128 filters.

Causal and decausal models are evaluated. For ED-TCN, models convolve from $t -
d$ to $t$ in the causal case, and from $t - d/2$ to $t + d/2$ in the acausal
case. For dilated TCN, units at time $t$ take the additional input from $t + s$
in the acausal case.

\subsubsection{Inference}

Adam optimizer~\cite{DBLP:journals/corr/KingmaB14} is used.

\subsubsection{Criticism}

``Action segmentation'' metrics use frame-wise mAP, while ``action detection''
metrics use segment-wise mAP for given IoU overlap fractions.

It is pointed out that the way confidence evaluated heavily affects the score
for a given type of metric. E.g.\ on MERL shopping, the mAP scores
from~\cite{merl-shopping-singh} increase from 50.9 to 69.8 by using maximum
instead of average prediction score over a given interval.

A new evaluation metric is proposed, which computes true positives, false
positives and false negatives over segments at a given IoU threshold, and defines
$F1 = 2\frac{prec * recall}{prec + recall}$.

\subsubsection{Results}

On GTEA, ED-TCN achieves an average F1 score of 64.0 over IoU thresholds of
0.1, 0.25 and 0.5 and dilated TCN achieves an average F1 score of 60.6. A
baseline method, with better descriptors, achieves 64.6.

On 50 salads, different activation units are compared, with normalized ReLUs
out-performing all with a F1@25 score of 58.4, compared with 40.4 using
standard ReLUs. Activation function choice is not found to affect dilated TCN
performance.

ED-TCN performed best with two layers and filter size of 15 (44 frame receptive
field). Dilated TCN performed best with four blocks and five layers per block
(128 frame receptive field), and similar performance with 96 frame receptive
field.

\subsubsection{Questions}

Dilated convolutions with larger filter sizes? Dilated convolutions over 3D
data?


\subsection{Deep Temporal Linear Encoding
            Networks\cite{DBLP:journals/corr/DibaSG16}}

A method called Temporal Linear Encoding (TLE) is proposed to encode entire
videos into vectors in a single feature space.

\subsubsection{Data}

\hyperref[ucf101]{UCF-101} and \hyperref[hmdb51]{HMDB-51}.

\subsubsection{Model}

The deep temporal linear encoding layer takes in a set $\{S_i\}$ of feature
maps extracted from clips in the video, and does,

\begin{enumerate}
        \item Aggregation of the features $\{S_i\}$.
        \item Encoding of the aggregated features.
\end{enumerate}

Aggregation operators include element-wise average, maximum and multiplication
of segments, with element-wise multiplication yielding the best results.

Encoding methods include,

\begin{enumerate}
        \item Bilinear combination given by $y = W[X \otimes X']$, where $X \in
                \mathbb{R}^{(hw) \times c}$ and in this case $X = X'$.
                Presumably, $W$ is element-wise multiplication of the of the
                resulting $[X \otimes X'] \in \mathbb{R}^{(cc')}$. The square
                brackets mean concatenation into a vector.

        \item Fully connected pooling
\end{enumerate}

AlexNet, VGG-16 and BN-Inception are used as two-stream ConvNet base models,
pre-trained on ImageNet.

C3D models are also used as a base feature extractor.

\subsubsection{Inference}

The Tensor Sketch algorithm of~\cite{Pham:2013:FSP:2487575.2487591} is used to
approximate the outer product for the bilinear encoding method by projecting
the two input tensors directly to a lower-dimensional space, without explicitly
computing the outer product.

Fully-connected layers are dropped from the pre-trained ConvNet models and the
feature maps from the last convolutional layers are fed into the bilinear
model. E.g. BN-Inception produces features maps of dimension $14 \times 14
\times 1024$, leading to bilinear outputs of dimension $1024 \times 1024$, and
corresponding compact bilinear representation of size $8196$.

The ConvNet models are then fine-tuned, first fine-tuning the last layer, then
the entire ConvNet.

Features output from the bilinear model have a signed square root and
L2-normalization applied before being input to a softmax layer.

A similar procedure to above is applied for the fully-connected pooling models.

They first split each video into three equal segments. Then, for two-stream,
one RGB frame and corresponding stack of optical flow frames is extracted for
each segment, and the frames extracted from all three segments are input to the
TLE model. Similarly, for C3D, one clip of 16 frames is extracted from each
segment and the three sets of extracted features are input to the TLE model.

For the two-stream model, the above three-segment extraction-prediction
sequence is completed five times and the resulting predictions are averaged.
For the C3D model, the above is repeated three times.

\subsubsection{Criticism}

Presumably cross-entropy loss on the output of the softmax function.

\subsubsection{Results}

Element-wise multiplication (94.8/70.4) beat average (92.6/68.1) and maximum
(91.3/67.4) aggregation functions on UCF101/HMDB51.

With two-stream ConvNets, TLE\@: bilinear achieved 95.6/71.1 on UCF101/HMDB51,
compared with $94.0/68.5$ in~\cite{DBLP:journals/corr/WangXW0LTG16}. With C3D,
TLE\@: bilinear scores $86.3/60.3$.

An experiment was run to include a fourth segment extracted from a VGG-16
network pre-trained on Places365, a so-called ``context'' feature map, in
addition to a three-segment spatial ConvNet TLE\@. This setup scored
$83.8/63.6$, as compared to $81.5/60.9$ using only spatial ConvNets\@.

\subsubsection{Questions}

Would knowledge transfrom from Places365 still work for the two-stream
ConvNets?


\section{Reinforcement Learning}


\subsection{Neural Optimizer Search with Reinforcement
            Learning~\cite{neural-optimizer-search-46114}}

\subsubsection{Data}

An optimizer is learned on CIFAR-10~\ref{cifar10}.

The optimizer is transferred to WMT~2014~\ref{wmt2014}.

The \href{https://en.wikipedia.org/wiki/Rosenbrock_function}{Rosenbrock
function} is trained on using the learned update rule.

\subsubsection{Model}

% TODO(brendan): TikZ diagram of model controller RNN and computation graph.

The controller is an RNN, which samples strings of length $5n$ from the DSL
during training. Values of $n$ equal to 1, 2 and 3 are used in the experiments.

The controller is a single-layer LSTM with a \num{150}-unit hidden layer and
weights uniformly initialized in $[-0.08, 0.08]$. An entropy penalty on the
weights is used, set to $0.0015$.

A domain specific language (DSL) is cooked up to write a binary expression
tree, which represents the learned update function. The DSL uses postfix
notation.

Refer to section 4.1 of the paper for a list of operands, unary and binary
functions used in the DSL\@. Adam and RMSProp are among the operand primitives.

Weight updates are computed as
$\Delta w = \lambda * b(u_1({op}_1), u_2({op}_2))$.

A two-layer $3 \times 3$ convolutional network, with ReLU activations and batch
normalization after each layer, is used as the model to optimize over for the
CIFAR-10 dataset. The learned update rule is then transferred to train a Wide
ResNet model.

The Google Neural Machine Translation (GNMT) model from~\cite{gnmt-45610} is
used for the WMT~2014 experiments.

\subsubsection{Inference}

Samples generated by the controller RNN are added to a queue, from which a
worker from a distributed set of workers dequeues, trains, and returns the
accuracy $R(\Delta)$ to the controller.

After workers receive an optimizer from the controller, the worker does a
hyperparameter sweep for learning rates given by $10^i$, with $i \in [-5, 1]$.
The best learning rate is then trained for five epochs before evaluating.

Child networks have a batch size of 100.

Update rule hyperparameters are: $\beta_1 = 0.9$, $\beta_2 = \beta_3 = 0.999$,
and $\epsilon = 10^{-8}$.

Trust Region Policy Optimization~\cite{DBLP:journals/corr/SchulmanLMJA15} is
found to improve over REINFORCE~\cite{Williams1992} for training the controller
RNN\@. The baseline function used for TRPO is an exponential moving average of
previous rewards.

Adam optimizer is used to train the RNN controller, with a learning rate of
$10^{-5}$ and a minibatch size of five.

\subsubsection{Criticism}

The training objective for the RNN controller is
$\mathbb{E}_{\Delta \sim p_\theta(.)}[R(\Delta)]$, where $R(\Delta)$ is the
accuracy of the model on a held-out validation set after training the
model-to-optimize using the sampled optimizer.

\subsubsection{Results}

Of a number of discovered optimizers, \verb|Optimizer_1|, given by $e^{sign(g)
* sign(m)} * g$, performs nearly as well as SGD with momentum on the Rosenbrock
function, and better than all other compared optimizers (Adam, RMSProp, and
SGD).

The best discovered optimizers outperform SGD with momentum on CIFAR-10 using a
Wide ResNet, scoring 93.2 compared with 92.3 for momentum.

On WMT~2014, the best optimizer achieves an improved BLEU score of 25.2, as
compared with 24.5 with Adam.

\subsubsection{Questions}

Can a learned optimizer improve upon existing standard optimizers such as Adam
and SGD with momentum?

Will optimizers learned on one task transfer well to other tasks?

What is the computational cost of doing a neural optimizer search, and how
could said cost be alleviated?

The neural optimizer search was distributed over \num{100} CPU servers over a
period of less than a day.

How can the neural architecture search be efficiently extended to check for
optimizers that perform well over the entire training cycle, but perform
relatively poorly during the first $N$ epochs?

\subsubsection{Similar Work}

Trust Region Policy Optimization~\cite{DBLP:journals/corr/SchulmanLMJA15}
(background).

\bibliographystyle{apalike}
\bibliography{ml-reading-notes}

\end{document}
